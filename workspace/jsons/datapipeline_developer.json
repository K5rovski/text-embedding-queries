[
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#what-is-datapipeline",
"main_header": "What is AWS Data Pipeline?",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"title": "Pipeline Definition File Syntax"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"title": "Task Runners"
}
],
"text": "A pipeline definition specifies the business logic of your data management. For more information, see Pipeline Definition File Syntax. A pipeline schedules and runs tasks by creating Amazon EC2 instances to perform the defined work activities. You upload your pipeline definition to the pipeline, and then activate the pipeline. You can edit the pipeline definition for a running pipeline and activate the pipeline again for it to take effect. You can deactivate the pipeline, modify a data source, and then activate the pipeline again. When you are finished with your pipeline, you can delete it. Task Runner polls for tasks and then performs those tasks. For example, Task Runner could copy log files to Amazon S3 and launch Amazon EMR clusters. Task Runner is installed and runs automatically on resources created by your pipeline definitions. You can write a custom task runner application, or you can use the Task Runner application that is provided by AWS Data Pipeline. For more information, see Task Runners.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#what-is-datapipeline",
"main_header": "What is AWS Data Pipeline?",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#what-is-datapipeline",
"main_header": "What is AWS Data Pipeline?",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-how-dp-works-v2.png",
"alt": "\n            AWS Data Pipeline functional overview\n        "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"h2": {
"urllink": "#accessing-datapipeline",
"text": "Accessing AWS Data Pipeline"
},
"links": [
{
"url": "https://aws.amazon.com/cli/",
"title": "AWS Command Line Interface"
},
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/index.html",
"title": "datapipeline"
},
{
"url": "http://aws.amazon.com/tools/#SDKs",
"title": "AWS SDKs"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/APIReference/",
"title": "AWS Data Pipeline API Reference"
}
],
"text": "AWS Management Console\u00e2\u0080\u0094 Provides a web interface that you can use to access AWS Data Pipeline.\n\nAWS Command Line Interface (AWS CLI) \u00e2\u0080\u0094 Provides commands for a broad set of AWS services, including AWS Data Pipeline, and is supported on Windows, macOS, and Linux. For more information about installing the AWS CLI, see AWS Command Line Interface. For a list of commands for AWS Data Pipeline, see datapipeline.\n\nAWS SDKs \u00e2\u0080\u0094 Provides language-specific APIs and takes care of many of the connection details, such as calculating signatures, handling request retries, and error handling. For more information, see AWS SDKs.\n\nQuery API\u00e2\u0080\u0094 Provides low-level APIs that you call using HTTPS requests. Using the Query API is the most direct way to access AWS Data Pipeline, but it requires that your application handle low-level details such as generating the hash to sign the request, and error handling. For more information, see the AWS Data Pipeline API Reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#accessing-datapipeline",
"main_header": "Accessing AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#datapipeline-related-services",
"text": "Related Services"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html#datapipeline-related-services",
"main_header": "Related Services",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#datapipeline-related-services",
"text": "Related Services"
},
"links": [
{
"url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/",
"title": "Amazon DynamoDB Developer Guide"
},
{
"url": "https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/",
"title": "Amazon Relational Database Service Developer Guide"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/",
"title": "Amazon Redshift Database Developer Guide"
},
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/dev/",
"title": "Amazon Simple Storage Service User Guide"
}
],
"text": "Amazon DynamoDB \u00e2\u0080\u0094 Provides a fully managed NoSQL database with fast performance at a low cost. For more information, see Amazon DynamoDB Developer Guide.\n\nAmazon RDS \u00e2\u0080\u0094 Provides a fully managed relational database that scales to large datasets. For more information, see Amazon Relational Database Service Developer Guide.\n\nAmazon Redshift \u00e2\u0080\u0094 Provides a fast, fully managed, petabyte-scale data warehouse that makes it easy and cost-effective to analyze a vast amount of data. For more information, see Amazon Redshift Database Developer Guide.\n\nAmazon S3 \u00e2\u0080\u0094 Provides secure, durable, and highly scalable object storage. For more information, see Amazon Simple Storage Service User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html#datapipeline-related-services",
"main_header": "Related Services",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#datapipeline-related-services",
"text": "Related Services"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/",
"title": "Amazon EC2 User Guide for Linux Instances"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/DeveloperGuide/",
"title": "Amazon EMR Developer Guide"
}
],
"text": "Amazon EC2 \u00e2\u0080\u0094 Provides resizable computing capacity\u00e2\u0080\u0094literally, servers in Amazon's data centers\u00e2\u0080\u0094that you use to build and host your software systems. For more information, see Amazon EC2 User Guide for Linux Instances.\n\nAmazon EMR \u00e2\u0080\u0094 Makes it easy, fast, and cost-effective for you to distribute and process vast amounts of data across Amazon EC2 servers, using a framework such as Apache Hadoop or Apache Spark. For more information, see Amazon EMR Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html#datapipeline-related-services",
"main_header": "Related Services",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#dp-supported-instance-types",
"text": "Supported Instance Types for Pipeline Work Activities"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html#dp-supported-instance-types",
"main_header": "Supported Instance Types for Pipeline Work Activities",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "Region Name\nRegion\nInstance Type US East (N. Virginia)\nus-east-1\nm1.small US West (Oregon)\nus-west-2\nm1.small Asia Pacific (Sydney)\nap-southeast-2\nm1.small Asia Pacific (Tokyo)\nap-northeast-1\nm1.small EU (Ireland)\neu-west-1\nm1.small",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "Region Name\nRegion\nInstance Type US East (N. Virginia)\nus-east-1\nm1.small US West (Oregon)\nus-west-2\nm1.small Asia Pacific (Sydney)\nap-southeast-2\nm1.small Asia Pacific (Tokyo)\nap-northeast-1\nm1.small EU (Ireland)\neu-west-1\nm1.small",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "Region Name\nRegion\nInstance Type US East (Ohio)\nus-east-2\nt2.small US West (N. California)\nus-west-1\nm1.small Asia Pacific (Mumbai)\nap-south-1\nt2.small Asia Pacific (Singapore)\nap-southeast-1\nm1.small Asia Pacific (Seoul)\nap-northeast-2\nt2.small Canada (Central)\nca-central-1\nt2.small EU (Frankfurt)\neu-central-1\nt2.small EU (London)\neu-west-2\nt2.small EU (Paris)\neu-west-3\nt2.small South America (S\u00c3\u00a3o Paulo)\nsa-east-1\nm1.small",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "Region Name\nRegion\nInstance Type US East (Ohio)\nus-east-2\nt2.small US West (N. California)\nus-west-1\nm1.small Asia Pacific (Mumbai)\nap-south-1\nt2.small Asia Pacific (Singapore)\nap-southeast-1\nm1.small Asia Pacific (Seoul)\nap-northeast-2\nt2.small Canada (Central)\nca-central-1\nt2.small EU (Frankfurt)\neu-central-1\nt2.small EU (London)\neu-west-2\nt2.small EU (Paris)\neu-west-3\nt2.small South America (S\u00c3\u00a3o Paulo)\nsa-east-1\nm1.small",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-supported-instance-types",
"text": "Additional Supported Amazon EC2 Instances"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html#dp-ec2-supported-instance-types",
"main_header": "Additional Supported Amazon EC2 Instances",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-supported-instance-types",
"text": "Additional Supported Amazon EC2 Instances"
},
"links": [],
"text": "Instance Class\nInstance Types General purpose\n\nt2.nano | t2.micro | t2.small | t2.medium | t2.large Compute optimized\n\nc3.large | c3.xlarge | c3.2xlarge | c3.4xlarge | c3.8xlarge | c4.large | c4.xlarge | c4.2xlarge | c4.4xlarge | c4.8xlarge | c5.xlarge | c5.9xlarge | c5.2xlarge | c5.4xlarge | c5.9xlarge | c5.18xlarge | c5d.xlarge | c5d.2xlarge | c5d.4xlarge | c5d.9xlarge | c5d.18xlarge Memory optimized\n\nm3.medium | m3.large | m3.xlarge | m3.2xlarge | m4.large | m4.xlarge | m4.2xlarge | m4.4xlarge | m4.10xlarge | m4.16xlarge | m5.xlarge | m5.2xlarge | m5.4xlarge | m5.12xlarge | m5.24xlarge | m5d.xlarge | m5d.2xlarge | m5d.4xlarge | m5d.12xlarge | m5d.24xlarge\nr3.large | r3.xlarge | r3.2xlarge | r3.4xlarge | r3.8xlarge | r4.large | r4.xlarge | r4.2xlarge | r4.4xlarge | r4.8xlarge | r4.16xlarge Storage optimized i2.xlarge | i2.2xlarge | i2.4xlarge | i2.8xlarge | hs1.8xlarge | g2.2xlarge | g2.8xlarge | d2.xlarge | d2.2xlarge | d2.4xlarge | d2.8xlarge",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html#dp-ec2-supported-instance-types",
"main_header": "Additional Supported Amazon EC2 Instances",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-supported-instance-types",
"text": "Additional Supported Amazon EC2 Instances"
},
"links": [],
"text": "Instance Class\nInstance Types General purpose\n\nt2.nano | t2.micro | t2.small | t2.medium | t2.large Compute optimized\n\nc3.large | c3.xlarge | c3.2xlarge | c3.4xlarge | c3.8xlarge | c4.large | c4.xlarge | c4.2xlarge | c4.4xlarge | c4.8xlarge | c5.xlarge | c5.9xlarge | c5.2xlarge | c5.4xlarge | c5.9xlarge | c5.18xlarge | c5d.xlarge | c5d.2xlarge | c5d.4xlarge | c5d.9xlarge | c5d.18xlarge Memory optimized\n\nm3.medium | m3.large | m3.xlarge | m3.2xlarge | m4.large | m4.xlarge | m4.2xlarge | m4.4xlarge | m4.10xlarge | m4.16xlarge | m5.xlarge | m5.2xlarge | m5.4xlarge | m5.12xlarge | m5.24xlarge | m5d.xlarge | m5d.2xlarge | m5d.4xlarge | m5d.12xlarge | m5d.24xlarge\nr3.large | r3.xlarge | r3.2xlarge | r3.4xlarge | r3.8xlarge | r4.large | r4.xlarge | r4.2xlarge | r4.4xlarge | r4.8xlarge | r4.16xlarge Storage optimized i2.xlarge | i2.2xlarge | i2.4xlarge | i2.8xlarge | hs1.8xlarge | g2.2xlarge | g2.8xlarge | d2.xlarge | d2.2xlarge | d2.4xlarge | d2.8xlarge",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html#dp-ec2-supported-instance-types",
"main_header": "Additional Supported Amazon EC2 Instances",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-emr-supported-instance-types",
"text": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html#dp-emr-supported-instance-types",
"main_header": "Supported Amazon EC2 Instances for Amazon EMR Clusters",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-emr-supported-instance-types",
"text": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
"links": [],
"text": "Instance Class\nInstance Types General purpose\n\nm1.small | m1.medium | m1.large | m1.xlarge | m3.xlarge | m3.2xlarge Compute optimized\n\nc1.medium | c1.xlarge | c3.xlarge | c3.2xlarge | c3.4xlarge | c3.8xlarge | cc1.4xlarge| cc2.8xlarge | c4.large | c4.xlarge | c4.2xlarge| c4.4xlarge | c4.8xlarge | c5.xlarge | c5.9xlarge | c5.2xlarge | c5.4xlarge | c5.9xlarge | c5.18xlarge | c5d.xlarge | c5d.2xlarge | c5d.4xlarge | c5d.9xlarge | c5d.18xlarge Memory optimized\nm2.xlarge | m2.2xlarge | m2.4xlarge | r3.xlarge | r3.2xlarge | r3.4xlarge | r3.8xlarge | cr1.8xlarge | m4.large | m4.xlarge | m4.2xlarge | m4.4xlarge | m4.10xlarge | m4.16large | m5.xlarge | m5.2xlarge | m5.4xlarge | m5.12xlarge |  m5.24xlarge |  m5d.xlarge | m5d.2xlarge |  m5d.4xlarge | m5d.12xlarge | m5d.24xlarge | r4.large | r4.xlarge | r4.2xlarge | r4.4xlarge | r4.8xlarge | r4.16xlarge Storage optimized\n\nh1.4xlarge | hs1.2xlarge | hs1.4xlarge| hs1.8xlarge | i2.xlarge | i2.2xlarge | i2.4large | i2.8xlarge | d2.xlarge | d2.2xlarge| d2.4xlarge | d2.8xlarge Accelerated computing\ng2.2xlarge | cg1.4xlarge",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html#dp-emr-supported-instance-types",
"main_header": "Supported Amazon EC2 Instances for Amazon EMR Clusters",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-emr-supported-instance-types",
"text": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
"links": [],
"text": "Instance Class\nInstance Types General purpose\n\nm1.small | m1.medium | m1.large | m1.xlarge | m3.xlarge | m3.2xlarge Compute optimized\n\nc1.medium | c1.xlarge | c3.xlarge | c3.2xlarge | c3.4xlarge | c3.8xlarge | cc1.4xlarge| cc2.8xlarge | c4.large | c4.xlarge | c4.2xlarge| c4.4xlarge | c4.8xlarge | c5.xlarge | c5.9xlarge | c5.2xlarge | c5.4xlarge | c5.9xlarge | c5.18xlarge | c5d.xlarge | c5d.2xlarge | c5d.4xlarge | c5d.9xlarge | c5d.18xlarge Memory optimized\nm2.xlarge | m2.2xlarge | m2.4xlarge | r3.xlarge | r3.2xlarge | r3.4xlarge | r3.8xlarge | cr1.8xlarge | m4.large | m4.xlarge | m4.2xlarge | m4.4xlarge | m4.10xlarge | m4.16large | m5.xlarge | m5.2xlarge | m5.4xlarge | m5.12xlarge |  m5.24xlarge |  m5d.xlarge | m5d.2xlarge |  m5d.4xlarge | m5d.12xlarge | m5d.24xlarge | r4.large | r4.xlarge | r4.2xlarge | r4.4xlarge | r4.8xlarge | r4.16xlarge Storage optimized\n\nh1.4xlarge | hs1.2xlarge | hs1.4xlarge| hs1.8xlarge | i2.xlarge | i2.2xlarge | i2.4large | i2.8xlarge | d2.xlarge | d2.2xlarge| d2.4xlarge | d2.8xlarge Accelerated computing\ng2.2xlarge | cg1.4xlarge",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html#dp-emr-supported-instance-types",
"main_header": "Supported Amazon EC2 Instances for Amazon EMR Clusters",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-concepts",
"text": "AWS Data Pipeline Concepts"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html#dp-concepts",
"main_header": "AWS Data Pipeline Concepts",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "Names, locations, and formats of your data sources Activities that transform the data The schedule for those activities Resources that run your activities and preconditions Preconditions that must be satisfied before the activities can be scheduled Ways to alert you with status updates as pipeline execution proceeds",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/APIReference/Welcome.html",
"title": "AWS Data Pipeline API"
}
],
"text": "Graphically, by using the AWS Data Pipeline console Textually, by writing a JSON file in the format used by the command line interface Programmatically, by calling the web service with either one of the AWS SDKs or the AWS Data Pipeline API",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html",
"title": "Data Nodes"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"title": "Activities"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"title": "Preconditions"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"title": "Scheduling Pipelines"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"title": "Actions"
}
],
"text": "Pipeline Components\n\nData Nodes\n\nThe location of input data for a task or the location where output data is to be stored.\n\nActivities\n\nA definition of work to perform on a schedule using a computational resource and typically input and output data nodes.\n\nPreconditions\n\nA conditional statement that must be true before an action can run. Scheduling Pipelines\n\nDefines the timing of a scheduled event, such as when an activity runs.\n\nResources\n\nThe computational resource that performs the work that a pipeline defines.\n\nActions\n\nAn action that is triggered when specified conditions are met, such as the failure of an activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"dl",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "Pipeline Components",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "Pipeline Components \u00e2\u0080\u0094 Pipeline components represent the business logic of the pipeline and are represented by the different sections of a pipeline definition. Pipeline components specify the data sources, activities, schedule, and preconditions of the workflow. They can inherit properties from parent components. Relationships among components are defined by reference. Pipeline components define the rules of data management. Instances \u00e2\u0080\u0094 When AWS Data Pipeline runs a pipeline, it compiles the pipeline components to create a set of actionable instances. Each instance contains all the information for performing a specific task. The complete set of instances is the to-do list of the pipeline. AWS Data Pipeline hands the instances out to task runners to process. Attempts \u00e2\u0080\u0094 To provide robust data management, AWS Data Pipeline retries a failed operation. It continues to do so until the task reaches the maximum number of allowed retry attempts. Attempt objects track the various attempts, results, and failure reasons if applicable. Essentially, it is the instance with a counter. AWS Data Pipeline performs retries using the same resources from the previous attempts, such as Amazon EMR clusters and EC2 instances.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "NoteRetrying failed tasks is an important part of a fault tolerance strategy, and AWS Data Pipeline definitions provide conditions and thresholds to control retries. However, too many retries can delay detection of an unrecoverable failure because AWS Data Pipeline does not report failure until it has exhausted all the retries that you specify. The extra retries may accrue additional charges if they are running on AWS resources. As a result, carefully consider when it is appropriate to exceed the AWS Data Pipeline default settings that you use to control re-tries and related settings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "Retrying failed tasks is an important part of a fault tolerance strategy, and AWS Data Pipeline definitions provide conditions and thresholds to control retries. However, too many retries can delay detection of an unrecoverable failure because AWS Data Pipeline does not report failure until it has exhausted all the retries that you specify. The extra retries may accrue additional charges if they are running on AWS resources. As a result, carefully consider when it is appropriate to exceed the AWS Data Pipeline default settings that you use to control re-tries and related settings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "Retrying failed tasks is an important part of a fault tolerance strategy, and AWS Data Pipeline definitions provide conditions and thresholds to control retries. However, too many retries can delay detection of an unrecoverable failure because AWS Data Pipeline does not report failure until it has exhausted all the retries that you specify. The extra retries may accrue additional charges if they are running on AWS resources. As a result, carefully consider when it is appropriate to exceed the AWS Data Pipeline default settings that you use to control re-tries and related settings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-object-types.png",
"alt": "\n                        AWS Data Pipeline components, instances, and attempts\n                    "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-task-lifecycle.png",
"alt": "\n                AWS Data Pipeline task lifecycle\n            "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "AWS Data Pipeline installs Task Runner for you on resources that are launched and managed by the AWS Data Pipeline web service. You install Task Runner on a computational resource that you manage, such as a long-running EC2 instance, or an on-premises server.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-datanodes",
"text": "Data Nodes"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html#dp-concepts-datanodes",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-datanodes",
"text": "Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"title": "DynamoDBDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"title": "HiveActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "DynamoDBDataNode\n\nA DynamoDB table that contains data for HiveActivity or EmrActivity to use.\n\nSqlDataNode\n\nAn SQL table and database query that represent data for a pipeline activity to use.\nNotePreviously, MySqlDataNode was used. Use SqlDataNode instead.\n\nRedshiftDataNode\n\nAn Amazon Redshift table that contains data for RedshiftCopyActivity to use.\n\nS3DataNode\n\nAn Amazon S3 location that contains one or more files for a pipeline activity to use.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html#dp-concepts-datanodes",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-databases",
"text": "Databases"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-databases.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-databases.html#dp-concepts-databases",
"main_header": "Databases",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-databases",
"text": "Databases"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"title": "JdbcDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"title": "RdsDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
}
],
"text": "JdbcDatabase\n\nA JDBC database.\n\nRdsDatabase\n\nAn Amazon RDS database.\n\nRedshiftDatabase\n\nAn Amazon Redshift database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-databases.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-databases.html#dp-concepts-databases",
"main_header": "Databases",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-activities",
"text": "Activities"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html#dp-concepts-activities",
"main_header": "Activities",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-activities",
"text": "Activities"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"title": "HiveActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"title": "HiveCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"title": "DynamoDBDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"title": "PigActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"title": "SqlActivity"
}
],
"text": "CopyActivity\n\nCopies data from one location to another.\n\nEmrActivity\n\nRuns an Amazon EMR cluster.\n\nHiveActivity\n\nRuns a Hive query on an Amazon EMR cluster.\n\nHiveCopyActivity\n\nRuns a Hive query on an Amazon EMR cluster with support for advanced data filtering and support for S3DataNode and DynamoDBDataNode.\n\nPigActivity\n\nRuns a Pig script on an Amazon EMR cluster.\n\nRedshiftCopyActivity\n\nCopies data to and from Amazon Redshift tables.\n\nShellCommandActivity\n\nRuns a custom UNIX/Linux shell command as an activity.\n\nSqlActivity\n\nRuns a SQL query on a database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html#dp-concepts-activities",
"main_header": "Activities",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-preconditions",
"text": "Preconditions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html#dp-concepts-preconditions",
"main_header": "Preconditions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-preconditions",
"text": "Preconditions"
},
"h2": {
"urllink": "#dp-concepts-system-preconditions",
"text": "System-Managed Preconditions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"title": "DynamoDBDataExists"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"title": "DynamoDBTableExists"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"title": "S3KeyExists"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"title": "S3PrefixNotEmpty"
}
],
"text": "DynamoDBDataExists\n\nChecks whether data exists in a specific DynamoDB table.\n\nDynamoDBTableExists\n\nChecks whether a DynamoDB table exists.\n\nS3KeyExists\n\nChecks whether an Amazon S3 key exists.\n\nS3PrefixNotEmpty\n\nChecks whether an Amazon S3 prefix is empty.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html#dp-concepts-system-preconditions",
"main_header": "System-Managed Preconditions",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-preconditions",
"text": "Preconditions"
},
"h2": {
"urllink": "#dp-concepts-user-preconditions",
"text": "User-Managed Preconditions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"title": "Exists"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"title": "ShellCommandPrecondition"
}
],
"text": "Exists\n\nChecks whether a data node exists.\n\nShellCommandPrecondition\n\nRuns a custom Unix/Linux shell command as a precondition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html#dp-concepts-user-preconditions",
"main_header": "User-Managed Preconditions",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-concepts-resources",
"main_header": "Resources",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "Ec2Resource\n\nAn EC2 instance that performs the work defined by a pipeline activity.\n\nEmrCluster\n\nAn Amazon EMR cluster that performs the work defined by a pipeline activity, such as EmrActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-concepts-resources",
"main_header": "Resources",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-limits",
"text": "Resource Limits"
},
"links": [],
"text": "NoteThe limit is one instance per Ec2Resource component object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-limits",
"main_header": "Resource Limits",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-limits",
"text": "Resource Limits"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-limits",
"main_header": "Resource Limits",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-limits",
"text": "Resource Limits"
},
"links": [],
"text": "The limit is one instance per Ec2Resource component object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-limits",
"main_header": "Resource Limits",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-limits",
"text": "Resource Limits"
},
"links": [],
"text": "The limit is one instance per Ec2Resource component object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-limits",
"main_header": "Resource Limits",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-supported-platforms",
"text": "Supported Platforms"
},
"links": [],
"text": "EC2-Classic\n\nYour resources run in a single, flat network that you share with other customers.\n\nEC2-VPC\n\nYour resources run in a virtual private cloud (VPC) that's logically isolated to your AWS account.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-supported-platforms",
"main_header": "Supported Platforms",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-emrspotinstances",
"text": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline"
},
"h3": {
"urllink": "#dp-emrspotinstances-considerations",
"text": "Spot Instances Considerations"
},
"links": [],
"text": "Your Spot Instances can terminate when the Spot Instance price goes above your maximum price for the instance, or due to Amazon EC2 capacity reasons. However, you do not lose your data because AWS Data Pipeline employs clusters with core nodes that are always On-Demand Instances and not subject to termination.\n\nSpot Instances can take more time to start as they fulfill capacity asynchronously. Therefore, a Spot Instance pipeline could run more slowly than an equivalent On-Demand Instance pipeline.\n\nYour cluster might not run if you do not receive your Spot Instances, such as when your maximum price is too low.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-emrspotinstances-considerations",
"main_header": "Spot Instances Considerations",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-actions",
"text": "Actions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html#dp-concepts-actions",
"main_header": "Actions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-actions",
"text": "Actions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"title": "Terminate"
}
],
"text": "SnsAlarm\n\nAn action that sends an SNS notification to a topic based on onSuccess, OnFail, and onLateAction events.\n\nTerminate\n\nAn action that triggers the cancellation of a pending or unfinished activity, resource, or data node. You cannot terminate actions that include onSuccess, OnFail, or onLateAction.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html#dp-concepts-actions",
"main_header": "Actions",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-get-setup",
"main_header": "Setting up for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-sign-up",
"title": "Sign up for AWS"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-roles-new",
"title": "Create IAM Roles for AWS Data Pipeline and Pipeline Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"title": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
}
],
"text": "TasksSign up for AWSCreate IAM Roles for AWS Data Pipeline and Pipeline ResourcesAllow IAM Principals (Users and Groups) to Perform Necessary Actions",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-get-setup",
"main_header": "Setting up for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-get-setup",
"main_header": "Setting up for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-sign-up",
"text": "Sign up for AWS"
},
"links": [
{
"url": "https://portal.aws.amazon.com/billing/signup",
"title": "https://portal.aws.amazon.com/billing/signup"
}
],
"text": "To create an AWS accountOpen https://portal.aws.amazon.com/billing/signup.\nFollow the online instructions.\nPart of the sign-up procedure involves receiving a phone call and entering a verification code on the phone keypad.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-sign-up",
"main_header": "Sign up for AWS",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-sign-up",
"text": "Sign up for AWS"
},
"links": [],
"text": "To create an AWS account",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-sign-up",
"main_header": "Sign up for AWS",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [
{
"url": "https://console.aws.amazon.com/iam/",
"title": "https://console.aws.amazon.com/iam/"
}
],
"text": "To create a user group DataPipelineDevelopers and attach the AWSDataPipeline_FullAccess policyOpen the IAM console at https://console.aws.amazon.com/iam/.\nIn the navigation pane, choose Groups, Create New Group.\n\nEnter a Group Name, for example, DataPipelineDevelopers, and then choose Next Step.\n\nEnter AWSDataPipeline_FullAccess for Filter and then select it from the list.\n\nChoose Next Step and then choose Create Group.\n\nTo add users to the group:\nSelect the group you created from the list of groups.Choose Group Actions, Add Users to Group.Select the users you want to add from the list and then choose Add Users to Group.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [],
"text": "To create a user group DataPipelineDevelopers and attach the AWSDataPipeline_FullAccess policy",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "ShellCommandActivity\n\nReads the input log file and counts the number of errors.\n\nS3DataNode (input)\n\nThe S3 bucket that contains the input log file.\n\nS3DataNode (output)\n\nThe S3 bucket for the output.\n\nEc2Resource\n\nThe compute resource that AWS Data Pipeline uses to perform the activity.\nNote that if you have a large amount of log file data, you can configure your pipeline to use an EMR cluster to process the files instead of an EC2 instance.\n\nSchedule\n\nDefines that the activity is performed every 15 minutes for an hour.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-create",
"title": "Create the Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-monitor",
"title": "Monitor the Running Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-output",
"title": "View the Output"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-delete",
"title": "Delete the Pipeline"
}
],
"text": "TasksCreate the PipelineMonitor the Running PipelineView the OutputDelete the Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-create",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To create the pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nFrom the navigation bar, select a region. You can select any region that's available to you, regardless of your location. Many AWS resources are specific to a region, but AWS Data Pipeline enables you to use resources that are in a different region than the pipeline.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using a template, and then select the following template: Getting Started using ShellCommandActivity.\n\nUnder the Parameters section, which opened when you selected the template, leave S3 input folder and Shell command to run with their default values. Click the folder icon next to S3 output folder, select one of your buckets or folders, and then click Select.\n\nUnder Schedule, leave the default values. When you activate the pipeline the pipeline runs start, and then continue every 15 minutes for an hour.\nIf you prefer, you can select Run once on pipeline activation instead.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Activate.\nIf you prefer, you can choose Edit in Architect to modify this pipeline. For example, you can add preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-create",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-create",
"text": "Create the Pipeline"
},
"links": [],
"text": "To create the pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-create",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-monitor",
"text": "Monitor the Running Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline\nClick Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then click Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-monitor",
"main_header": "Monitor the Running Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-monitor",
"text": "Monitor the Running Pipeline"
},
"links": [],
"text": "To monitor the progress of your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-monitor",
"main_header": "Monitor the Running Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-delete",
"text": "Delete the Pipeline"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-delete",
"main_header": "Delete the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-delete",
"text": "Delete the Pipeline"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-delete",
"main_header": "Delete the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-managing-pipeline",
"text": "Working with Pipelines"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html#dp-managing-pipeline",
"main_header": "Working with Pipelines",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-managing-pipeline",
"text": "Working with Pipelines"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "ImportantBefore you begin, see Setting up for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html#dp-managing-pipeline",
"main_header": "Working with Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-managing-pipeline",
"text": "Working with Pipelines"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html#dp-managing-pipeline",
"main_header": "Working with Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-managing-pipeline",
"text": "Working with Pipelines"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "Before you begin, see Setting up for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html#dp-managing-pipeline",
"main_header": "Working with Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-managing-pipeline",
"text": "Working with Pipelines"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "Before you begin, see Setting up for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html#dp-managing-pipeline",
"main_header": "Working with Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-schedules",
"main_header": "Scheduling Pipelines",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "Field\nAction Name\nEnter a name for the pipeline. Description\n\n(Optional) Enter a description for the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "Field\nAction Name\nEnter a name for the pipeline. Description\n\n(Optional) Enter a description for the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "Field\nAction Run Choose on activation to run the pipeline as an on-demand pipeline. This creates a pipeline that can be run when it is activated. Run every \nEnter a period for every pipeline run. Starting\nEnter a time and date for the pipeline to start. Alternatively, your start date and time are automatically selected at pipeline activation. Ending\nEnter a time and date for the pipeline to end. If you select never, your pipeline continues to execute indefinitely.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "Field\nAction Run Choose on activation to run the pipeline as an on-demand pipeline. This creates a pipeline that can be run when it is activated. Run every \nEnter a period for every pipeline run. Starting\nEnter a time and date for the pipeline to start. Alternatively, your start date and time are automatically selected at pipeline activation. Ending\nEnter a time and date for the pipeline to end. If you select never, your pipeline continues to execute indefinitely.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "Field\nAction Default\nChoose this to have AWS Data Pipeline determine the roles for you. Custom\nChoose this to designate your own IAM roles. If you select this option, you can choose the following roles: Pipeline role\u00e2\u0080\u0094the role that determines what AWS Data Pipeline can do with resources in the account.\n\nEC2 instance role\u00e2\u0080\u0094the role that controls what Amazon EC2 applications can do with resources in the account.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "Field\nAction Default\nChoose this to have AWS Data Pipeline determine the roles for you. Custom\nChoose this to designate your own IAM roles. If you select this option, you can choose the following roles: Pipeline role\u00e2\u0080\u0094the role that determines what AWS Data Pipeline can do with resources in the account.\n\nEC2 instance role\u00e2\u0080\u0094the role that controls what Amazon EC2 applications can do with resources in the account.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "NoteYou can find the Default object on the Architect page in the Other section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "You can find the Default object on the Architect page in the Other section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "You can find the Default object on the Architect page in the Other section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "NoteThe minimum scheduling interval is 15 minutes.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "The minimum scheduling interval is 15 minutes.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "The minimum scheduling interval is 15 minutes.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "NoteYou can find the Default object on the Architect page in the Other section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "You can find the Default object on the Architect page in the Other section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "You can find the Default object on the Architect page in the Other section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-creating-pipelines",
"text": "Creating a Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html#dp-creating-pipelines",
"main_header": "Creating a Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-creating-pipelines",
"text": "Creating a Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"title": "Creating Pipelines Using Console Templates"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"title": "Creating Pipelines Using the Console Manually"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"title": "Working with the API"
}
],
"text": "Use the console with a template provided for your convenience. For more information, see Creating Pipelines Using Console Templates.\n\nUse the console to manually add individual pipeline objects. For more information, see Creating Pipelines Using the Console Manually.\n\nUse the AWS Command Line Interface (CLI) with a pipeline definition file in JSON format.\n\nUse an AWS SDK with a language-specific API. For more information, see Working with the API.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html#dp-creating-pipelines",
"main_header": "Creating a Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-console-templates",
"main_header": "Creating Pipelines Using Console Templates",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"h2": {
"urllink": "#dp-create-pipeline",
"text": "Initialize, Create, and Schedule a Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-choose-templates",
"title": "Choose a Template"
}
],
"text": "To create and schedule a pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nClick either Get started now or Create Pipeline.\n\nEnter a pipeline name and an optional description for the pipeline.\n\nChoose Build using Architect to interactively create and edit nodes in a pipeline definition or Build using a template to select a template. For more information about templates, see Choose a Template.\nIf you use choose to use a template, the console displays a form that is specific to that template under Parameters. Complete the form as appropriate.\n\nChoose whether to run the pipeline once on activation or on a schedule.\nIf you choose to run the pipeline on a schedule:\n\nFor Run every, choose a period for the pipeline. The start and end time must define an interval that's long enough to accommodate this period.\n\nChoose a Starting time. If you choose on pipeline activation, the pipeline uses the current activation time.\n\nChoose an Ending time. If you choose never, the pipeline runs indefinitely. Select an option for IAM Roles. If you select Default, AWS Data Pipeline assigns its own default roles. You can optionally select Custom to choose other roles available to your account. Choose either Edit in Architect or Activate.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-create-pipeline",
"main_header": "Initialize, Create, and Schedule a Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"h2": {
"urllink": "#dp-create-pipeline",
"text": "Initialize, Create, and Schedule a Pipeline"
},
"links": [],
"text": "To create and schedule a pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-create-pipeline",
"main_header": "Initialize, Create, and Schedule a Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"h2": {
"urllink": "#dp-choose-templates",
"text": "Choose a Template"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html",
"title": "Getting Started Using ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-runawscli.html",
"title": "Run AWS CLI Command"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html",
"title": "Export DynamoDB Table to S3"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html",
"title": "Import DynamoDB Backup Data from S3"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html",
"title": "Run Job on an Amazon EMR Cluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html",
"title": "Full Copy of Amazon RDS MySQL Table to Amazon S3"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
"title": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html",
"title": "Load S3 Data into Amazon RDS MySQL Table"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html",
"title": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"title": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"title": "Load Data from Amazon S3 into Amazon Redshift"
}
],
"text": "Templates\n\nGetting Started Using ShellCommandActivity\n\nRun AWS CLI Command\n\nExport DynamoDB Table to S3\n\nImport DynamoDB Backup Data from S3\n\nRun Job on an Amazon EMR Cluster\n\nFull Copy of Amazon RDS MySQL Table to Amazon S3\n\nIncremental Copy of Amazon RDS MySQL Table to Amazon S3 Load S3 Data into Amazon RDS MySQL Table\n\nFull Copy of Amazon RDS MySQL Table to Amazon Redshift Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift\n\nLoad Data from Amazon S3 into Amazon Redshift",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-choose-templates",
"main_header": "Choose a Template",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"h2": {
"urllink": "#dp-choose-templates",
"text": "Choose a Template"
},
"links": [],
"text": "Templates",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-choose-templates",
"main_header": "Choose a Template",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-gettingstartedshell",
"text": "Getting Started Using ShellCommandActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html#dp-template-gettingstartedshell",
"main_header": "Getting Started Using ShellCommandActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-gettingstartedshell",
"text": "Getting Started Using ShellCommandActivity"
},
"links": [],
"text": "ShellCommandActivity\n\nS3InputNode\n\nS3OutputNode\n\nEc2Resource",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html#dp-template-gettingstartedshell",
"main_header": "Getting Started Using ShellCommandActivity",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-runawscli",
"text": "Run AWS CLI Command"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-runawscli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-runawscli.html#dp-template-runawscli",
"main_header": "Run AWS CLI Command",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exportddbtos3",
"text": "Export DynamoDB Table to S3"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html#dp-template-exportddbtos3",
"main_header": "Export DynamoDB Table to S3",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exportddbtos3",
"text": "Export DynamoDB Table to S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"title": "DynamoDBDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "EmrActivity\n\nEmrCluster\n\nDynamoDBDataNode\n\nS3DataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html#dp-template-exportddbtos3",
"main_header": "Export DynamoDB Table to S3",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exports3toddb",
"text": "Import DynamoDB Backup Data from S3"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html#dp-template-exports3toddb",
"main_header": "Import DynamoDB Backup Data from S3",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exports3toddb",
"text": "Import DynamoDB Backup Data from S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"title": "DynamoDBDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"title": "S3PrefixNotEmpty"
}
],
"text": "EmrActivity\n\nEmrCluster\n\nDynamoDBDataNode\n\nS3DataNode\n\nS3PrefixNotEmpty",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html#dp-template-exports3toddb",
"main_header": "Import DynamoDB Backup Data from S3",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-emr",
"text": "Run Job on an Amazon EMR Cluster"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html#dp-template-emr",
"main_header": "Run Job on an Amazon EMR Cluster",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-emr",
"text": "Run Job on an Amazon EMR Cluster"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
],
"text": "EmrActivity\n\nEmrCluster",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html#dp-template-emr",
"main_header": "Run Job on an Amazon EMR Cluster",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copyrdstos3",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html#dp-template-copyrdstos3",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copyrdstos3",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "CopyActivity\n\nEc2Resource\n\nSqlDataNode\n\nS3DataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html#dp-template-copyrdstos3",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-incrementalcopyrdstos3",
"text": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html#dp-template-incrementalcopyrdstos3",
"main_header": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-incrementalcopyrdstos3",
"text": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "CopyActivity\n\nEc2Resource\n\nSqlDataNode\n\nS3DataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html#dp-template-incrementalcopyrdstos3",
"main_header": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copys3tords",
"text": "Load S3 Data into Amazon RDS MySQL Table"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html#dp-template-copys3tords",
"main_header": "Load S3 Data into Amazon RDS MySQL Table",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copys3tords",
"text": "Load S3 Data into Amazon RDS MySQL Table"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "CopyActivity\n\nEc2Resource\n\nSqlDataNode\n\nS3DataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html#dp-template-copys3tords",
"main_header": "Load S3 Data into Amazon RDS MySQL Table",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "If a distribution key is not specified, the first primary key from the Amazon RDS table is set as the distribution key.\n\nYou cannot skip a column that is present in an Amazon RDS MySQL table when you are doing a copy to Amazon Redshift.\n\n(Optional) You can provide an Amazon RDS MySQL to Amazon Redshift column data type mapping as one of the parameters in the template. If this is specified, the script uses this to create the Amazon Redshift table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "If a distribution key is not provided, a primary key on the Amazon RDS MySQL table is used.\n\nIf there are composite primary keys on the table, the first one is used as the distribution key if the distribution key is not provided. Only the first composite key is set as the primary key in the Amazon Redshift table.\n\nIf a distribution key is not provided and there is no primary key on the Amazon RDS MySQL table, the copy operation fails.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-security-groups.html",
"title": "Amazon Redshift Cluster"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html",
"title": "COPY"
},
{
"url": "http://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html",
"title": "Distribution styles"
},
{
"url": "http://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html",
"title": "examples"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html",
"title": "Sort Keys"
}
],
"text": "Amazon Redshift Cluster\n\nAmazon Redshift COPY\n\nDistribution styles and DISTKEY examples\n\nSort Keys",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [
{
"url": "http://tools.ietf.org/html/rfc3629",
"title": "RFC3629"
},
{
"url": "http://dev.mysql.com/doc/refman/5.0/en/integer-types.html",
"title": "TINYINT(1)"
}
],
"text": "Data Type Translations Between MySQL and Amazon Redshift\n\nMySQL Data Type\nAmazon Redshift Data Type \nNotes TINYINT,\nTINYINT (size)\n\nSMALLINT\n\nMySQL: -128 to 127. The maximum number of digits may be specified in parentheses.\n Amazon Redshift: INT2. Signed two-byte integer TINYINT UNSIGNED,\nTINYINT (size) UNSIGNED\n\nSMALLINT\n\nMySQL: 0 to 255 UNSIGNED. The maximum number of digits may be specified in parentheses.\nAmazon Redshift: INT2. Signed two-byte integer SMALLINT,\nSMALLINT(size)\n\nSMALLINT\n\nMySQL: -32768 to 32767 normal. The maximum number of digits may be specified in parentheses.\nAmazon Redshift: INT2. Signed two-byte integer SMALLINT UNSIGNED,\nSMALLINT(size) UNSIGNED,\n\nINTEGER\n\nMySQL: 0 to 65535 UNSIGNED*. The maximum number of digits may be specified in parentheses\nAmazon Redshift: INT4. Signed four-byte integer MEDIUMINT,\nMEDIUMINT(size)\n\nINTEGER\n\nMySQL: 388608 to 8388607. The maximum number of digits may be specified in parentheses\nAmazon Redshift: INT4. Signed four-byte integer MEDIUMINT UNSIGNED,\nMEDIUMINT(size)\nUNSIGNED INTEGER MySQL: 0 to 16777215. The maximum number of digits may be specified in parentheses\nAmazon Redshift: INT4. Signed four-byte integer INT,\nINT(size)\n\nINTEGER\n\nMySQL: 147483648 to 2147483647\nAmazon Redshift: INT4. Signed four-byte integer INT UNSIGNED,\nINT(size) UNSIGNED\n\nBIGINT\n\nMySQL: 0 to 4294967295\nAmazon Redshift: INT8. Signed eight-byte integer BIGINT\nBIGINT(size)\n\nBIGINT\n\nAmazon Redshift: INT8. Signed eight-byte integer BIGINT UNSIGNED\nBIGINT(size) UNSIGNED\n\nVARCHAR(20*4)\n\nMySQL: 0 to 18446744073709551615 \nAmazon Redshift: No native equivalent, so using char array. FLOAT\nFLOAT(size,d)\nFLOAT(size,d) UNSIGNED\n\nREAL\n\nThe maximum number of digits may be specified in the size parameter. The maximum number of digits to the right of the decimal point is specified in the d parameter.\nAmazon Redshift: FLOAT4 DOUBLE(size,d)\n\nDOUBLE PRECISION\n\nThe maximum number of digits may be specified in the size parameter. The maximum number of digits to the right of the decimal point is specified in the d parameter.\nAmazon Redshift: FLOAT8 DECIMAL(size,d)\n\nDECIMAL(size,d) A DOUBLE stored as a string, allowing for a fixed decimal point. The maximum number of digits may be specified in the size parameter. The maximum number of digits to the right of the decimal point is specified in the d parameter.\nAmazon Redshift: No native equivalent. CHAR(size)\n\nVARCHAR(size*4)\n\nHolds a fixed-length string, which can contain letters, numbers, and special characters. The fixed size is specified as the parameter in parentheses. Can store up to 255 characters.\nRight padded with spaces.\nAmazon Redshift: CHAR data type does not support multibyte character so VARCHAR is used.\nThe maximum number of bytes per character is 4 according to RFC3629, which limits the character table to U+10FFFF. VARCHAR(size)\nVARCHAR(size*4)\n\nCan store up to 255 characters.\nVARCHAR does not support the following invalid UTF-8 code points: 0xD800- 0xDFFF, (Byte sequences: ED A0 80- ED BF BF), 0xFDD0- 0xFDEF, 0xFFFE, and 0xFFFF, (Byte sequences: EF B7 90- EF B7 AF, EF BF BE, and EF BF BF) TINYTEXT\nVARCHAR(255*4)\nHolds a string with a maximum length of 255 characters TEXT\nVARCHAR(max)\n\nHolds a string with a maximum length of 65,535 characters. MEDIUMTEXT\nVARCHAR(max)\n\n0 to 16,777,215 Chars LONGTEXT\nVARCHAR(max)\n0 to 4,294,967,295 Chars BOOLEAN\nBOOL\nTINYINT(1)\n\nBOOLEAN\n\nMySQL: These types are synonyms for TINYINT(1). A value of zero is considered false. Nonzero values are considered true. BINARY[(M)]\nvarchar(255)\n\nM is 0 to 255 bytes, FIXED VARBINARY(M)\nVARCHAR(max)\n\n0 to 65,535 bytes TINYBLOB\nVARCHAR(255)\n0 to 255 bytes BLOB\nVARCHAR(max)\n\n0 to 65,535 bytes MEDIUMBLOB\nVARCHAR(max)\n\n0 to 16,777,215 bytes LONGBLOB\nVARCHAR(max)\n\n0 to 4,294,967,295 bytes ENUM\nVARCHAR(255*2)\nThe limit is not on the length of the literal enum string, but rather on the table definition for number of enum values. SET\nVARCHAR(255*2)\nLike enum. DATE\nDATE\n\n(YYYY-MM-DD)\n\"1000-01-01\" to \"9999-12-31\" TIME\nVARCHAR(10*4)\n\n(hh:mm:ss)\n\"-838:59:59\" to \"838:59:59\" DATETIME\nTIMESTAMP\n\n(YYYY-MM-DD hh:mm:ss)\n1000-01-01 00:00:00\" to \"9999-12-31 23:59:59\" TIMESTAMP\nTIMESTAMP\n\n(YYYYMMDDhhmmss)\n19700101000000 to 2037+ YEAR\nVARCHAR(4*4)\n\n(YYYY)\n1900 to 2155 column SERIAL ID generation / This attribute is not needed for an OLAP data warehouse since this column is copied.\nSERIAL keyword is not added while translating. SERIAL is in fact an entity named SEQUENCE. It exists independently on the rest of your table.\ncolumn GENERATED BY DEFAULT\nequivalent to:\nCREATE SEQUENCE name; CREATE TABLE table ( column INTEGER NOT NULL DEFAULT nextval(name) ); column BIGINT UNSIGNED NOT NULL AUTO_INCREMENT UNIQUE\n\nID generation / This attribute is not needed for OLAP data warehouse since this column is copied.\nSo SERIAL keyword is not added while translating. SERIAL is in fact an entity named SEQUENCE. It exists independently on the rest of your table.\ncolumn GENERATED BY DEFAULT\nequivalent to:\nCREATE SEQUENCE name; CREATE TABLE table ( column INTEGER NOT NULL DEFAULT nextval(name) ); ZEROFILL\nZEROFILL keyword is not added while translating.\n\nINT UNSIGNED ZEROFILL NOT NULL\nZEROFILL pads the displayed value of the field with zeros up to the display width specified in the column definition. Values longer than the display width are not truncated. Note that usage of ZEROFILL also implies UNSIGNED.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [
{
"url": "http://tools.ietf.org/html/rfc3629",
"title": "RFC3629"
},
{
"url": "http://dev.mysql.com/doc/refman/5.0/en/integer-types.html",
"title": "TINYINT(1)"
}
],
"text": "Data Type Translations Between MySQL and Amazon Redshift\n\nMySQL Data Type\nAmazon Redshift Data Type \nNotes TINYINT,\nTINYINT (size)\n\nSMALLINT\n\nMySQL: -128 to 127. The maximum number of digits may be specified in parentheses.\n Amazon Redshift: INT2. Signed two-byte integer TINYINT UNSIGNED,\nTINYINT (size) UNSIGNED\n\nSMALLINT\n\nMySQL: 0 to 255 UNSIGNED. The maximum number of digits may be specified in parentheses.\nAmazon Redshift: INT2. Signed two-byte integer SMALLINT,\nSMALLINT(size)\n\nSMALLINT\n\nMySQL: -32768 to 32767 normal. The maximum number of digits may be specified in parentheses.\nAmazon Redshift: INT2. Signed two-byte integer SMALLINT UNSIGNED,\nSMALLINT(size) UNSIGNED,\n\nINTEGER\n\nMySQL: 0 to 65535 UNSIGNED*. The maximum number of digits may be specified in parentheses\nAmazon Redshift: INT4. Signed four-byte integer MEDIUMINT,\nMEDIUMINT(size)\n\nINTEGER\n\nMySQL: 388608 to 8388607. The maximum number of digits may be specified in parentheses\nAmazon Redshift: INT4. Signed four-byte integer MEDIUMINT UNSIGNED,\nMEDIUMINT(size)\nUNSIGNED INTEGER MySQL: 0 to 16777215. The maximum number of digits may be specified in parentheses\nAmazon Redshift: INT4. Signed four-byte integer INT,\nINT(size)\n\nINTEGER\n\nMySQL: 147483648 to 2147483647\nAmazon Redshift: INT4. Signed four-byte integer INT UNSIGNED,\nINT(size) UNSIGNED\n\nBIGINT\n\nMySQL: 0 to 4294967295\nAmazon Redshift: INT8. Signed eight-byte integer BIGINT\nBIGINT(size)\n\nBIGINT\n\nAmazon Redshift: INT8. Signed eight-byte integer BIGINT UNSIGNED\nBIGINT(size) UNSIGNED\n\nVARCHAR(20*4)\n\nMySQL: 0 to 18446744073709551615 \nAmazon Redshift: No native equivalent, so using char array. FLOAT\nFLOAT(size,d)\nFLOAT(size,d) UNSIGNED\n\nREAL\n\nThe maximum number of digits may be specified in the size parameter. The maximum number of digits to the right of the decimal point is specified in the d parameter.\nAmazon Redshift: FLOAT4 DOUBLE(size,d)\n\nDOUBLE PRECISION\n\nThe maximum number of digits may be specified in the size parameter. The maximum number of digits to the right of the decimal point is specified in the d parameter.\nAmazon Redshift: FLOAT8 DECIMAL(size,d)\n\nDECIMAL(size,d) A DOUBLE stored as a string, allowing for a fixed decimal point. The maximum number of digits may be specified in the size parameter. The maximum number of digits to the right of the decimal point is specified in the d parameter.\nAmazon Redshift: No native equivalent. CHAR(size)\n\nVARCHAR(size*4)\n\nHolds a fixed-length string, which can contain letters, numbers, and special characters. The fixed size is specified as the parameter in parentheses. Can store up to 255 characters.\nRight padded with spaces.\nAmazon Redshift: CHAR data type does not support multibyte character so VARCHAR is used.\nThe maximum number of bytes per character is 4 according to RFC3629, which limits the character table to U+10FFFF. VARCHAR(size)\nVARCHAR(size*4)\n\nCan store up to 255 characters.\nVARCHAR does not support the following invalid UTF-8 code points: 0xD800- 0xDFFF, (Byte sequences: ED A0 80- ED BF BF), 0xFDD0- 0xFDEF, 0xFFFE, and 0xFFFF, (Byte sequences: EF B7 90- EF B7 AF, EF BF BE, and EF BF BF) TINYTEXT\nVARCHAR(255*4)\nHolds a string with a maximum length of 255 characters TEXT\nVARCHAR(max)\n\nHolds a string with a maximum length of 65,535 characters. MEDIUMTEXT\nVARCHAR(max)\n\n0 to 16,777,215 Chars LONGTEXT\nVARCHAR(max)\n0 to 4,294,967,295 Chars BOOLEAN\nBOOL\nTINYINT(1)\n\nBOOLEAN\n\nMySQL: These types are synonyms for TINYINT(1). A value of zero is considered false. Nonzero values are considered true. BINARY[(M)]\nvarchar(255)\n\nM is 0 to 255 bytes, FIXED VARBINARY(M)\nVARCHAR(max)\n\n0 to 65,535 bytes TINYBLOB\nVARCHAR(255)\n0 to 255 bytes BLOB\nVARCHAR(max)\n\n0 to 65,535 bytes MEDIUMBLOB\nVARCHAR(max)\n\n0 to 16,777,215 bytes LONGBLOB\nVARCHAR(max)\n\n0 to 4,294,967,295 bytes ENUM\nVARCHAR(255*2)\nThe limit is not on the length of the literal enum string, but rather on the table definition for number of enum values. SET\nVARCHAR(255*2)\nLike enum. DATE\nDATE\n\n(YYYY-MM-DD)\n\"1000-01-01\" to \"9999-12-31\" TIME\nVARCHAR(10*4)\n\n(hh:mm:ss)\n\"-838:59:59\" to \"838:59:59\" DATETIME\nTIMESTAMP\n\n(YYYY-MM-DD hh:mm:ss)\n1000-01-01 00:00:00\" to \"9999-12-31 23:59:59\" TIMESTAMP\nTIMESTAMP\n\n(YYYYMMDDhhmmss)\n19700101000000 to 2037+ YEAR\nVARCHAR(4*4)\n\n(YYYY)\n1900 to 2155 column SERIAL ID generation / This attribute is not needed for an OLAP data warehouse since this column is copied.\nSERIAL keyword is not added while translating. SERIAL is in fact an entity named SEQUENCE. It exists independently on the rest of your table.\ncolumn GENERATED BY DEFAULT\nequivalent to:\nCREATE SEQUENCE name; CREATE TABLE table ( column INTEGER NOT NULL DEFAULT nextval(name) ); column BIGINT UNSIGNED NOT NULL AUTO_INCREMENT UNIQUE\n\nID generation / This attribute is not needed for OLAP data warehouse since this column is copied.\nSo SERIAL keyword is not added while translating. SERIAL is in fact an entity named SEQUENCE. It exists independently on the rest of your table.\ncolumn GENERATED BY DEFAULT\nequivalent to:\nCREATE SEQUENCE name; CREATE TABLE table ( column INTEGER NOT NULL DEFAULT nextval(name) ); ZEROFILL\nZEROFILL keyword is not added while translating.\n\nINT UNSIGNED ZEROFILL NOT NULL\nZEROFILL pads the displayed value of the field with zeros up to the display width specified in the column definition. Values longer than the display width are not truncated. Note that usage of ZEROFILL also implies UNSIGNED.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsfull",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html#dp-template-redshiftrdsfull",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsfull",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
}
],
"text": "CopyActivity\n\nRedshiftCopyActivity\n\nS3DataNode\n\nSqlDataNode\n\nRedshiftDataNode\n\nRedshiftDatabase",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html#dp-template-redshiftrdsfull",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
}
],
"text": "CopyActivity\n\nRedshiftCopyActivity\n\nS3DataNode\n\nSqlDataNode\n\nRedshiftDataNode\n\nRedshiftDatabase",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-s3redshift",
"text": "Load Data from Amazon S3 into Amazon Redshift"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html#dp-template-s3redshift",
"main_header": "Load Data from Amazon S3 into Amazon Redshift",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-s3redshift",
"text": "Load Data from Amazon S3 into Amazon Redshift"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
}
],
"text": "CopyActivity\n\nRedshiftCopyActivity\n\nS3DataNode\n\nRedshiftDataNode\n\nRedshiftDatabase\n\nEc2Resource",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html#dp-template-s3redshift",
"main_header": "Load Data from Amazon S3 into Amazon Redshift",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#dp-custom-templates",
"main_header": "Creating a Pipeline Using Parametrized Templates",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables",
"title": "Add myVariables to the Pipeline Definition"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"title": "Define Parameter Objects"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-values",
"title": "Define Parameter Values"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#submit-pipeline-definition",
"title": "Submitting the Pipeline Definition"
}
],
"text": "ContentsAdd myVariables to the Pipeline DefinitionDefine Parameter ObjectsDefine Parameter ValuesSubmitting the Pipeline Definition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#dp-custom-templates",
"main_header": "Creating a Pipeline Using Parametrized Templates",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#dp-custom-templates",
"main_header": "Creating a Pipeline Using Parametrized Templates",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#add-pipeline-variables",
"text": "Add myVariables to the Pipeline Definition"
},
"links": [],
"text": "NoteA pipeline definition has an upper limit of 50 parameters.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables",
"main_header": "Add myVariables to the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#add-pipeline-variables",
"text": "Add myVariables to the Pipeline Definition"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables",
"main_header": "Add myVariables to the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#add-pipeline-variables",
"text": "Add myVariables to the Pipeline Definition"
},
"links": [],
"text": "A pipeline definition has an upper limit of 50 parameters.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables",
"main_header": "Add myVariables to the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#add-pipeline-variables",
"text": "Add myVariables to the Pipeline Definition"
},
"links": [],
"text": "A pipeline definition has an upper limit of 50 parameters.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables",
"main_header": "Add myVariables to the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "NoteYou could add these objects directly to the pipeline definition file instead of using a separate file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "You could add these objects directly to the pipeline definition file instead of using a separate file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "You could add these objects directly to the pipeline definition file instead of using a separate file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "Parameter Attributes\n\nAttribute\nType\nDescription id\nString\nThe unique identifier of the parameter. To mask the value while it is typed or displayed, add an asterisk ('*') as a prefix. For example, *myVariable\u00e2\u0080\u0094. Notes that this also encrypts the value before it is stored by AWS Data Pipeline. description\nString\nA description of the parameter. type\nString, Integer, Double, or AWS::S3::ObjectKey\nThe parameter type that defines the allowed range of input values and validation rules. The default is String. optional\nBoolean\nIndicates whether the parameter is optional or required. The default is false. allowedValues\nList of Strings\nEnumerates all permitted values for the parameter. default\nString\nThe default value for the parameter. If you specify a value for this parameter using parameter values, it overrides the default value. isArray\nBoolean\nIndicates whether the parameter is an array.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "Parameter Attributes\n\nAttribute\nType\nDescription id\nString\nThe unique identifier of the parameter. To mask the value while it is typed or displayed, add an asterisk ('*') as a prefix. For example, *myVariable\u00e2\u0080\u0094. Notes that this also encrypts the value before it is stored by AWS Data Pipeline. description\nString\nA description of the parameter. type\nString, Integer, Double, or AWS::S3::ObjectKey\nThe parameter type that defines the allowed range of input values and validation rules. The default is String. optional\nBoolean\nIndicates whether the parameter is optional or required. The default is false. allowedValues\nList of Strings\nEnumerates all permitted values for the parameter. default\nString\nThe default value for the parameter. If you specify a value for this parameter using parameter values, it overrides the default value. isArray\nBoolean\nIndicates whether the parameter is an array.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#submit-pipeline-definition",
"text": "Submitting the Pipeline Definition"
},
"links": [],
"text": "NoteA pipeline definition has an upper limit of 50 parameters. The size of the file for parameter-values-uri has an upper limit of 15 KB.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#submit-pipeline-definition",
"main_header": "Submitting the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#submit-pipeline-definition",
"text": "Submitting the Pipeline Definition"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#submit-pipeline-definition",
"main_header": "Submitting the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#submit-pipeline-definition",
"text": "Submitting the Pipeline Definition"
},
"links": [],
"text": "A pipeline definition has an upper limit of 50 parameters. The size of the file for parameter-values-uri has an upper limit of 15 KB.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#submit-pipeline-definition",
"main_header": "Submitting the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#submit-pipeline-definition",
"text": "Submitting the Pipeline Definition"
},
"links": [],
"text": "A pipeline definition has an upper limit of 50 parameters. The size of the file for parameter-values-uri has an upper limit of 15 KB.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#submit-pipeline-definition",
"main_header": "Submitting the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual",
"main_header": "Creating Pipelines Using the Console Manually",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-create",
"title": "Create the Pipeline Definition"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activity",
"title": "Define an Activity Using the AWS Data Pipeline Architect"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-validate",
"title": "Validate and Save the Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"title": "Activate the Pipeline"
}
],
"text": "TasksCreate the Pipeline DefinitionDefine an Activity Using the AWS Data Pipeline ArchitectValidate and Save the PipelineActivate the Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual",
"main_header": "Creating Pipelines Using the Console Manually",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual",
"main_header": "Creating Pipelines Using the Console Manually",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-create",
"text": "Create the Pipeline Definition"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"title": "Creating Pipelines Using Console Templates"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"title": "Pipeline Definition File Syntax"
}
],
"text": "To create your pipeline definition\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nChoose Get started now (if this is your first pipeline) or Create new pipeline.\n\nIn Name, enter a name for the pipeline (for example, CopyMyS3Data).\n\nIn Description, enter a description.\n\nChoose a Source for your pipeline defintion. For this walkthrough, choose Build using architect to use the AWS Data Pipeline Architect to design the pipeline. For more information about the Build using a template option, see Creating Pipelines Using Console Templates. For more information about the Import a definition option to specify a pipeline definition file in Amazon S3 or locally, see Pipeline Definition File Syntax. Under Schedule, leave the default selections.\n\nUnder Pipeline Configuration, leave Logging enabled and enter a location in Amazon S3 where log files are saved.\n\nUnder Security/Access, leave Default selected for IAM roles.\nAlternatively, if you created your own IAM roles, choose Custom and then select your roles for the Pipeline role and EC2 instance role.\n\nOptionally, under Tags enter tag keys and values to help you identify and categorize the pipeline.\n\nChoose Edit in Architect.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-create",
"main_header": "Create the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-create",
"text": "Create the Pipeline Definition"
},
"links": [],
"text": "To create your pipeline definition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-create",
"main_header": "Create the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"links": [],
"text": "To define a copy activity\nSelect Add, CopyActivity. Under Activies, fields appear for configuring the properties and resources for the copy activity.\n\nUnder Activities, configure the activity according to the following guidelines: For this parameter...\nDo this... Name Enter a name to help you identify the activity, for example, copy-myS3-data. Type This is configured by default to CopyActivity based on your earlier selection to add a CopyActivity. Leave the default. Input Select Create new: DataNode from the list. A new data node with a default name of DefaultDataNode1 is created. This is the source data node from which data is copied. You configure the details of this data node later. If you have an existing data node, you can select that. Output Select Create new: DataNode from the list. A new data node with a default name of DefaultDataNode2 is created. This is the destination data node to which data is copied. You configure the details of this data node later. If you have an existing data node, you can select that. Schedule Select Create new: Schedule from the list. A new schedule with a default name of DefaultSchedule1 is created. This schedule determines when the pipeline runs. You configure the details of this schedule later. If you have an existing schedule, you can select that. Add an optional field... Select RunsOn from the list.\nAn empty list appears for a new Runs On selection.\nFrom the blank list, select Create new: Resource. A resource with a default name of DefaultResource1 is created. This is the AWS resource that the pipeline uses to run the activity. You configure the details of the resource later. If you have an existing resource, you can select that. The left pane graphically depicts the activity you configured. You can choose any of the pipeline components in this pane or expand each section in the right pane to view details and perform the following configuration tasks.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activity",
"main_header": "Define an Activity Using the AWS Data Pipeline Architect",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"links": [],
"text": "To define a copy activity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activity",
"main_header": "Define an Activity Using the AWS Data Pipeline Architect",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-schedule",
"text": "Configure the Schedule"
},
"links": [],
"text": "To configure the date and time for your pipeline to run\nOn the pipeline page, in the right pane, choose Schedules.\n\nEnter a schedule name for this activity, for example, copy-myS3-data-schedule.\n\nIn Start Date Time, select the date from the calendar, and then enter the time to start the activity.\n\nIn Period, enter the duration for the activity (for example, 1), and then select the period category (for example, Days).\n\n(Optional) To specify the date and time to end the activity, in Add an optional field, select End Date Time, and enter the date and time.\nTo get your pipeline to launch immediately, set Start Date Time to a date one day in the past. AWS Data Pipeline then starts launching the \"past due\" runs immediately in an attempt to address what it perceives as a backlog of work. This backfilling means that you don't have to wait an hour to see AWS Data Pipeline launch its first cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-schedule",
"main_header": "Configure the Schedule",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-schedule",
"text": "Configure the Schedule"
},
"links": [],
"text": "To configure the date and time for your pipeline to run",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-schedule",
"main_header": "Configure the Schedule",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-data",
"text": "Configure Data Nodes"
},
"links": [],
"text": "To configure the input and output data nodes\nOn the pipeline page, in the right pane, choose DataNodes or choose the individual data node from the workflow in the left pane.\n\nConfigure each data node according to the following guidelines. For this parameter...\nDo this... Name Enter a name that helps you identify this node's purpose. For example, replace DefaultDataNode1 with S3LocationForCopyActivityInput and DefaultDataNode2 with S3LocationForCopyActivityOutput. Type Select S3DataNode. Schedule Select the schedule that you configured in the previous step. Add an optional field... Select File Path from the list.\nAn empty list appears for a new File Path selection.\nEnter an existing file path in Amazon S3 appropriate for the data node that you're configuring. For example, if you are configuring the data node specified as the Input data node for the copy activity, you might enter s3://mybucket/myinputdata; if you are configuring the Output data node, you might enter s3://mybucket/mycopy.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-data",
"main_header": "Configure Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-data",
"text": "Configure Data Nodes"
},
"links": [],
"text": "To configure the input and output data nodes",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-data",
"main_header": "Configure Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-resources",
"text": "Configure Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
"title": "IAM Roles"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
"title": "IAM Roles"
}
],
"text": "To configure an EC2 instance as the resource for your pipeline copy activity\nOn the pipeline page, in the right pane, choose Resources.\n\nConfigure the resource according to the following guidelines. For this parameter...\nDo this... Name Enter a name for the resource that help you identify it, for example, Ec2InstanceForCopyActivity. Type Select Ec2Resource. Resource Role Leave the default DataPipelineDefaultResource selected, or select a custom IAM role. For more information, see IAM Roles for AWS Data Pipeline and IAM Roles in the IAM User Guide. Schedule Make sure that the schedule that you created above is selected. Role Leave the default DataPipelineDefaultRole selected, or select a custom IAM role. For more information, see IAM Roles for AWS Data Pipeline and IAM Roles in the IAM User Guide. Add an optional field... Choose Subnet ID from the list.\nAn new, empty field Subnet Id appears.\nEnter the subnet ID of a subnet in the VPC your are using, for example, subnet-1a2bcd34.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-resources",
"main_header": "Configure Resources",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-resources",
"text": "Configure Resources"
},
"links": [],
"text": "To configure an EC2 instance as the resource for your pipeline copy activity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-resources",
"main_header": "Configure Resources",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-validate",
"text": "Validate and Save the Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-validate",
"main_header": "Validate and Save the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-validate",
"text": "Validate and Save the Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-validate",
"main_header": "Validate and Save the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-list-pipelines",
"text": "Viewing Your Pipelines"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html#dp-list-pipelines",
"main_header": "Viewing Your Pipelines",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-list-pipelines",
"text": "Viewing Your Pipelines"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"title": "Interpreting Pipeline and Component Health State"
}
],
"text": "To view your pipelines using the consoleOpen the AWS Data Pipeline console. If you have created any pipelines in that region, the console displays them in a list. Otherwise, you see a welcome screen. To view information about a pipeline, expand the arrow. The console displays information about the schedule, activities, and tags for the pipeline. For more information about the health status, see Interpreting Pipeline and Component Health State.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html#dp-list-pipelines",
"main_header": "Viewing Your Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-list-pipelines",
"text": "Viewing Your Pipelines"
},
"links": [],
"text": "To view your pipelines using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html#dp-list-pipelines",
"main_header": "Viewing Your Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-list-pipelines",
"text": "Viewing Your Pipelines"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/list-pipelines.html",
"title": "list-pipelines"
}
],
"text": "To view your pipelines using the AWS CLIUse the following list-pipelines command to list your pipelines:\naws datapipeline list-pipelines",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html#dp-list-pipelines",
"main_header": "Viewing Your Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-list-pipelines",
"text": "Viewing Your Pipelines"
},
"links": [],
"text": "To view your pipelines using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html#dp-list-pipelines",
"main_header": "Viewing Your Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-interpret-status",
"text": "Interpreting Pipeline Status Codes"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html#dp-interpret-status",
"main_header": "Interpreting Pipeline Status Codes",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-status",
"text": "Interpreting Pipeline Status Codes"
},
"links": [],
"text": "Status Codes\n\nACTIVATING\n\nThe component or resource is being started, such as an EC2 instance.\n\nCANCELED\n\nThe component was canceled by a user or AWS Data Pipeline before it could run. This can happen automatically when a failure occurs in a different component or resource that this component depends on.\n\nCASCADE_FAILED\n\nThe component or resource was canceled as a result of a cascade failure from one of its dependencies, but the component was probably not the original source of the failure.\n\nDEACTIVATING\n\nThe pipeline is being deactivated.\n\nFAILED\n\nThe component or resource encountered an error and stopped working. When a component or resource fails, it can cause cancelations and failures to cascade to other components that depend on it.\n\nFINISHED\n\nThe component completed its assigned work.\n\nINACTIVE\n\nThe pipeline was deactivated.\n\nPAUSED\n\nThe component was paused and is not currently performing its work.\n\nPENDING\n\nThe pipeline is ready to be activated for the first time.\n\nRUNNING\n\nThe resource is running and ready to receive work.\n\nSCHEDULED\n\nThe resource is scheduled to run.\n\nSHUTTING_DOWN\n\nThe resource is shutting down after successfully completing its work.\n\nSKIPPED\n\nThe component skipped intervals of execution after the pipeline was activated using a time stamp that is later than the current schedule.\n\nTIMEDOUT\n\nThe resource exceeded the terminateAfter threshold and was stopped by AWS Data Pipeline. After the resource reaches this status, AWS Data Pipeline ignores the actionOnResourceFailure, retryDelay, and retryTimeout values for that resource. This status applies only to resources.\n\nVALIDATING\n\nThe pipeline definition is being validated by AWS Data Pipeline.\n\nWAITING_FOR_RUNNER\n\nThe component is waiting for its worker client to retrieve a work item. The component and worker client relationship is controlled by the runsOn or workerGroup fields defined by that component.\n\nWAITING_ON_DEPENDENCIES\n\nThe component is verifying that its default and user-configured preconditions are met before performing its work.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html#dp-interpret-status",
"main_header": "Interpreting Pipeline Status Codes",
"images": [],
"container_type": "div",
"children_tags": [
"dl",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-status",
"text": "Interpreting Pipeline Status Codes"
},
"links": [],
"text": "Status Codes",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html#dp-interpret-status",
"main_header": "Interpreting Pipeline Status Codes",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "Pipeline Health States\n\nHEALTHY\n\nThe aggregate health status of all components is HEALTHY. This means at least one component must have successfully completed. You can click on the HEALTHY status to see the most recent successfully completed pipeline component instance on the Execution Details page.\n\nERROR\n\nAt least one component in the pipeline has a health status of ERROR. You can click on the ERROR status to see the most recent failed pipeline component instance on the Execution Details page.\n\nNo Completed Executions or No Health Information Available.\n\nNo health status was reported for this pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "div",
"children_tags": [
"dl",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "Pipeline Health States",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "NoteWhile components update their health status almost immediately, it may take up to five minutes for a pipeline health status to update.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "While components update their health status almost immediately, it may take up to five minutes for a pipeline health status to update.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "While components update their health status almost immediately, it may take up to five minutes for a pipeline health status to update.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "Component Health States\n\nHEALTHY\n\nA component (Activity or DataNode) has a health status of HEALTHY if it has completed a successful execution where it was marked with a status of FINISHED or MARK_FINISHED. You can click on the name of the component or the HEALTHY status to see the most recent successfully completed pipeline component instances on the Execution Details page.\n\nERROR\n\nAn error occurred at the component level or one of its preconditions failed. Statuses of FAILED, TIMEOUT, or CANCELED trigger this error. You can click on the name of the component or the ERROR status to see the most recent failed pipeline component instance on the Execution Details page.\n\nNo Completed Executions or No Health Information Available\n\nNo health status was reported for this component.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "div",
"children_tags": [
"dl",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "Component Health States",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "To view a pipeline definition using the console\nOn the List Pipelines page, choose the Pipeline ID for the desired pipeline. Then choose Edit Pipeline to display the pipeline Architect page.\n\nOn the pipeline Architect page, click the object icons in the design pane to expand the corresponding section in the right pane.\nAlternatively, expand one of the sections in the right pane to view its objects and their associated fields.\n\nIf your pipeline definition graph does not fit in the design pane, use the pan buttons on the right side of the design pane to slide the canvas. You can also view the entire text pipeline definition by clicking Export. A dialog appears with the JSON pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "To view a pipeline definition using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "To monitor the progress of a pipeline using the console\nOn the List Pipelines page, in the Pipeline ID column, click the arrow for your pipeline and click View execution details.\n\nThe Execution details page lists the name, type, status, and schedule information of each component.\nYou can then click on the arrow for each component name to view dependency information for that component. In the inline summary, you can view instance details, re-run an activity, mark it as FINISHED, or explore the dependency chain. NoteIf you do not see runs listed, check when your pipeline was scheduled. Either change End (in UTC) to a later date or change Start (in UTC) an earlier date, and then click Update.\n\nIf the Status column of all components in your pipeline is FINISHED, your pipeline has successfully completed the activity. You should receive an email about the successful completion of this task, to the account that you specified to receive Amazon SNS notifications.\nYou can also check the content of your output data node.\n\nIf the Status column of any component in your pipeline is not FINISHED, either your pipeline is waiting for some dependency or it has failed. To troubleshoot failed or the incomplete instance runs, use the following procedure.\n\nClick the triangle next to a component or activity.\nIf the status of the instance is FAILED, the Attempts box has an Error Message indicating the reason for failure under the latest attempt. For example, Status Code: 403, AWS Service: Amazon S3, AWS Request ID: 1A3456789ABCD, AWS Error Code: null, AWS Error Message: Forbidden. You can also click on More... in the Details column to view the instance details of this attempt.\n\nTo take action on your incomplete or failed component, choose Rerun, Mark Finished, or Cancel.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "To monitor the progress of a pipeline using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-logging-configured.png",
"alt": "\n                    Instance summary pane\n                "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-logs-hadoopactivity.png",
"alt": "\n                    Instance summary pane\n                "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-manage-pipeline-modify-console",
"main_header": "Editing Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-limits",
"title": "Limitations"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-console",
"title": "Editing a Pipeline Using the Console"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"title": "Editing a Pipeline Using the AWS CLI"
}
],
"text": "ContentsLimitationsEditing a Pipeline Using the ConsoleEditing a Pipeline Using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-manage-pipeline-modify-console",
"main_header": "Editing Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-manage-pipeline-modify-console",
"main_header": "Editing Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-limits",
"text": "Limitations"
},
"links": [],
"text": "You can't remove an object\n\nYou can't change the schedule period of an existing object\n\nYou can't add, delete, or modify reference fields in an existing object\n\nYou can't reference an existing object in an output field of a new object\n\nYou can't change the scheduled start date of an object (instead, activate the pipeline with a specific date and time)",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-limits",
"main_header": "Limitations",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-console",
"text": "Editing a Pipeline Using the Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"title": "Cloning Your Pipeline"
}
],
"text": "To edit a pipeline using the console\nOn the List Pipelines page, check the Pipeline ID and Name columns for your pipeline, and then choose your Pipeline ID. Then choose Edit Pipeline to display the pipeline Architect page.\n\nTo complete or modify your pipeline definition:\n\nOn the pipeline (Architect) page, click the object panes in the right pane and finish defining the objects and fields of your pipeline definition. If you are modifying an active pipeline, some fields are grayed out and can't be modified. It might be easier to clone the pipeline and edit the copy, depending on the changes you need to make. For more information, see Cloning Your Pipeline.\n\nClick Save pipeline. If there are validation errors, fix them and save the pipeline again. After you've saved your pipeline definition with no validation errors, click Activate.\n\nIn the List Pipelines page, check whether your newly created pipeline is listed and the Schedule State column displays SCHEDULED.\n\nAfter editing an active pipeline, you might decide to rerun one or more pipeline components.\nOn the List Pipelines page, in the detail dropdown of your pipeline, click View execution details.\n\nOn the Execution details page, choose a pipeline component dropdown from the list to view the details for a component.\n\nClick Rerun.\n\nAt the confirmation prompt, click Continue.\nThe changed pipeline component and any dependencies change status. For example, resources change to the CREATING status and activities change to the WAITING_FOR_RUNNER status.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-console",
"main_header": "Editing a Pipeline Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-console",
"text": "Editing a Pipeline Using the Console"
},
"links": [],
"text": "To edit a pipeline using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-console",
"main_header": "Editing a Pipeline Using the Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "NoteYou can't clone a pipeline using the command line interface (CLI).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "You can't clone a pipeline using the command line interface (CLI).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "You can't clone a pipeline using the command line interface (CLI).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "To clone a pipeline using the console\nIn the List Pipelines page, select the pipeline to clone.\n\nClick Actions, and then click Clone.\n\nIn the Clone a Pipeline dialog box, enter a name for the new pipeline and click Clone.\n\nIn the Schedule pane, specify a schedule for the new pipeline.\n\nTo activate the new pipeline, click Actions, and then click Activate.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "To clone a pipeline using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To tag your pipeline using the console\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nOn the List Pipelines page, in the Pipeline ID column, click the expand arrow next to your pipeline, and then click View all/Edit under Tags.\n\nIn the View all / Edit dialog box, do the following:\n\nSpecify a key and a value for each tag that you'd like to add.\n\nClick the remove icon of any tags that you'd like to remove.\n\nClick Save.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [],
"text": "To tag your pipeline using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline",
"main_header": "Deactivating Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"title": "Deactivate Your Pipeline Using the Console"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-cli",
"title": "Deactivate Your Pipeline Using the AWS CLI"
}
],
"text": "ContentsDeactivate Your Pipeline Using the ConsoleDeactivate Your Pipeline Using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline",
"main_header": "Deactivating Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline",
"main_header": "Deactivating Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-console",
"text": "Deactivate Your Pipeline Using the Console"
},
"links": [],
"text": "To deactivate a pipeline\nIn the List Pipelines page, select the pipeline to deactivate.\n\nClick Actions, and then click Deactivate.\n\nIn the Deactivate a Pipeline dialog box, select an option, and then click Deactivate.\n\nWhen prompted for confirmation, click Deactivate.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"main_header": "Deactivate Your Pipeline Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-console",
"text": "Deactivate Your Pipeline Using the Console"
},
"links": [],
"text": "To deactivate a pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"main_header": "Deactivate Your Pipeline Using the Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-console",
"text": "Deactivate Your Pipeline Using the Console"
},
"links": [],
"text": "To activate a pipeline\nIn the List Pipelines page, select the pipeline to activate.\n\nClick Actions, and then click Activate.\n\nIn the Activate a Pipeline dialog box, select an option, and then choose Activate.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"main_header": "Deactivate Your Pipeline Using the Console",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-console",
"text": "Deactivate Your Pipeline Using the Console"
},
"links": [],
"text": "To activate a pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"main_header": "Deactivate Your Pipeline Using the Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "ImportantYou can't restore a pipeline after you delete it, so be sure that you won't need the pipeline in the future before you delete it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "You can't restore a pipeline after you delete it, so be sure that you won't need the pipeline in the future before you delete it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "You can't restore a pipeline after you delete it, so be sure that you won't need the pipeline in the future before you delete it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "To delete a pipeline using the console\nIn the List Pipelines page, select the pipeline.\n\nClick Actions, and then click Delete.\n\nWhen prompted for confirmation, click Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "To delete a pipeline using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "Data staging with ShellCommandActivity\n\nTable staging with Hive and staging-supported data nodes\n\nTable staging with Hive and staging-unsupported data nodes",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
}
],
"text": "NoteStaging only functions when the stage field is set to true on an activity, such as ShellCommandActivity. For more information, see ShellCommandActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
}
],
"text": "Staging only functions when the stage field is set to true on an activity, such as ShellCommandActivity. For more information, see ShellCommandActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
}
],
"text": "Staging only functions when the stage field is set to true on an activity, such as ShellCommandActivity. For more information, see ShellCommandActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "Staging data locally on a resource\n\nThe input data automatically copies into the resource local file system. Output data automatically copies from the resource local file system to the output data node. For example, when you configure ShellCommandActivity inputs and outputs with staging = true, the input data is available as INPUTx_STAGING_DIR and output data is available as OUTPUTx_STAGING_DIR, where x is the number of input or output.\n\nStaging input and output definitions for an activity\n\nThe input data format (column names and table names) automatically copies into the activity's resource. For example, when you configure HiveActivity with staging = true. The data format specified on the input S3DataNode is used to stage the table definition from the Hive table.\n\nStaging not enabled\n\nThe input and output objects and their fields are available for the activity, but the data itself is not. For example, EmrActivity by default or when you configure other activities with staging = false. In this configuration, the data fields are available for the activity to make a reference to them using the AWS Data Pipeline expression syntax, and this only occurs when the dependency is satisfied. This serves as dependency checking only. Code in the activity is responsible for copying the data from the input to the resource running the activity.\n\nDependency relationship between objects\n\nThere is a depends-on relationship between two objects, which results in a similar situation to when staging is not enabled. This causes a data node or activity to act as a precondition for the execution of another activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-datastaging",
"text": "Data Staging with ShellCommandActivity"
},
"links": [],
"text": "Note This scenario only works as described if your data inputs and outputs are S3DataNode objects. Additionally, output data staging is allowed only when directoryPath is set on the output S3DataNode object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-datastaging",
"main_header": "Data Staging with ShellCommandActivity",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-datastaging",
"text": "Data Staging with ShellCommandActivity"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-datastaging",
"main_header": "Data Staging with ShellCommandActivity",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-datastaging",
"text": "Data Staging with ShellCommandActivity"
},
"links": [],
"text": "This scenario only works as described if your data inputs and outputs are S3DataNode objects. Additionally, output data staging is allowed only when directoryPath is set on the output S3DataNode object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-datastaging",
"main_header": "Data Staging with ShellCommandActivity",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-datastaging",
"text": "Data Staging with ShellCommandActivity"
},
"links": [],
"text": "This scenario only works as described if your data inputs and outputs are S3DataNode objects. Additionally, output data staging is allowed only when directoryPath is set on the output S3DataNode object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-datastaging",
"main_header": "Data Staging with ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-tablestaging",
"text": "Table Staging with Hive and Staging-Supported Data Nodes"
},
"links": [],
"text": "Note This scenario only works as described if your data inputs and outputs are S3DataNode or MySqlDataNode objects. Table staging is not supported for DynamoDBDataNode.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-tablestaging",
"main_header": "Table Staging with Hive and Staging-Supported Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-tablestaging",
"text": "Table Staging with Hive and Staging-Supported Data Nodes"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-tablestaging",
"main_header": "Table Staging with Hive and Staging-Supported Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-tablestaging",
"text": "Table Staging with Hive and Staging-Supported Data Nodes"
},
"links": [],
"text": "This scenario only works as described if your data inputs and outputs are S3DataNode or MySqlDataNode objects. Table staging is not supported for DynamoDBDataNode.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-tablestaging",
"main_header": "Table Staging with Hive and Staging-Supported Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-tablestaging",
"text": "Table Staging with Hive and Staging-Supported Data Nodes"
},
"links": [],
"text": "This scenario only works as described if your data inputs and outputs are S3DataNode or MySqlDataNode objects. Table staging is not supported for DynamoDBDataNode.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-tablestaging",
"main_header": "Table Staging with Hive and Staging-Supported Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-nostaging",
"text": "Table Staging with Hive and Staging-Unsupported Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"title": "Expression Evaluation"
}
],
"text": "NoteIn this example, the table name variable has the # (hash) character prefix because AWS Data Pipeline uses expressions to access the tableName or directoryPath. For more information about how expression evaluation works in AWS Data Pipeline, see Expression Evaluation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-nostaging",
"main_header": "Table Staging with Hive and Staging-Unsupported Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-nostaging",
"text": "Table Staging with Hive and Staging-Unsupported Data Nodes"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-nostaging",
"main_header": "Table Staging with Hive and Staging-Unsupported Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-nostaging",
"text": "Table Staging with Hive and Staging-Unsupported Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"title": "Expression Evaluation"
}
],
"text": "In this example, the table name variable has the # (hash) character prefix because AWS Data Pipeline uses expressions to access the tableName or directoryPath. For more information about how expression evaluation works in AWS Data Pipeline, see Expression Evaluation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-nostaging",
"main_header": "Table Staging with Hive and Staging-Unsupported Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-nostaging",
"text": "Table Staging with Hive and Staging-Unsupported Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"title": "Expression Evaluation"
}
],
"text": "In this example, the table name variable has the # (hash) character prefix because AWS Data Pipeline uses expressions to access the tableName or directoryPath. For more information about how expression evaluation works in AWS Data Pipeline, see Expression Evaluation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-nostaging",
"main_header": "Table Staging with Hive and Staging-Unsupported Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "ImportantIf your AWS account was created before December 4, 2013, you might have the option to create EC2Resource objects for a pipeline in an EC2-Classic network rather than a VPC. We strongly recommend that you create resources for all your pipelines in VPCs. In addition, if you have existing resources in EC2-Classic, we strongly recommend that you migrate them to a VPC. For more information, see Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "If your AWS account was created before December 4, 2013, you might have the option to create EC2Resource objects for a pipeline in an EC2-Classic network rather than a VPC. We strongly recommend that you create resources for all your pipelines in VPCs. In addition, if you have existing resources in EC2-Classic, we strongly recommend that you migrate them to a VPC. For more information, see Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "If your AWS account was created before December 4, 2013, you might have the option to create EC2Resource objects for a pipeline in an EC2-Classic network rather than a VPC. We strongly recommend that you create resources for all your pipelines in VPCs. In addition, if you have existing resources in EC2-Classic, we strongly recommend that you migrate them to a VPC. For more information, see Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [],
"text": "First, create a VPC and subnets using Amazon VPC. Configure the VPC so that instances in the VPC can access AWS Data Pipeline endpoint and Amazon S3.\n\nNext, set up a security group that grants Task Runner access to your data sources.\n\nFinally, specify a subnet from the VPC when you configure your instances and clusters and when you create your data sources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-create-vpc",
"title": "Create and Configure a VPC"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"title": "Set up Connectivity Between Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-configure-resource",
"title": "Configure the Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "TopicsCreate and Configure a VPCSet up Connectivity Between ResourcesConfigure the ResourceMigrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [],
"text": "Topics",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-create-vpc",
"text": "Create and Configure a VPC"
},
"links": [
{
"url": "https://console.aws.amazon.com/vpc/",
"title": "https://console.aws.amazon.com/vpc/"
},
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html",
"title": "Using DNS"
},
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html",
"title": "DHCP options sets"
}
],
"text": "To create and configure your VPC using the VPC wizard\nOpen the Amazon VPC console at https://console.aws.amazon.com/vpc/.\n\nFrom the navigation bar, use the region selector to select the region for your VPC. You launch all instances and clusters into this VPC, so select the region that makes sense for your pipeline.\n\nChoose VPC Dashboard from the navigation pane and then choose Launch VPC Wizard.\n\nSelect the first option, VPC with a Single Public Subnet Only, and then click Select.\n\nThe configuration page shows the CIDR ranges and settings that you've chosen. Verify that Enable DNS hostnames is Yes. Make any other changes that you need, and then click Create VPC to create your VPC, subnet, internet gateway, and route table.\n\nAfter the VPC is created, choose Your VPCs in the navigation pane, and then select your VPC from the list to verify settings.\n\nOn the Description tab, make sure that both DNS resolution and DNS hostnames are Enabled. For more information about DNS settings and updating DNS support for a VPC, see Using DNS in the Amazon VPC User Guide.On the Description tab, beside DHCP options set, choose the identifier to open the DHCP options set to verify the configuration.\nThe list of DHCP options sets opens with your options set selected.On the Details tab, next to Options, verify the following:\n\ndomain-name is set to ec2.internal for the US East (N. Virginia) Region, or region.compute.internal for all other regions (for example, us-west-2.compute.internal for US West (Oregon)).domain-name-servers is set AmazonProvidedDNS\nFor more information, see DHCP options sets in the Amazon VPC User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-create-vpc",
"main_header": "Create and Configure a VPC",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-create-vpc",
"text": "Create and Configure a VPC"
},
"links": [],
"text": "To create and configure your VPC using the VPC wizard",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-create-vpc",
"main_header": "Create and Configure a VPC",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://console.aws.amazon.com/ec2/",
"title": "https://console.aws.amazon.com/ec2/"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://console.aws.amazon.com/ec2/",
"title": "https://console.aws.amazon.com/ec2/"
}
],
"text": "If your resource is of type EmrCluster, Task Runner runs on the cluster by default. We create security groups named ElasticMapReduce-master and ElasticMapReduce-slave when you launch the cluster. You need the IDs of these security groups later on.\nTo get the IDs of the security groups for a cluster in a VPC\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n\nIn the navigation pane, click Security Groups.\n\nIf you have a lengthy list of security groups, you can click the Name column to sort your security groups by name. If you don't see a Name column, click the Show/Hide Columns icon, and then click Name.\n\nNote the IDs of the ElasticMapReduce-master and ElasticMapReduce-slave security groups. If your resource is of type Ec2Resource, Task Runner runs on the EC2 instance by default. Create a security group for the VPC and specify it when you launch the EC2 instance. You need the ID of this security group later on.\nTo create a security group for an EC2 instance in a VPC\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n\nIn the navigation pane, click Security Groups.\n\nClick Create Security Group.\n\nSpecify a name and description for the security group.\n\nSelect your VPC from the list, and then click Create.\n\nNote the ID of the new security group. If you are running Task Runner on your own computer, note its public IP address, in CIDR notation. If the computer is behind a firewall, note the entire address range of its network. You need this address later on.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [
{
"url": "https://console.aws.amazon.com/rds/",
"title": "https://console.aws.amazon.com/rds/"
}
],
"text": "To add a rule to the security group for an Amazon RDS database\nOpen the Amazon RDS console at https://console.aws.amazon.com/rds/.\n\nIn the navigation pane, click Instances.\n\nClick the details icon for the DB instance. Under Security and Network, click the link to the security group, which takes you to the Amazon EC2 console. If you're using the old console design for security groups, switch to the new console design by clicking the icon that's displayed at the top of the console page.\n\nFrom the Inbound tab, click Edit and then click Add Rule. Specify the database port that you used when you launched the DB instance. Start typing the ID of the security group or IP address used by the resource running Task Runner in Source.\n\nClick Save.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [],
"text": "To add a rule to the security group for an Amazon RDS database",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [
{
"url": "https://console.aws.amazon.com/redshift/",
"title": "https://console.aws.amazon.com/redshift/"
}
],
"text": "To add a rule to the security group for an Amazon Redshift cluster\nOpen the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n\nIn the navigation pane, click Clusters.\n\nClick the details icon for the cluster. Under Cluster Properties, note the name or ID of the security group, and then click View VPC Security Groups, which takes you to the Amazon EC2 console. If you're using the old console design for security groups, switch to the new console design by clicking the icon that's displayed at the top of the console page.\n\nSelect the security group for the cluster.\n\nFrom the Inbound tab, click Edit and then click Add Rule. Specify the type, protocol, and port range. Start typing the ID of the security group or IP address used by the resource running Task Runner in Source.\n\nClick Save.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [],
"text": "To add a rule to the security group for an Amazon Redshift cluster",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-ec2classic-to-vpc",
"text": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/vpc-migrate.html#full-migrate",
"title": "Migrate your resources to a VPC"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-create-vpc",
"title": "Create and Configure a VPC"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"title": "Set up Connectivity Between Resources"
},
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To migrate pipeline resources from EC2-Classic to VPCIdentify the Ec2Resource objects in your pipelines that use EC2-Classic. These objects have a securityGroups property. In contrast, objects created in a VPC have securityGroupIDs and subnetID properties.For each object, make a note of the EC2-Classic securityGroups specified for the object. You will copy each security group's settings to a new VPC security group in the next step.Follow the steps in Migrate your resources to a VPC in the Amazon EC2 User Guide for Linux Instances. The migration steps have you set up a new VPC and copy security group settings. As you perform these steps, attend to the following:\n\nSet up your VPC according to the guidelines in Create and Configure a VPC above.Make a note of the Subnet ID in the VPC that you create. You will use this later when you migrate objects. Make a note of security group IDs as you create them and note the corresponding EC2-Classic security groups. You will need the IDs later when you migrate objects. If you need to create new VPC security groups, see Set up Connectivity Between Resources above for guidelines.\n\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/ and edit resource objects that use EC2-Classic according to the following steps:\nNoteAs an alternative to editing a pipeline directly, you can clone the pipeline, update the clone using the remaining steps in this procedure, and then delete the original pipeline after the cloned pipeline validates. To clone a pipeline, select it from the list, choose Actions, Clone. You can then delete the original pipeline after the cloned pipeline validates and runs successfully.\nFrom the list of pipelines, select the pipeline that contains the object to migrate and then choose Actions, Edit.\nIn the architect view for the pipeline, select the resource object that you need to migrate from the design pane on the left.Select the Resources section on the right to view settings for the resource.Make a note of the Security Groups listed for the resource. These are the EC2-Classic security group or groups that you will replace.From the Add an optional field... list, select Security Group IDs.In the Security Group IDs box that appears, type the IDs of the VPC security group or groups that corresponds to EC2-Classic security groups. Use a comma to separate multiple IDs.From the Add an optional field... list, select Subnet Id, and then type the Subnet Id associated with the VPC you want to use. For example, subnet-12345678.Choose the delete icon to the right of Security Groups to remove the EC2-Classic Security Groups.Choose Save.\nAWS Data Pipeline validates the pipeline configuration. If any validation errors occur, detail appears in the lower left of the architect view. Address any errors before running the pipeline.If the pipeline is an on-demand pipeline, the updated definition is used the next time the pipeline runs. If the pipeline is a scheduled pipeline, choose Activate. AWS Data Pipeline uses the updated definition during the next scheduled run.\nRepeat the steps above for each pipeline that uses EC2-Classic resources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"main_header": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-ec2classic-to-vpc",
"text": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
},
"links": [],
"text": "To migrate pipeline resources from EC2-Classic to VPC",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"main_header": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-ec2classic-to-vpc",
"text": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
},
"links": [
{
"url": "https://console.aws.amazon.com/ec2/",
"title": "https://console.aws.amazon.com/ec2/"
}
],
"text": "To confirm the migration to VPC for a pipelineFrom the list of pipelines, choose the Pipeline ID to open execution details, and then find the EC2 instance IDs that the pipeline launched.Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.Choose InstancesFor each instance from above, select the Instance ID. On the Description tab, note the value of the VPC field to confirm that the pipeline launched instances into a VPC. Instances launched in EC2-Classic have an empty VPC field.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"main_header": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-ec2classic-to-vpc",
"text": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
},
"links": [],
"text": "To confirm the migration to VPC for a pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"main_header": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-spot-instances",
"text": "Using Amazon EC2 Spot Instances in a Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html#dp-spot-instances",
"main_header": "Using Amazon EC2 Spot Instances in a Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-spot-instances",
"text": "Using Amazon EC2 Spot Instances in a Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To use Spot Instances in your pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nOpen your pipeline in Architect.\n\nIn the Resources pane, go to the EMR cluster. In Add an optional field, select Task Instance Bid Price. Set Task Instance Bid Price to your maximum price for a Spot Instance per hour. This is the maximum dollar amount you pay, and is a decimal value between 0 and 20.00 exclusive.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html#dp-spot-instances",
"main_header": "Using Amazon EC2 Spot Instances in a Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-spot-instances",
"text": "Using Amazon EC2 Spot Instances in a Pipeline"
},
"links": [],
"text": "To use Spot Instances in your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html#dp-spot-instances",
"main_header": "Using Amazon EC2 Spot Instances in a Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html#datapipeline_region",
"title": "AWS Regions and Endpoints"
}
],
"text": "NoteThe following list includes regions in which AWS Data Pipeline can orchestrate workflows and launch Amazon EMR or Amazon EC2 resources. AWS Data Pipeline may not be supported in these regions. For information about regions in which AWS Data Pipeline is supported, see AWS Regions and Endpoints.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html#datapipeline_region",
"title": "AWS Regions and Endpoints"
}
],
"text": "The following list includes regions in which AWS Data Pipeline can orchestrate workflows and launch Amazon EMR or Amazon EC2 resources. AWS Data Pipeline may not be supported in these regions. For information about regions in which AWS Data Pipeline is supported, see AWS Regions and Endpoints.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html#datapipeline_region",
"title": "AWS Regions and Endpoints"
}
],
"text": "The following list includes regions in which AWS Data Pipeline can orchestrate workflows and launch Amazon EMR or Amazon EC2 resources. AWS Data Pipeline may not be supported in these regions. For information about regions in which AWS Data Pipeline is supported, see AWS Regions and Endpoints.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "Region Name\nRegion Code US East (N. Virginia)\nus-east-1 US East (Ohio)\nus-east-2 US West (N. California)\nus-west-1 US West (Oregon)\nus-west-2 Canada (Central)\nca-central-1 Europe (Ireland)\neu-west-1 Europe (London)\neu-west-2 Europe (Frankfurt)\neu-central-1 Asia Pacific (Singapore)\nap-southeast-1 Asia Pacific (Sydney)\nap-southeast-2 Asia Pacific (Mumbai)\nap-south-1 Asia Pacific (Tokyo)\nap-northeast-1 Asia Pacific (Seoul)\nap-northeast-2 South America (S\u00c3\u00a3o Paulo)\nsa-east-1",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "Region Name\nRegion Code US East (N. Virginia)\nus-east-1 US East (Ohio)\nus-east-2 US West (N. California)\nus-west-1 US West (Oregon)\nus-west-2 Canada (Central)\nca-central-1 Europe (Ireland)\neu-west-1 Europe (London)\neu-west-2 Europe (Frankfurt)\neu-central-1 Asia Pacific (Singapore)\nap-southeast-1 Asia Pacific (Sydney)\nap-southeast-2 Asia Pacific (Mumbai)\nap-south-1 Asia Pacific (Tokyo)\nap-northeast-1 Asia Pacific (Seoul)\nap-northeast-2 South America (S\u00c3\u00a3o Paulo)\nsa-east-1",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-failandrerun",
"main_header": "Cascading Failures and Reruns",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"links": [],
"text": "When an object fails, its consumers are set to CASCADE_FAILED and both the original object and its consumers' preconditions are set to CANCELED.\n\nAny objects that are already FINISHED, FAILED, or CANCELED are ignored.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-failandrerun",
"main_header": "Cascading Failures and Reruns",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-rerun",
"text": "Rerunning Cascade-Failed Objects"
},
"links": [],
"text": "The target object's consumers are in the CASCADE_FAILED state.\n\nThe target object's dependencies have no rerun commands pending.\n\nThe target object's dependencies are not in the FAILED, CASCADE_FAILED, or CANCELED state.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-rerun",
"main_header": "Rerunning Cascade-Failed Objects",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-writing-pipeline-definition",
"main_header": "Pipeline Definition File Syntax",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-userdefined-fields",
"text": "User-Defined Fields"
},
"links": [],
"text": "NoteOn user-defined fields, AWS Data Pipeline only checks for valid references to other pipeline components, not any custom field string values that you add.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"main_header": "User-Defined Fields",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-userdefined-fields",
"text": "User-Defined Fields"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"main_header": "User-Defined Fields",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-userdefined-fields",
"text": "User-Defined Fields"
},
"links": [],
"text": "On user-defined fields, AWS Data Pipeline only checks for valid references to other pipeline components, not any custom field string values that you add.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"main_header": "User-Defined Fields",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-userdefined-fields",
"text": "User-Defined Fields"
},
"links": [],
"text": "On user-defined fields, AWS Data Pipeline only checks for valid references to other pipeline components, not any custom field string values that you add.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"main_header": "User-Defined Fields",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-program-pipeline",
"main_header": "Working with the API",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "Note If you are not writing programs that interact with AWS Data Pipeline, you do not need to install any of the AWS SDKs. You can create and run pipelines using the console or command-line interface. For more information, see Setting up for AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-program-pipeline",
"main_header": "Working with the API",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-program-pipeline",
"main_header": "Working with the API",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "If you are not writing programs that interact with AWS Data Pipeline, you do not need to install any of the AWS SDKs. You can create and run pipelines using the console or command-line interface. For more information, see Setting up for AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-program-pipeline",
"main_header": "Working with the API",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "If you are not writing programs that interact with AWS Data Pipeline, you do not need to install any of the AWS SDKs. You can create and run pipelines using the console or command-line interface. For more information, see Setting up for AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-program-pipeline",
"main_header": "Working with the API",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"h2": {
"urllink": "#dp-set-up-aws-sdk",
"text": "Install the AWS SDK"
},
"links": [
{
"url": "http://aws.amazon.com/java",
"title": "AWS SDK for Java"
},
{
"url": "http://aws.amazon.com/sdkfornodejs",
"title": "AWS SDK for Node.js"
},
{
"url": "http://aws.amazon.com/sdkforphp",
"title": "AWS SDK for PHP"
},
{
"url": "http://aws.amazon.com/sdkforpython",
"title": "AWS SDK for Python (Boto)"
},
{
"url": "http://aws.amazon.com/sdkforruby",
"title": "AWS SDK for Ruby"
},
{
"url": "http://aws.amazon.com/net",
"title": "AWS SDK for .NET"
}
],
"text": "AWS SDK for Java AWS SDK for Node.js AWS SDK for PHP AWS SDK for Python (Boto) AWS SDK for Ruby AWS SDK for .NET",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-set-up-aws-sdk",
"main_header": "Install the AWS SDK",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-make-http-request",
"main_header": "Making an HTTP Request to AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-header",
"text": "HTTP Header Contents"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html",
"title": "Regions and Endpoints"
},
{
"url": "https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html",
"title": "Signature Version 4 Signing Process"
}
],
"text": "host The AWS Data Pipeline endpoint. \nFor information about endpoints, see Regions and Endpoints. x-amz-date You must provide the time stamp in either the HTTP Date header or the AWS x-amz-date header. (Some HTTP client libraries don't let you set the Date header.) When an x-amz-date header is present, the system ignores any Date header during the request authentication. The date must be specified in one of the following three formats, as specified in the HTTP/1.1 RFC: Sun, 06 Nov 1994 08:49:37 GMT (RFC 822, updated by RFC 1123) Sunday, 06-Nov-94 08:49:37 GMT (RFC 850, obsoleted by RFC 1036) Sun Nov 6 08:49:37 1994 (ANSI C asctime() format) Authorization The set of authorization parameters that AWS uses to ensure the validity and authenticity of the request. For more information about constructing this header, go to Signature Version 4 Signing Process. x-amz-target The destination service of the request and the operation for the data, in the format: <<serviceName>>_<<API version>>.<<operationName>>\n\nFor example, DataPipeline_20121129.ActivatePipeline content-type Specifies JSON and the version. For example, Content-Type: application/x-amz-json-1.0",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-http-header",
"main_header": "HTTP Header Contents",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-body-content",
"text": "HTTP Body Content"
},
"h3": {
"urllink": "#dp-handle-http-responses",
"text": "Handle the HTTP Response"
},
"links": [],
"text": "HTTP/1.1\u00e2\u0080\u0094This header is followed by a status code. A code value of 200 indicates a successful operation. Any other value indicates an error. x-amzn-RequestId\u00e2\u0080\u0094This header contains a request ID that you can use if you need to troubleshoot a request with AWS Data Pipeline. An example of a request ID is K2QH8DNOU907N97FNA2GDLL8OBVV4KQNSO5AEMVJF66Q9ASUAAJG. x-amz-crc32\u00e2\u0080\u0094AWS Data Pipeline calculates a CRC32 checksum of the HTTP payload and returns this checksum in the x-amz-crc32 header. We recommend that you compute your own CRC32 checksum on the client side and compare it with the x-amz-crc32 header; if the checksums do not match, it might indicate that the data was corrupted in transit. If this happens, you should retry your request.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-handle-http-responses",
"main_header": "Handle the HTTP Response",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#security",
"text": "Security in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html#security",
"main_header": "Security in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#security",
"text": "Security in AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/compliance/programs/",
"title": "AWS Compliance Programs"
},
{
"url": "http://aws.amazon.com/compliance/services-in-scope/",
"title": "AWS Services in Scope by Compliance Program"
}
],
"text": "Security of the cloud \u00e2\u0080\u0093 AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud. AWS also provides you with services that you can use securely. Third-party auditors regularly test and verify the effectiveness of our security as part of the AWS Compliance Programs. To learn about the compliance programs that apply to AWS Data Pipeline, see AWS Services in Scope by Compliance Program.\n\nSecurity in the cloud \u00e2\u0080\u0093 Your responsibility is determined by the AWS service that you use. You are also responsible for other factors including the sensitivity of your data, your company\u00e2\u0080\u0099s requirements, and applicable laws and regulations.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html#security",
"main_header": "Security in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#data-protection",
"text": "Data Protection in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html#data-protection",
"main_header": "Data Protection in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#data-protection",
"text": "Data Protection in AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/compliance/fips/",
"title": "Federal Information Processing Standard (FIPS) 140-2"
}
],
"text": "Use multi-factor authentication (MFA) with each account.\n\nUse SSL/TLS to communicate with AWS resources. We recommend TLS 1.2 or later.\n\nSet up API and user activity logging with AWS CloudTrail.\n\nUse AWS encryption solutions, along with all default security controls within AWS services.\n\nUse advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.\n\nIf you require FIPS 140-2 validated cryptographic modules when accessing AWS through a command line interface or an API, use a FIPS endpoint. For more information about the available FIPS endpoints, see Federal Information Processing Standard (FIPS) 140-2.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html#data-protection",
"main_header": "Data Protection in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "Control which IAM users can access specific pipelines\n\nProtect a production pipeline from being edited by mistake\n\nAllow an auditor to have read-only access to pipelines, but prevent them from making changes",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "Create users and groups in your AWS account.\n\nEasily share your AWS resources between the users in your AWS account.\n\nAssign unique security credentials to each user.\n\nControl each user's access to services and resources.\n\nGet a single bill for all users in your AWS account.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-iam-resourcebased-access",
"main_header": "IAM Policies for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-policy-syntax",
"title": "Policy Syntax"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"title": "Controlling Access to Pipelines Using Tags"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-workergroup",
"title": "Controlling Access to Pipelines Using Worker Groups"
}
],
"text": "ContentsPolicy SyntaxControlling Access to Pipelines Using TagsControlling Access to Pipelines Using Worker Groups",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-iam-resourcebased-access",
"main_header": "IAM Policies for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-iam-resourcebased-access",
"main_header": "IAM Policies for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-policy-syntax",
"text": "Policy Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/APIReference/API_Operations.html",
"title": "Actions"
},
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/AccessPolicyLanguage_ElementDescriptions.html#AvailableKeys",
"title": "Available Keys for Conditions"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex3",
"title": "Grant the pipeline owner full access"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"title": "Controlling Access to Pipelines Using Tags"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-workergroup",
"title": "Controlling Access to Pipelines Using Worker Groups"
}
],
"text": "Effect: The effect can be Allow or Deny. By default, IAM users don't have permission to use resources and API actions, so all requests are denied. An explicit allow overrides the default. An explicit deny overrides any allows.\n\nAction: The action is the specific API action for which you are granting or denying permission. For a list of actions for AWS Data Pipeline, see Actions in the AWS Data Pipeline API Reference.\n\nResource: The resource that's affected by the action. The only valid value here is \"*\". Condition: Conditions are optional. They can be used to control when your policy will be in effect.\nAWS Data Pipeline implements the AWS-wide context keys (see Available Keys for Conditions), plus the following service-specific keys. datapipeline:PipelineCreator \u00e2\u0080\u0094 To grant access to the user that created the pipeline. For an example, see Grant the pipeline owner full access.\n\ndatapipeline:Tag \u00e2\u0080\u0094 To grant access based on pipeline tagging. For more information, see Controlling Access to Pipelines Using Tags.\n\ndatapipeline:workerGroup \u00e2\u0080\u0094 To grant access based on the name of the worker group. For more information, see Controlling Access to Pipelines Using Worker Groups.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-policy-syntax",
"main_header": "Policy Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-tags",
"text": "Controlling Access to Pipelines Using Tags"
},
"links": [],
"text": "Grant read-only access to a pipeline\n\nGrant read/write access to a pipeline\n\nBlock access to a pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"main_header": "Controlling Access to Pipelines Using Tags",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#dp-example-tag-policies",
"main_header": "Example Policies for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex1",
"title": "Example 1: Grant users read-only access based on a tag"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex2",
"title": "Example 2: Grant users full access based on a tag"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex3",
"title": "Example 3: Grant the pipeline owner full access"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#example4-grant-users-access-to-console",
"title": "Example 4: Grant users access to the AWS Data Pipeline console"
}
],
"text": "ContentsExample 1: Grant users read-only access based on a tagExample 2: Grant users full access based on a tagExample 3: Grant the pipeline owner full accessExample 4: Grant users access to the AWS Data Pipeline console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#dp-example-tag-policies",
"main_header": "Example Policies for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#dp-example-tag-policies",
"main_header": "Example Policies for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "The pipeline role controls AWS Data Pipeline access to your AWS resources. In pipeline object definitions, the role field specifies this role.\n\nThe EC2 instance role controls the access that applications running on EC2 instances, including the EC2 instances in Amazon EMR clusters, have to AWS resources. In pipeline object definitions, the resourceRole field specifies this role.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "ImportantIf you created a pipeline before April 28, 2021 using the AWS Data Pipeline console with default roles, AWS Data Pipeline created the DataPipelineDefaultRole for you and attached the AWSDataPipelineRole managed policy to the role. As of April 28, 2021, the AWSDataPipelineRole managed policy is deprecated and the pipeline role must be specified for a pipeline when using the console.We recommend that you review existing pipelines and determine if the DataPipelineDefaultRole is associated with the pipeline and whether the AWSDataPipelineRole is attached to that role. If so, review the access that this policy allows to ensure it is appropriate for your security requirements. Add, update, or replace the policies and policy statements attached to this role as necessary. Alternatively, you can update a pipeline to use a role that you create with different permissions policies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "If you created a pipeline before April 28, 2021 using the AWS Data Pipeline console with default roles, AWS Data Pipeline created the DataPipelineDefaultRole for you and attached the AWSDataPipelineRole managed policy to the role. As of April 28, 2021, the AWSDataPipelineRole managed policy is deprecated and the pipeline role must be specified for a pipeline when using the console.We recommend that you review existing pipelines and determine if the DataPipelineDefaultRole is associated with the pipeline and whether the AWSDataPipelineRole is attached to that role. If so, review the access that this policy allows to ensure it is appropriate for your security requirements. Add, update, or replace the policies and policy statements attached to this role as necessary. Alternatively, you can update a pipeline to use a role that you create with different permissions policies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "If you created a pipeline before April 28, 2021 using the AWS Data Pipeline console with default roles, AWS Data Pipeline created the DataPipelineDefaultRole for you and attached the AWSDataPipelineRole managed policy to the role. As of April 28, 2021, the AWSDataPipelineRole managed policy is deprecated and the pipeline role must be specified for a pipeline when using the console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "We recommend that you review existing pipelines and determine if the DataPipelineDefaultRole is associated with the pipeline and whether the AWSDataPipelineRole is attached to that role. If so, review the access that this policy allows to ensure it is appropriate for your security requirements. Add, update, or replace the policies and policy statements attached to this role as necessary. Alternatively, you can update a pipeline to use a role that you create with different permissions policies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-role-permissions-policy-examples",
"text": "Example Permissions Policies for AWS Data Pipeline Roles"
},
"h3": {
"urllink": "#dp-role-example-policy",
"text": "Example Pipeline Role Permissions Policy"
},
"links": [],
"text": "Replace 111122223333 with your AWS account ID.Replace NameOfDataPipelineRole with the name of pipeline role (the role to which this policy is attached).Replace NameOfDataPipelineResourceRole with the name of EC2 instance role.Replace us-west-1 with the appropriate Region for your application.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-role-example-policy",
"main_header": "Example Pipeline Role Permissions Policy",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "NoteWhen you create roles for AWS Data Pipeline using the console as described below, IAM creates and attaches the appropriate trust policies that the role requires.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "When you create roles for AWS Data Pipeline using the console as described below, IAM creates and attaches the appropriate trust policies that the role requires.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "When you create roles for AWS Data Pipeline using the console as described below, IAM creates and attaches the appropriate trust policies that the role requires.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [
{
"url": "https://console.aws.amazon.com/iam/",
"title": "https://console.aws.amazon.com/iam/"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-role-example-policy",
"title": "Example Pipeline Role Permissions Policy"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-resource-role-example-policy",
"title": "Default Managed Policy for the EC2 Instance Role"
}
],
"text": "To create a permissions policy to use with a role for AWS Data PipelineOpen the IAM console at https://console.aws.amazon.com/iam/.In the navigation pane, choose Policies, and then choose Create policy.Choose the JSON tab.If you are creating a pipeline role, copy and paste the contents of the policy example in Example Pipeline Role Permissions Policy, editing it as appropriate for your security requirements. Alternatively, if you are creating a custom EC2 instance role, do the same for the example in Default Managed Policy for the EC2 Instance Role.Choose Review policy.Enter a name for the policy\u00e2\u0080\u0094for example, MyDataPipelineRolePolicy\u00e2\u0080\u0094and an optional Description, and then choose Create policy.Note the name of the policy. You need it when you create your role.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "To create a permissions policy to use with a role for AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [
{
"url": "https://console.aws.amazon.com/iam/",
"title": "https://console.aws.amazon.com/iam/"
}
],
"text": "To create an IAM role for AWS Data PipelineOpen the IAM console at https://console.aws.amazon.com/iam/.\nIn the navigation pane, choose Roles, and then choose Create Role.\nUnder Choose a use case, choose Data Pipeline.Under Select your use case, do one of the following:\n\nChoose Data Pipeline to create a pipeline role.Choose EC2 Role for Data Pipeline to create a resource role.\nChoose Next: Permissions.If the default policy for AWS Data Pipeline is listed, proceed with the following steps to create the role and then edit it according to the instructions in the next procedure. Otherwise, enter the name of the policy that you created in the procedure above, and then select it from the list.Choose Next: Tags, enter any tags to add to the role, and then choose Next: Review.Enter a name for the role\u00e2\u0080\u0094for example, MyDataPipelineRole\u00e2\u0080\u0094and an optional Description, and then choose Create role.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "To create an IAM role for AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [
{
"url": "https://console.aws.amazon.com/iam/",
"title": "https://console.aws.amazon.com/iam/"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#step3",
"title": "step 3"
}
],
"text": "To attach or detach a permissions policy for an IAM role for AWS Data Pipeline\nOpen the IAM console at https://console.aws.amazon.com/iam/.\nIn the navigation pane, choose RolesIn the search box, begin typing the name of the role you want to edit\u00e2\u0080\u0094for example, DataPipelineDefaultRole or MyDataPipelineRole\u00e2\u0080\u0094and then choose the Role name from the list.\nOn the Permissions tab, do the following:\n\nTo detach a permissions policy, under Permissions policies, choose the remove button on the far right of the policy entry. Choose Detach when prompted to confirm.To attach a policy that you created earlier, choose Attach policies. In the search box, begin typing the name of the policy you want to edit, select it from the list, and then choose Attach policy.To create a new policy and attach it, choose Add inline policy and then create a policy by following the instructions beginning with step 3 of the procedure To create a permissions policy to use with a role for AWS Data Pipeline above.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "To attach or detach a permissions policy for an IAM role for AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-change-console",
"text": "Changing Roles for an Existing Pipelines"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To edit the roles assigned to a pipeline using the console\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nSelect the pipeline from the list, and then choose Actions, Edit.\n\nIn the right pane of the architect editor, choose Others.\nFrom the Resource Role and Role lists, choose the roles for AWS Data Pipeline that you want to assign, and then choose Save.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-change-console",
"main_header": "Changing Roles for an Existing Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-change-console",
"text": "Changing Roles for an Existing Pipelines"
},
"links": [],
"text": "To edit the roles assigned to a pipeline using the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-change-console",
"main_header": "Changing Roles for an Existing Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#dp-cloudtrail-logging",
"main_header": "Logging and Monitoring in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail.html",
"title": "Overview for Creating a Trail"
},
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations",
"title": "CloudTrail Supported Services and Integrations"
},
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/getting_notifications_top_level.html",
"title": "Configuring Amazon SNS Notifications for CloudTrail"
},
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html",
"title": "Receiving CloudTrail Log Files from Multiple Regions"
},
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html",
"title": "Receiving CloudTrail Log Files from Multiple Accounts"
}
],
"text": "Overview for Creating a Trail\n\nCloudTrail Supported Services and Integrations\n\nConfiguring Amazon SNS Notifications for CloudTrail\n\nReceiving CloudTrail Log Files from Multiple Regions and Receiving CloudTrail Log Files from Multiple Accounts",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [],
"text": "Whether the request was made with root or AWS Identity and Access Management (IAM) user credentials.\n\nWhether the request was made with temporary security credentials for a role or federated user.\n\nWhether the request was made by another AWS service.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#incident-response",
"text": "Incident Response in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/incident-response.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/incident-response.html#incident-response",
"main_header": "Incident Response in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#compliance-validation.title",
"text": "Compliance Validation for AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/compliance-validation.title.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/compliance-validation.title.html#compliance-validation.title",
"main_header": "Compliance Validation for AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#disaster-recovery-resiliency",
"text": "Resilience in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/disaster-recovery-resiliency.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/disaster-recovery-resiliency.html#disaster-recovery-resiliency",
"main_header": "Resilience in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#infrastructure-security",
"text": "Infrastructure Security in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html#infrastructure-security",
"main_header": "Infrastructure Security in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#configuration-and-vulnerability-analysis",
"text": "Configuration and Vulnerability Analysis in AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/configuration-and-vulnerability-analysis.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/configuration-and-vulnerability-analysis.html#configuration-and-vulnerability-analysis",
"main_header": "Configuration and Vulnerability Analysis in AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#welcome",
"text": "Tutorials"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html#welcome",
"main_header": "Tutorials",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow",
"text": "Process Data Using Amazon EMR with Hadoop Streaming"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html#dp-launch-emr-jobflow",
"main_header": "Process Data Using Amazon EMR with Hadoop Streaming",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow",
"text": "Process Data Using Amazon EMR with Hadoop Streaming"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
}
],
"text": "EmrActivity\n\nDefines the work to perform in the pipeline (run a pre-existing Hadoop Streaming job provided by Amazon EMR).\n\nEmrCluster\n\nResource AWS Data Pipeline uses to perform this activity.\nA cluster is a set of Amazon EC2 instances. AWS Data Pipeline launches the cluster and then terminates it after the task finishes. Schedule\n\nStart date, time, and the duration for this activity. You can optionally specify the end date and time.\n\nSnsAlarm\n\nSends an Amazon SNS notification to the topic you specify after the task finishes successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html#dp-launch-emr-jobflow",
"main_header": "Process Data Using Amazon EMR with Hadoop Streaming",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-prereq.html#dp-emr-jobflow-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-prereq",
"text": "Before You Begin"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
},
{
"url": "https://docs.aws.amazon.com/sns/latest/gsg/CreateTopic.html",
"title": "Create a Topic"
}
],
"text": "Complete the tasks in Setting up for AWS Data Pipeline.\n\n(Optional) Set up a VPC for the cluster and a security group for the VPC. For more information, see Launching Resources for Your Pipeline into a VPC.\n\nCreate a topic for sending email notification and make a note of the topic Amazon Resource Name (ARN). For more information, see Create a Topic in the Amazon Simple Notification Service Getting Started Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-prereq.html#dp-emr-jobflow-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-console",
"main_header": "Launch a Cluster Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"title": "Create the Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-save-pipeline-console",
"title": "Save and Validate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"title": "Activate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-execution-pipeline-console",
"title": "Monitor the Pipeline Runs"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-delete-pipeline-console",
"title": "(Optional) Delete Your Pipeline"
}
],
"text": "TasksCreate the PipelineSave and Validate Your PipelineActivate Your PipelineMonitor the Pipeline Runs(Optional) Delete Your Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-console",
"main_header": "Launch a Cluster Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-console",
"main_header": "Launch a Cluster Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To create your pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using Architect.\n\nUnder Schedule, choose on pipeline activation.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Edit in Architect.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To create your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the activity\nClick Add activity.\n\nIn the Activities pane:\n\nIn the Name field, enter the name for the activity (for example, MyEMRActivity).\n\nFrom Type, select EmrActivity.\n\nIn Step, enter:\ncommand-runner.jar,hadoop-streaming,-files,s3://elasticmapreduce/samples/wordcount/wordSplitter.py,-mapper,wordSplitter.py,-reducer,aggregate,-input,s3://elasticmapreduce/samples/wordcount/input,-output,s3://YourBucket/output-qa/#{@scheduledStartTime}\n\nIn Add an optional field, select Runs On. Set the value to Create new: EmrCluster.\n\nIn Add an optional field, select On Success. Set the value to Create new: Action.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the activity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the resource\nIn the right pane, choose Resources.\n\nIn the Name field, enter the name for your Amazon EMR cluster (for example, MyEMRCluster).\n\nLeave Type set to EmrCluster.\n\n[EC2-VPC] (Optional) From Add an optional field, select Subnet Id. Set the value to the ID of the subnet.\n\n(Optional) From Add an optional field, select Enable Debugging. Set the value to true.\nNoteThis option can incur extra costs because of log data storage. Use this option selectively, such as for prototyping and troubleshooting.\n\n(Optional) In the right pane, choose Others. Under Default, from Add an optional field, select Pipeline Log Uri. Set the value to an Amazon S3 bucket for the Amazon EMR logs. For example, s3://examples-bucket/emrlogs.\nNoteThis option can incur extra costs because of log file storage. Use this option selectively, such as for prototyping and troubleshooting.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the resource",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/sns/latest/dg/CreateTopic.html",
"title": "Create a Topic"
}
],
"text": "To configure the notification action\nIn the right pane, click Others.\n\nUnder DefaultAction1, do the following:\n\nEnter the name for your notification (for example, MyEMRJobNotice).\n\nFrom Type, select SnsAlarm.\n\nIn the Subject field, enter the subject line for your notification.\n\nIn the Topic Arn field, enter the ARN of your topic (see Create a Topic).\n\nIn Message, enter the message content.\n\nLeave Role set to the default value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the notification action",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline runs\nChoose Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then choose Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks. If you created an SNS notification, you should receive email about the successful completion of this task.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "To monitor the progress of your pipeline runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#accessing-datapipeline",
"title": "Accessing AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "PrerequisitesBefore you can use the CLI, you must complete the following steps:\nInstall and configure a command line interface (CLI). For more information, see Accessing AWS Data Pipeline.\n\nEnsure that the IAM roles named DataPipelineDefaultRole and DataPipelineDefaultResourceRole exist. The AWS Data Pipeline console creates these roles for you automatically. If you haven't used the AWS Data Pipeline console at least once, then you must create these roles manually. For more information, see IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [],
"text": "Before you can use the CLI, you must complete the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-json",
"title": "Creating the Pipeline Definition File"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"title": "Uploading and Activating the Pipeline Definition"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-monitor",
"title": "Monitor the Pipeline Runs"
}
],
"text": "TasksCreating the Pipeline Definition FileUploading and Activating the Pipeline DefinitionMonitor the Pipeline Runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-json",
"text": "Creating the Pipeline Definition File"
},
"links": [],
"text": "Hourly, which represents the schedule of the work. You can set a schedule as one of the fields on an activity. When you do, the activity runs according to that schedule, or in this case, hourly. MyCluster, which represents the set of Amazon EC2 instances used to run the cluster. You can specify the size and number of EC2 instances to run as the cluster. If you do not specify the number of instances, the cluster launches with two, a master node and a task node. You can specify a subnet to launch the cluster into. You can add additional configurations to the cluster, such as bootstrap actions to load additional software onto the Amazon EMR-provided AMI. MyEmrActivity, which represents the computation to process with the cluster. Amazon EMR supports several types of clusters, including streaming, Cascading, and Scripted Hive. The runsOn field refers back to MyCluster, using that as the specification for the underpinnings of the cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-json",
"main_header": "Creating the Pipeline Definition File",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-monitor",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "To check the progress of clusters launched by AWS Data Pipeline\nOpen the Amazon EMR console.\n\nThe clusters that were spawned by AWS Data Pipeline have a name formatted as follows: <pipeline-identifier>_@<emr-cluster-name>_<launch-time>. After one of the runs is complete, open the Amazon S3 console and check that the time-stamped output folder exists and contains the expected results of the cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-monitor",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-monitor",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "To check the progress of clusters launched by AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-monitor",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb",
"text": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html#dp-importexport-ddb",
"main_header": "Import and Export DynamoDB Data Using AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-part1",
"text": "Part One: Import Data into DynamoDB"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html#dp-importexport-ddb-part1",
"main_header": "Part One: Import Data into DynamoDB",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
},
{
"url": "https://docs.aws.amazon.com/sns/latest/gsg/CreateTopic.html",
"title": "Create a Topic"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-table-cli",
"title": "Create a DynamoDB Table"
}
],
"text": "Complete the tasks in Setting up for AWS Data Pipeline.\n\n(Optional) Set up a VPC for the cluster and a security group for the VPC. For more information, see Launching Resources for Your Pipeline into a VPC.\n\nCreate a topic and subscribe to receive notifications from AWS Data Pipeline regarding the status of your pipeline components. For more information, see Create a Topic in the Amazon Simple Notification Service Getting Started Guide.\nCreate a DynamoDB table to store data. For more information, see Create a DynamoDB Table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"links": [
{
"url": "https://aws.amazon.com/elasticmapreduce/pricing/",
"title": "Amazon EMR Pricing"
},
{
"url": "https://aws.amazon.com/datapipeline/pricing/",
"title": "AWS Data Pipeline Pricing"
},
{
"url": "https://aws.amazon.com/s3/pricing/",
"title": "Amazon S3 Pricing"
}
],
"text": "Underlying service costs\nBe aware of the costs. AWS Data Pipeline manages the import/export process for you, but you still pay for the underlying AWS services used. The import and export pipelines will create Amazon EMR clusters to read and write data and there are per-instance charges for each node in the cluster. You can read more about the details of Amazon EMR Pricing. The default cluster configuration is one m1.small instance master node and one m1.xlarge instance task node, though you can change this configuration in the pipeline definition. There are also charges for AWS Data Pipeline. For more information, see AWS Data Pipeline Pricing and Amazon S3 Pricing.\nImports may overwrite data\nWhen you import data from Amazon S3, the import may overwrite items in your DynamoDB table. Make sure that you are importing the right data and into the right table. Be careful not to accidentally set up a recurring import pipeline that will import the same data multiple times.\n\nExports may overwrite data\nWhen you export data to Amazon S3, you may overwrite previous exports if you write to the same bucket path. The default behavior of the Export DynamoDB to S3 template will append the job's scheduled time to the Amazon S3 bucket path, which will help you avoid this problem.\n\nJobs consume throughput capacity\nImport and Export jobs will consume some of your DynamoDB table's provisioned throughput capacity. This section explains how to schedule an import or export job using Amazon EMR. The Amazon EMR cluster will consume some read capacity during exports or write capacity during imports. You can control the percentage of the provisioned capacity that the import/export jobs consume by with the settings MyImportJob.myDynamoDBWriteThroughputRatio and MyExportJob.myDynamoDBReadThroughputRatio. Be aware that these settings determine how much capacity to consume at the beginning of the import/export process and will not adapt in real time if you change your table's provisioned capacity in the middle of the process.\n\nOn-Demand Capacity works only with EMR 5.24.0 or later\nDynamoDB tables configured for On-Demand Capacity are supported only when using Amazon EMR release version 5.24.0 or later. When you use a template to create a pipeline for DynamoDB, choose Edit in Architect and then choose Resources to configure the Amazon EMR cluster that AWS Data Pipeline provisions. For Release label, choose emr-5.24.0 or later.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"h2": {
"urllink": "#dp-importexport-ddb-table-cli",
"text": "Create a DynamoDB Table"
},
"links": [
{
"url": "https://console.aws.amazon.com/dynamodb/",
"title": "https://console.aws.amazon.com/dynamodb/"
},
{
"url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughputIntro.html",
"title": "Provisioned Throughput in Amazon DynamoDB"
}
],
"text": "To create a DynamoDB table \nOpen the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n\nClick Create Table.\n\nEnter a unique name for your table in Table Name.\n\nIn the Primary Key: Partition Key field, enter the string Id.\n\nClick Continue to skip the optional Add Indexes page.\n\nOn the Provisioned Throughput Capacity page, do the following. Note that these values are small because the sample data is small. For information about calculating the required size for your own data, see Provisioned Throughput in Amazon DynamoDB in the Amazon DynamoDB Developer Guide.\n\nIn Read Capacity Units, enter 5.\n\nIn Write Capacity Units, enter 5.\n\nClick Continue. On the Throughput Alarms page, in Send notification to, enter your email address, and then click Continue.\n\nOn the Review page, click Create.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-table-cli",
"main_header": "Create a DynamoDB Table",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"h2": {
"urllink": "#dp-importexport-ddb-table-cli",
"text": "Create a DynamoDB Table"
},
"links": [],
"text": "To create a DynamoDB table",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-table-cli",
"main_header": "Create a DynamoDB Table",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To create the pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using a template, and then select the following template: Import DynamoDB backup data from S3.\n\nUnder Parameters, set Input S3 folder to s3://elasticmapreduce/samples/Store/ProductCatalog, which is a directory that contains the sample data source, ProductCatalog.txt,  and set DynamoDB table name to the name of your table. Under Schedule, choose on pipeline activation.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Edit in Architect.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "To create the pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-sns",
"title": "ARN resource names for Amazon SNS"
},
{
"url": "https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-sns",
"title": "ARN resource names for Amazon SNS"
}
],
"text": "To configure the success and failure actions\nIn the right pane, click Activities.\n\nFrom Add an optional field, select On Success.\n\nFrom the newly added On Success, select Create new: Action.\n\nFrom Add an optional field, select On Fail.\n\nFrom the newly added On Fail, select Create new: Action.\n\nIn the right pane, click Others.\n\nFor DefaultAction1, do the following:\n\nChange the name to SuccessSnsAlarm.\n\nFrom Type, select SnsAlarm.\n\nIn Topic Arn, enter the ARN of the Amazon SNS topic that you created (see ARN resource names for Amazon SNS).\n\nEnter a subject and a message. For DefaultAction2, do the following:\n\nChange the name to FailureSnsAlarm.\n\nFrom Type, select SnsAlarm.\n\nIn Topic Arn, enter the ARN of the topic that you created (see ARN resource names for Amazon SNS).\n\nEnter a subject and a message.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "To configure the success and failure actions",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"title": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
}
],
"text": "ImportantIf your pipeline uses an Amazon EMR release version in the 6.x series, you must add a bootstrap action to copy the following Jar file to the Hadoop classpath where MyRegion is the AWS Region where your pipeline runs: s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar. For more information, see Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies.In addition, you must change the first argument in the step field in EmrActivity with name TableLoadActivity from s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar to s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"title": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
}
],
"text": "If your pipeline uses an Amazon EMR release version in the 6.x series, you must add a bootstrap action to copy the following Jar file to the Hadoop classpath where MyRegion is the AWS Region where your pipeline runs: s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar. For more information, see Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies.In addition, you must change the first argument in the step field in EmrActivity with name TableLoadActivity from s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar to s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"title": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
}
],
"text": "If your pipeline uses an Amazon EMR release version in the 6.x series, you must add a bootstrap action to copy the following Jar file to the Hadoop classpath where MyRegion is the AWS Region where your pipeline runs: s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar. For more information, see Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "In addition, you must change the first argument in the step field in EmrActivity with name TableLoadActivity from s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar to s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html#dp-import-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline runs\nChoose Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then choose Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks. If you created an SNS notification, you should receive email about the successful completion of this task.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html#dp-import-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [],
"text": "To monitor the progress of your pipeline runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html#dp-import-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-pipelinejson-verifydata",
"text": "Step 5: Verify the Data Import"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html#dp-import-ddb-pipelinejson-verifydata",
"main_header": "Step 5: Verify the Data Import",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-pipelinejson-verifydata",
"text": "Step 5: Verify the Data Import"
},
"links": [],
"text": "To verify the DynamoDB table \nOpen the DynamoDB console.\n\nOn the Tables screen, click your DynamoDB table and click Explore Table.\n\nOn the Browse Items tab, columns that correspond to the data input file should display, such as Id, Price, ProductCategory, as shown in the following screen. This indicates that the import operation from the file to the DynamoDB table occurred successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html#dp-import-ddb-pipelinejson-verifydata",
"main_header": "Step 5: Verify the Data Import",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-pipelinejson-verifydata",
"text": "Step 5: Verify the Data Import"
},
"links": [],
"text": "To verify the DynamoDB table",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html#dp-import-ddb-pipelinejson-verifydata",
"main_header": "Step 5: Verify the Data Import",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html#dp-import-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html#dp-import-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html#dp-import-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-part2",
"text": "Part Two: Export Data from DynamoDB"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html#dp-importexport-ddb-part2",
"main_header": "Part Two: Export Data from DynamoDB",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq2",
"text": "Before You Begin"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html#dp-importexport-ddb-prereq2",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq2",
"text": "Before You Begin"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/sns/latest/gsg/CreateTopic.html",
"title": "Create a Topic"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
],
"text": "Complete the tasks in Setting up for AWS Data Pipeline.\nCreate a topic and subscribe to receive notifications from AWS Data Pipeline regarding the status of your pipeline components. For more information, see Create a Topic in the Amazon SNS Getting Started Guide.\nEnsure that you have the DynamoDB table that was created and populated with data in part one of this tutorial. This table will be your data source for part two of the tutorial. For more information, see Part One: Import Data into DynamoDB.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html#dp-importexport-ddb-prereq2",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq2",
"text": "Before You Begin"
},
"links": [
{
"url": "https://aws.amazon.com/elasticmapreduce/pricing/",
"title": "Amazon EMR Pricing"
},
{
"url": "https://aws.amazon.com/datapipeline/pricing/",
"title": "AWS Data Pipeline Pricing"
},
{
"url": "https://aws.amazon.com/s3/pricing/",
"title": "Amazon S3 Pricing"
}
],
"text": "Underlying service costs\nBe aware of the costs. AWS Data Pipeline manages the import/export process for you, but you still pay for the underlying AWS services used. The import and export pipelines will create Amazon EMR clusters to read and write data and there are per-instance charges for each node in the cluster. You can read more about the details of Amazon EMR Pricing. The default cluster configuration is one m1.small instance master node and one m1.xlarge instance task node, though you can change this configuration in the pipeline definition. There are also charges for AWS Data Pipeline. For more information, see AWS Data Pipeline Pricing and Amazon S3 Pricing.\nImports may overwrite data\nWhen you import data from Amazon S3, the import may overwrite items in your DynamoDB table. Make sure that you are importing the right data and into the right table. Be careful not to accidentally set up a recurring import pipeline that will import the same data multiple times.\n\nExports may overwrite data\nWhen you export data to Amazon S3, you may overwrite previous exports if you write to the same bucket path. The default behavior of the Export DynamoDB to S3 template will append the job's scheduled time to the Amazon S3 bucket path, which will help you avoid this problem.\n\nJobs consume throughput capacity\nImport and Export jobs will consume some of your DynamoDB table's provisioned throughput capacity. This section explains how to schedule an import or export job using Amazon EMR. The Amazon EMR cluster will consume some read capacity during exports or write capacity during imports. You can control the percentage of the provisioned capacity that the import/export jobs consume by with the settings MyImportJob.myDynamoDBWriteThroughputRatio and MyExportJob.myDynamoDBReadThroughputRatio. Be aware that these settings determine how much capacity to consume at the beginning of the import/export process and will not adapt in real time if you change your table's provisioned capacity in the middle of the process.\n\nOn-Demand Capacity works only with EMR 5.24.0 or later\nDynamoDB tables configured for On-Demand Capacity are supported only when using Amazon EMR release version 5.24.0 or later. When you use a template to create a pipeline for DynamoDB, choose Edit in Architect and then choose Resources to configure the Amazon EMR cluster that AWS Data Pipeline provisions. For Release label, choose emr-5.24.0 or later.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html#dp-importexport-ddb-prereq2",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To create the pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using a template, and then select the following template: Export DynamoDB table to S3.\n\nUnder Parameters, set DynamoDB table name to the name of your table. Click the folder icon next to Output S3 folder, select one of your Amazon S3 buckets, and then click Select.\n\nUnder Schedule, choose on pipeline activation.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Edit in Architect.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "To create the pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-sns",
"title": "ARN resource names for Amazon SNS"
},
{
"url": "https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-sns",
"title": "ARN resource names for Amazon SNS"
},
{
"url": "https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#arn-syntax-sns",
"title": "ARN resource names for Amazon SNS"
}
],
"text": "To configure the success, failure, and late notification actions\nIn the right pane, click Activities.\n\nFrom Add an optional field, select On Success.\n\nFrom the newly added On Success, select Create new: Action.\n\nFrom Add an optional field, select On Fail.\n\nFrom the newly added On Fail, select Create new: Action.\n\nFrom Add an optional field, select On Late Action.\n\nFrom the newly added On Late Action, select Create new: Action.\n\nIn the right pane, click Others.\n\nFor DefaultAction1, do the following:\n\nChange the name to SuccessSnsAlarm.\n\nFrom Type, select SnsAlarm.\n\nIn Topic Arn, enter the ARN of the topic that you created. See ARN resource names for Amazon SNS.\n\nEnter a subject and a message. For DefaultAction2, do the following:\n\nChange the name to FailureSnsAlarm.\n\nFrom Type, select SnsAlarm.\n\nIn Topic Arn, enter the ARN of the topic that you created (see ARN resource names for Amazon SNS.\n\nEnter a subject and a message. For DefaultAction3, do the following: Change the name to LateSnsAlarm.\n\nFrom Type, select SnsAlarm.\n\nIn Topic Arn, enter the ARN of the topic that you created (see ARN resource names for Amazon SNS.\n\nEnter a subject and a message.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "To configure the success, failure, and late notification actions",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"title": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
}
],
"text": "ImportantIf your pipeline uses an Amazon EMR release version in the 6.x series, you must add a bootstrap action to copy the following Jar file to the Hadoop classpath where MyRegion is the AWS Region where your pipeline runs: s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar. For more information, see Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies.In addition, you must change the first argument in the step field in EmrActivity with name TableBackupActivity from s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar to s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"title": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
}
],
"text": "If your pipeline uses an Amazon EMR release version in the 6.x series, you must add a bootstrap action to copy the following Jar file to the Hadoop classpath where MyRegion is the AWS Region where your pipeline runs: s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar. For more information, see Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies.In addition, you must change the first argument in the step field in EmrActivity with name TableBackupActivity from s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar to s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"title": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
}
],
"text": "If your pipeline uses an Amazon EMR release version in the 6.x series, you must add a bootstrap action to copy the following Jar file to the Hadoop classpath where MyRegion is the AWS Region where your pipeline runs: s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar. For more information, see Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "In addition, you must change the first argument in the step field in EmrActivity with name TableBackupActivity from s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar to s3://dynamodb-dpl-MyRegion/emr-ddb-storage-handler/4.14.0/emr-dynamodb-tools-4.14.0-jar-with-dependencies.jar.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html#dp-export-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline runs\nChoose Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then choose Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks. If you created an SNS notification, you should receive email about the successful completion of this task.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html#dp-export-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [],
"text": "To monitor the progress of your pipeline runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html#dp-export-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-pipelinejson-verifydata2",
"text": "Step 5: Verify the Data Export File"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html#dp-importexport-ddb-pipelinejson-verifydata2",
"main_header": "Step 5: Verify the Data Export File",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-pipelinejson-verifydata2",
"text": "Step 5: Verify the Data Export File"
},
"links": [],
"text": "To view the export file contents\nOpen the Amazon S3 console.\n\nOn the Buckets pane, click the Amazon S3 bucket that contains your file output (the example pipeline uses the output path s3://mybucket/output/MyTable) and open the output file with your preferred text editor. The output file name is an identifier value with no extension, such as this example: ae10f955-fb2f-4790-9b11-fbfea01a871e_000000.\n\nUsing your preferred text editor, view the contents of the output file and ensure that there is a data file that corresponds to the DynamoDB source table. The presence of this text file indicates that the export operation from DynamoDB to the output file occurred successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html#dp-importexport-ddb-pipelinejson-verifydata2",
"main_header": "Step 5: Verify the Data Export File",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-pipelinejson-verifydata2",
"text": "Step 5: Verify the Data Export File"
},
"links": [],
"text": "To view the export file contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html#dp-importexport-ddb-pipelinejson-verifydata2",
"main_header": "Step 5: Verify the Data Export File",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html#dp-export-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html#dp-export-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html#dp-export-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3",
"text": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html#dp-copydata-s3",
"main_header": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3",
"text": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
}
],
"text": "CopyActivity\n\nThe activity that AWS Data Pipeline performs for this pipeline (copy CSV data from one Amazon S3 bucket to another).\nImportantThere are limitations when using the CSV file format with CopyActivity and S3DataNode. For more information, see CopyActivity.\n\nSchedule\n\nThe start date, time, and the recurrence for this activity. You can optionally specify the end date and time.\n\nEc2Resource\n\nThe resource (an EC2 instance) that AWS Data Pipeline uses to perform this activity.\n\nS3DataNode\n\nThe input and output nodes (Amazon S3 buckets) for this pipeline.\n\nSnsAlarm\n\nThe action AWS Data Pipeline must take when the specified conditions are met (send Amazon SNS notifications to a topic after the task finishes successfully).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html#dp-copydata-s3",
"main_header": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-prereq.html#dp-copydata-s3-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-prereq",
"text": "Before You Begin"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
},
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html",
"title": "Create a Bucket"
},
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/gsg/PuttingAnObjectInABucket.html",
"title": "Add an Object to a Bucket"
},
{
"url": "https://docs.aws.amazon.com/sns/latest/gsg/CreateTopic.html",
"title": "Create a Topic"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "Complete the tasks in Setting up for AWS Data Pipeline.\n\n(Optional) Set up a VPC for the instance and a security group for the VPC. For more information, see Launching Resources for Your Pipeline into a VPC.\n\nCreate an Amazon S3 bucket as a data source. \nFor more information, see Create a Bucket in the Amazon Simple Storage Service User Guide.\n\nUpload your data to your Amazon S3 bucket. \nFor more information, see Add an Object to a Bucket in the Amazon Simple Storage Service User Guide.\n\nCreate another Amazon S3 bucket as a data target\n\nCreate a topic for sending email notification and make a note of the topic Amazon Resource Name (ARN). For more information, see Create a Topic in the Amazon Simple Notification Service Getting Started Guide.\n\n(Optional) This tutorial uses the default IAM role policies created by AWS Data Pipeline. If you would rather create and configure your own IAM role policy and trust relationships, follow the instructions described in IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-prereq.html#dp-copydata-s3-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-console",
"main_header": "Copy CSV Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"title": "Create the Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-save-pipeline-console",
"title": "Save and Validate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"title": "Activate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-execution-pipeline-console",
"title": "Monitor the Pipeline Runs"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-delete-pipeline-console",
"title": "(Optional) Delete Your Pipeline"
}
],
"text": "TasksCreate the PipelineSave and Validate Your PipelineActivate Your PipelineMonitor the Pipeline Runs(Optional) Delete Your Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-console",
"main_header": "Copy CSV Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-console",
"main_header": "Copy CSV Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To create the pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using Architect.\n\nUnder Schedule, choose on pipeline activation.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Edit in Architect.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To create the pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the activity for your pipeline\nClick Add activity.\n\nIn the Activities pane:\n\nIn the Name field, enter a name for the activity, for example, copy-myS3-data.\n\nFrom Type, select CopyActivity.\n\nFrom Output, select Create new: DataNode.\n\nFrom Schedule, select Create new: Schedule.\n\nFrom Input, select Create new: DataNode.\n\nFrom Add an optional field, select Runs On.\n\nFrom the newly added Runs On, select Create new: Resource.\n\nFrom Add an optional field, select On Success.\n\nFrom the newly added On Success, select Create new: Action. In the left pane, separate the icons by dragging them apart. This is a graphical representation of your pipeline. The arrows indicate the connections between the objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the activity for your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the input and output data nodes for your pipeline\nIn the right pane, click DataNodes.\n\nFor DefaultDataNode1, which represents your data source, do the following:\n\nEnter a name for your input node (for example, MyS3Input).\n\nFrom Type, select S3DataNode.\n\nFrom Schedule, select your schedule (for example, copy-S3data-schedule).\n\nFrom Add an optional field, select File Path.\n\nIn the File Path field, enter the path in Amazon S3 for your data source. For DefaultDataNode2, which represents your data target, do the following:\n\nEnter a name for your output node (for example, MyS3Output).\n\nFrom Type, select S3DataNode. From Schedule, select your schedule (for example, copy-S3data-schedule).\n\nFrom Add an optional field, select File Path.\n\nIn the File Path field, enter the path in Amazon S3 for your data target.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the input and output data nodes for your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the resource\nIn the right pane, click Resources.\n\nEnter a name for your resource (for example, CopyDataInstance).\n\nFrom Type, select Ec2Resource.\n\nFrom Schedule, select your schedule (for example, copy-S3data-schedule).\n\nLeave Resource Role and Role set to their default values.\nIf you have created your own IAM roles, you can select them if you prefer.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the resource",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the notification action\nIn the right pane, click Others.\n\nUnder DefaultAction1, do the following:\n\nEnter a name for your notification (for example, CopyDataNotice).\n\nFrom Type, select SnsAlarm.\n\nIn the Subject field, enter the subject line for your notification.\n\nIn the Topic Arn field, enter the ARN of your topic.\n\nIn the Message field, enter the message content.\n\nLeave Role field set to the default value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the notification action",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline runs\nChoose Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then choose Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks. If you created an SNS notification, you should receive email about the successful completion of this task.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "To monitor the progress of your pipeline runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#accessing-datapipeline",
"title": "Accessing AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "PrerequisitesBefore you begin, you must complete the following steps:\nInstall and configure a command line interface (CLI). For more information, see Accessing AWS Data Pipeline.\n\nEnsure that the IAM roles named DataPipelineDefaultRole and DataPipelineDefaultResourceRole exist. The AWS Data Pipeline console creates these roles for you automatically. If you haven't used the AWS Data Pipeline console at least once, then you must create these roles manually. For more information, see IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [],
"text": "Before you begin, you must complete the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"title": "Define a Pipeline in JSON Format"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"title": "Upload and Activate the Pipeline Definition"
}
],
"text": "TasksDefine a Pipeline in JSON FormatUpload and Activate the Pipeline Definition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "Note We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-s3-node-cli",
"text": "Amazon S3 Data Nodes"
},
"links": [],
"text": "Id\n\nThe user-defined name for the input location (a label for your reference only).\n\nType\n\nThe pipeline component type, which is \"S3DataNode\" to match the location where the data resides, in an Amazon S3 bucket.\n\nSchedule\n\nA reference to the schedule component that we created in the preceding lines of the JSON file labeled \u00e2\u0080\u009cMySchedule\u00e2\u0080\u009d.\n\nPath\n\nThe path to the data associated with the data node. The syntax for a data node is determined by its type. For example, the syntax for an Amazon S3 path follows a different syntax that is appropriate for a database table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-s3-node-cli",
"main_header": "Amazon S3 Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-resource-cli",
"text": "Resource"
},
"links": [
{
"url": "http://aws.amazon.com/ec2/instance-types/",
"title": "Amazon EC2 Instance Types"
}
],
"text": "Id\nThe user-defined name for the pipeline schedule, which is a label for your reference only.\nType\nThe type of computational resource to perform work; in this case, an EC2 instance. There are other resource types available, such as an EmrCluster type.\nSchedule\nThe schedule on which to create this computational resource.\ninstanceType\nThe size of the EC2 instance to create. Ensure that you set the appropriate size of EC2 instance that best matches the load of the work that you want to perform with AWS Data Pipeline. In this case, we set an m1.medium EC2 instance. For more information about the different instance types and when to use each one, see Amazon EC2 Instance Types topic at http://aws.amazon.com/ec2/instance-types/.\nRole\nThe IAM role of the account that accesses resources, such as accessing an Amazon S3 bucket to retrieve data.\nresourceRole\nThe IAM role of the account that creates resources, such as creating and configuring an EC2 instance on your behalf. Role and ResourceRole can be the same role, but separately provide greater granularity in your security configuration.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-activity-cli",
"text": "Activity"
},
"links": [],
"text": "Id\n\nThe user-defined name for the activity, which is a label for your reference only.\n\nType\n\nThe type of activity to perform, such as MyCopyActivity.\n\nrunsOn\n\nThe computational resource that performs the work that this activity defines. In this example, we provide a reference to the EC2 instance defined previously. Using the runsOn field causes AWS Data Pipeline to create the EC2 instance for you. The runsOn field indicates that the resource exists in the AWS infrastructure, while the workerGroup value indicates that you want to use your own on-premises resources to perform the work.\n\nInput\n\nThe location of the data to copy.\n\nOutput\n\nThe target location data.\n\nSchedule\n\nThe schedule on which to run this activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql",
"text": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html#dp-copydata-mysql",
"main_header": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql",
"text": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"title": "MySqlDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
}
],
"text": "CopyActivityEc2ResourceMySqlDataNodeS3DataNodeSnsAlarm",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html#dp-copydata-mysql",
"main_header": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html#dp-copydata-mysql-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-prereq",
"text": "Before You Begin"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
},
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html",
"title": "Create a Bucket"
},
{
"url": "https://docs.aws.amazon.com/AmazonRDS/latest/GettingStartedGuide/LaunchDBInstance.html",
"title": "Launch a DB Instance"
},
{
"url": "https://dev.mysql.com/doc/refman/8.0/en/creating-tables.html",
"title": "Create a Table"
},
{
"url": "https://dev.mysql.com/doc/refman/8.0/en/creating-tables.html",
"title": "Create a Table"
},
{
"url": "http://www.mysql.com/products/workbench/",
"title": "MySQL Workbench product page"
},
{
"url": "https://docs.aws.amazon.com/sns/latest/gsg/CreateTopic.html",
"title": "Create a Topic"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "Complete the tasks in Setting up for AWS Data Pipeline.\n\n(Optional) Set up a VPC for the instance and a security group for the VPC. For more information, see Launching Resources for Your Pipeline into a VPC.\n\nCreate an Amazon S3 bucket as a data output.\nFor more information, see Create a Bucket in Amazon Simple Storage Service User Guide.\n\nCreate and launch a MySQL database instance as your data source. \nFor more information, see Launch a DB Instance in the Amazon RDS Getting Started Guide. After you have an Amazon RDS instance, see Create a Table in the MySQL documentation.\nNoteMake a note of the user name and the password you used for creating the MySQL instance. After you've launched your MySQL database instance, make a note of the instance's endpoint. You'll need this information later.\n\nConnect to your MySQL database instance, create a table, and then add test data values to the newly created table.\nFor illustration purposes, we created this tutorial using a MySQL table with the following configuration and sample data. The following screen shot is from MySQL Workbench 5.2 CE: For more information, see Create a Table in the MySQL documentation and the MySQL Workbench product page.\n\nCreate a topic for sending email notification and make a note of the topic Amazon Resource Name (ARN). For more information, see Create a Topic in Amazon Simple Notification Service Getting Started Guide.\n\n(Optional) This tutorial uses the default IAM role policies created by AWS Data Pipeline. If you would rather create and configure your IAM role policy and trust relationships, follow the instructions described in IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html#dp-copydata-mysql-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-console",
"main_header": "Copy MySQL Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"title": "Create the Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-save-pipeline-console",
"title": "Save and Validate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-verify-pipeline-console",
"title": "Verify Your Pipeline Definition"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"title": "Activate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-execution-pipeline-console",
"title": "Monitor the Pipeline Runs"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-delete-pipeline-console",
"title": "(Optional) Delete Your Pipeline"
}
],
"text": "TasksCreate the PipelineSave and Validate Your PipelineVerify Your Pipeline DefinitionActivate Your PipelineMonitor the Pipeline Runs(Optional) Delete Your Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-console",
"main_header": "Copy MySQL Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-console",
"main_header": "Copy MySQL Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
},
{
"url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html",
"title": "Connecting to a DB Instance Running the MySQL Database Engine"
}
],
"text": "To create your pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using a template, and then select the following template: Full copy of RDS MySQL table to S3.\n\nUnder the Parameters section, which opened when you selected the template, do the following:\n\nFor DBInstance ID, enter the DB instance name of the Aurora DB instance you want to use to copy data from the Aurora cluster.\nTo locate the endpoint details for your DB instance, see Connecting to a DB Instance Running the MySQL Database Engine in the Amazon RDS User Guide. For RDS MySQL username, enter the user name you used when you created your MySQL database instance.\n\nIn the RDS MySQL password field, enter the password you used when you created your DB instance.\n\nIn the EC2 instance type field, enter the instance type for your EC2 instance.\n\nClick the folder icon next to Output S3 folder, select one of your buckets or folders, and then click Select. Under Schedule, choose on pipeline activation.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Edit in Architect.\n\nIn the left pane, separate the icons by dragging them apart. This is a graphical representation of your pipeline. The arrows indicate the connections between the objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To create your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "In the left pane, click RDSDatabase.In the right pane, under the rds_mysql section, for Add an optional field... choose Database Name.Type the Database Name of your target database and add optional fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the Amazon SNS notification action\nIn the right pane, click Activities.\n\nFrom Add an optional field, select On Success.\n\nFrom the newly added On Success, select Create new: Action.\n\nIn the right pane, click Others.\n\nUnder DefaultAction1, do the following:\n\nEnter a name for your notification (for example, CopyDataNotice).\n\nFrom Type, select SnsAlarm.\n\nIn the Message field, enter the message content.\n\nIn the Subject field, enter the subject line for your notification.\n\nIn the Topic Arn field, enter the ARN of your topic.\n\nLeave Role field set to the default value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To configure the Amazon SNS notification action",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-verify-pipeline-console",
"text": "Verify Your Pipeline Definition"
},
"links": [],
"text": "To verify your pipeline definition \nOn the List Pipelines page, look for your newly-created pipeline.\nAWS Data Pipeline has created a unique Pipeline ID for your pipeline definition. \nThe Schedule State column in the row listing your pipeline should show PENDING.\n\nChoose the triangle icon next to your pipeline. A pipeline summary pane below shows the details of your pipeline runs. Because your pipeline is not yet activated, you are not likely to see any execution details. However, you will see the configuration of the pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-verify-pipeline-console",
"main_header": "Verify Your Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-verify-pipeline-console",
"text": "Verify Your Pipeline Definition"
},
"links": [],
"text": "To verify your pipeline definition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-verify-pipeline-console",
"main_header": "Verify Your Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline runs\nChoose Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then choose Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks. If you created an SNS notification, you should receive email about the successful completion of this task.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "To monitor the progress of your pipeline runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#accessing-datapipeline",
"title": "Accessing AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html",
"title": "Before You Begin"
}
],
"text": "PrerequisitesBefore you begin, you must complete the following steps:\nInstall and configure a command line interface (CLI). For more information, see Accessing AWS Data Pipeline.\n\nEnsure that the IAM roles named DataPipelineDefaultRole and DataPipelineDefaultResourceRole exist. The AWS Data Pipeline console creates these roles for you automatically. If you haven't used the AWS Data Pipeline console at least once, then you must create these roles manually. For more information, see IAM Roles for AWS Data Pipeline.\n\nSet up an Amazon S3 bucket and an Amazon RDS instance. For more information, see Before You Begin.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [],
"text": "Before you begin, you must complete the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"title": "Define a Pipeline in JSON Format"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"title": "Upload and Activate the Pipeline Definition"
}
],
"text": "TasksDefine a Pipeline in JSON FormatUpload and Activate the Pipeline Definition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "Note We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-rds-node-cli",
"text": "MySQL Data Node"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html",
"title": "Special Characters"
}
],
"text": "Id\nThe user-defined name, which is a label for your reference only.\nUsername\nThe user name of the database account that has sufficient permission to retrieve data from the database table. Replace my-username with the name of your user account.\nSchedule\nA reference to the schedule component that we created in the preceding lines of the JSON file.\nName\nThe user-defined name, which is a label for your reference only.\n*Password\nThe password for the database account with the asterisk prefix to indicate that AWS Data Pipeline must encrypt the password value. Replace my-password with the correct password for your user account. The password field is preceded by the asterisk special character. For more information, see Special Characters.\nTable\nThe name of the database table that contains the data to copy. Replace table-name with the name of your database table.\nconnectionString\nThe JDBC connection string for the CopyActivity object to connect to the database.\nselectQuery\nA valid SQL SELECT query that specifies which data to copy from the database table. Note that #{table} is an expression that re-uses the table name provided by the \"table\" variable in the preceding lines of the JSON file.\nType\nThe SqlDataNode type, which is an Amazon RDS instance using MySQL in this example.NoteThe MySqlDataNode type is deprecated. While you can still use MySqlDataNode, we recommend using SqlDataNode.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-rds-node-cli",
"main_header": "MySQL Data Node",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-json-s3-node-cli",
"text": "Amazon S3 Data Node"
},
"links": [],
"text": "Id\nThe user-defined ID, which is a label for your reference only.\nSchedule\nA reference to the schedule component that we created in the preceding lines of the JSON file.\nfilePath\nThe path to the data associated with the data node, which is an CSV output file in this example. \nName\nThe user-defined name, which is a label for your reference only.\nType\nThe pipeline object type, which is S3DataNode to match the location where the data resides, in an Amazon S3 bucket.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-s3-node-cli",
"main_header": "Amazon S3 Data Node",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-json-resource-cli",
"text": "Resource"
},
"links": [],
"text": "Id\nThe user-defined ID, which is a label for your reference only.\nSchedule\nThe schedule on which to create this computational resource.\nName\nThe user-defined name, which is a label for your reference only.\nRole\nThe IAM role of the account that accesses resources, such as accessing an Amazon S3 bucket to retrieve data.\nType\nThe type of computational resource to perform work; in this case, an EC2 instance. There are other resource types available, such as an EmrCluster type.\nresourceRole\nThe IAM role of the account that creates resources, such as creating and configuring an EC2 instance on your behalf. Role and ResourceRole can be the same role, but separately provide greater granularity in your security configuration.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-json-activity-cli",
"text": "Activity"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
}
],
"text": "Id\nThe user-defined ID, which is a label for your reference only\nInput\nThe location of the MySQL data to copy\nSchedule\nThe schedule on which to run this activity\nName\nThe user-defined name, which is a label for your reference only\nrunsOn\nThe computational resource that performs the work that this activity defines. In this example, we provide a reference to the EC2 instance defined previously. Using the runsOn field causes AWS Data Pipeline to create the EC2 instance for you. The runsOn field indicates that the resource exists in the AWS infrastructure, while the workerGroup value indicates that you want to use your own on-premises resources to perform the work.\nonSuccess\nThe SnsAlarm to send if the activity completes successfully\nonFail\nThe SnsAlarm to send if the activity fails\nOutput\nThe Amazon S3 location of the CSV output file\nType\nThe type of activity to perform.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift",
"text": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html#dp-copydata-redshift",
"main_header": "Copy Data to Amazon Redshift Using AWS Data Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [],
"text": "Load data from Amazon S3.\n\nSet up the COPY activity in Amazon Redshift.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-prereq",
"text": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html#dp-copydata-redshift-prereq",
"main_header": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-prereq",
"text": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html",
"title": "Getting Started with Amazon Redshift"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/managing-clusters-console.html#create-cluster",
"title": "Creating a Cluster"
}
],
"text": "To set up for the tutorial\nComplete the tasks in Setting up for AWS Data Pipeline.\n\nCreate a security group.\nOpen the Amazon EC2 console.In the navigation pane, click Security Groups.Click Create Security Group.Specify a name and description for the security group.[EC2-Classic] Select No VPC for VPC.[EC2-VPC] Select the ID of your VPC for VPC.Click Create.\n\n[EC2-Classic] Create an Amazon Redshift cluster security group and specify the Amazon EC2 security group.\nOpen the Amazon Redshift console.In the navigation pane, click Security Groups.Click Create Cluster Security Group.In the Create Cluster Security Group dialog box, specify a name and description for the cluster security group.Click the name of the new cluster security group.Click Add Connection Type.In the Add Connection Type dialog box, select EC2 Security Group from Connection Type, select the security group that you created from EC2 Security Group Name, and then click Authorize.\n\n[EC2-VPC] Create an Amazon Redshift cluster security group and specify the VPC security group.\nOpen the Amazon EC2 console.In the navigation pane, click Security Groups.Click Create Security Group.In the Create Security Group dialog box, specify a name and description for the security group, and select the ID of your VPC for VPC.Click Add Rule. Specify the type, protocol, and port range, and start typing the ID of the security group in Source. Select the security group that you created in the second step.Click Create.\n\nThe following is a summary of the steps.\nIf you have an existing Amazon Redshift cluster, make a note of the cluster ID. \nTo create a new cluster and load sample data, follow the steps in Getting Started with Amazon Redshift. For more information about creating clusters, see Creating a Cluster in the Amazon Redshift Cluster Management Guide. \nOpen the Amazon Redshift console.Click Launch Cluster.Provide the required details for your cluster, and then click Continue.Provide the node configuration, and then click Continue.On the page for additional configuration information, select the cluster security group that you created, and then click Continue.Review the specifications for your cluster, and then click Launch Cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html#dp-copydata-redshift-prereq",
"main_header": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-prereq",
"text": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster"
},
"links": [],
"text": "To set up for the tutorial",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html#dp-copydata-redshift-prereq",
"main_header": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-define-objects-console",
"title": "Create the Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-save-pipeline-console",
"title": "Save and Validate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"title": "Activate Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-execution-pipeline-console",
"title": "Monitor the Pipeline Runs"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-delete-pipeline-console",
"title": "(Optional) Delete Your Pipeline"
}
],
"text": "TasksCreate the PipelineSave and Validate Your PipelineActivate Your PipelineMonitor the Pipeline Runs(Optional) Delete Your Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [],
"text": "Tasks",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://console.aws.amazon.com/datapipeline/",
"title": "https://console.aws.amazon.com/datapipeline/"
}
],
"text": "To create the pipeline\nOpen the AWS Data Pipeline console at https://console.aws.amazon.com/datapipeline/.\n\nThe first screen that you see depends on whether you've created a pipeline in the current region.\n\nIf you haven't created a pipeline in this region, the console displays an introductory screen. Choose Get started now.\n\nIf you've already created a pipeline in this region, the console displays a page that lists your pipelines for the region. Choose Create new pipeline. In Name, enter a name for your pipeline.\n\n(Optional) In Description, enter a description for your pipeline.\n\nFor Source, select Build using a template, and then select the following template: Load data from S3 into Redshift.\n\nUnder Parameters, provide information about your input folder in Amazon S3 and the Amazon Redshift database that you created.\n\nUnder Schedule, choose on pipeline activation.\n\nUnder Pipeline Configuration, leave logging enabled. Choose the folder icon under S3 location for logs, select one of your buckets or folders, and then choose Select.\nIf you prefer, you can disable logging instead.\n\nUnder Security/Access, leave IAM roles set to Default.\n\nClick Activate.\nIf you prefer, you can choose Edit in Architect to modify this pipeline. For example, you can add preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "To create the pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline\nChoose Save pipeline.\n\nAWS Data Pipeline validates your pipeline definition and returns either success or error or warning messages. If you get an error message, choose Close and then, in the right pane, choose Errors/Warnings.\n\nThe Errors/Warnings pane lists the objects that failed validation. Choose the plus (+) sign next to the object names and look for an error message in red.\n\nWhen you see an error message, go to the specific object pane where you see the error and fix it. For example, if you see an error message in the DataNodes object, go to the DataNodes pane to fix the error.\n\nAfter you fix the errors listed in the Errors/Warnings pane, choose Save Pipeline.\n\nRepeat the process until your pipeline validates successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "To save and validate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "ImportantIf activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/pricing",
"title": "AWS Data Pipeline pricing"
}
],
"text": "If activation succeeds, your pipeline is running and might incur usage charges. For more information, see AWS Data Pipeline pricing. To stop incurring usage charges for AWS Data Pipeline, delete your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline\nChoose Activate.\n\nIn the confirmation dialog box, choose Close.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "To activate your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "To monitor the progress of your pipeline runs\nChoose Update or press F5 to update the status displayed.\nTipIf there are no runs listed, ensure that Start (in UTC) and End (in UTC) cover the scheduled start and end of your pipeline, and then choose Update.\n\nWhen the status of every object in your pipeline is FINISHED, your pipeline has successfully completed the scheduled tasks. If you created an SNS notification, you should receive email about the successful completion of this task.\n\nIf your pipeline doesn't complete successfully, check your pipeline settings for issues. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "To monitor the progress of your pipeline runs",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline\nOn the List Pipelines page, select your pipeline.\n\nClick Actions, and then choose Delete.\n\nWhen prompted for confirmation, choose Delete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To delete your pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-cli",
"text": "Copy Data to Amazon Redshift Using the Command Line"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html#dp-copydata-redshift-cli",
"main_header": "Copy Data to Amazon Redshift Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-cli",
"text": "Copy Data to Amazon Redshift Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#accessing-datapipeline",
"title": "Accessing AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"title": "Before You Begin: Configure COPY Options and Load Data"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html",
"title": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster"
}
],
"text": "PrerequisitesBefore you begin, you must complete the following steps:\nInstall and configure a command line interface (CLI). For more information, see Accessing AWS Data Pipeline.\n\nEnsure that the IAM roles named DataPipelineDefaultRole and DataPipelineDefaultResourceRole exist. The AWS Data Pipeline console creates these roles for you automatically. If you haven't used the AWS Data Pipeline console at least once, then you must create these roles manually. For more information, see IAM Roles for AWS Data Pipeline.\n\nSet up the COPY command in Amazon Redshift, since you will need to have these same options working when you perform the copying within AWS Data Pipeline. For information, see Before You Begin: Configure COPY Options and Load Data.\n\nSet up an Amazon Redshift database. For more information, see Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html#dp-copydata-redshift-cli",
"main_header": "Copy Data to Amazon Redshift Using the Command Line",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-cli",
"text": "Copy Data to Amazon Redshift Using the Command Line"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html#dp-copydata-redshift-cli",
"main_header": "Copy Data to Amazon Redshift Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-cli",
"text": "Copy Data to Amazon Redshift Using the Command Line"
},
"links": [],
"text": "Before you begin, you must complete the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html#dp-copydata-redshift-cli",
"main_header": "Copy Data to Amazon Redshift Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html#dp-copydata-redshift-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "id\n\nThe user-defined ID, which is a label for your reference only.\n\nschedule\n\nA reference to the schedule component.\n\nfilePath\n\nThe path to the data associated with the data node, which is an CSV input file in this example.\n\nname\n\nThe user-defined name, which is a label for your reference only.\n\ndataFormat\n\nA reference to the format of the data for the activity to process.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "id\n\nThe user-defined ID, which is a label for your reference only.\n\nschedule\n\nA reference to the schedule component.\n\ntableName\n\nThe name of the Amazon Redshift table.\n\nname\n\nThe user-defined name, which is a label for your reference only.\n\ncreateTableSql\n\nA SQL expression to create the table in the database.\n\ndatabase\n\nA reference to the Amazon Redshift database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "id\n\nThe user-defined ID, which is a label for your reference only.\n\ndatabaseName\n\nThe name of the logical database.\n\nusername\n\nThe user name to connect to the database.\n\nname\n\nThe user-defined name, which is a label for your reference only.\n\npassword\n\nThe password to connect to the database.\n\nclusterId\n\nThe ID of the Redshift cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-resource-cli",
"text": "Resource"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html#dp-copydata-redshift-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-resource-cli",
"text": "Resource"
},
"links": [],
"text": "id\n\nThe user-defined ID, which is a label for your reference only.\n\nschedule\n\nThe schedule on which to create this computational resource.\n\nsecurityGroups\n\nThe security group to use for the instances in the resource pool.\n\nname\n\nThe user-defined name, which is a label for your reference only.\n\nrole\n\nThe IAM role of the account that accesses resources, such as accessing an Amazon S3 bucket to retrieve data.\n\nlogUri\n\nThe Amazon S3 destination path to back up Task Runner logs from the Ec2Resource.\n\nresourceRole\n\nThe IAM role of the account that creates resources, such as creating and configuring an EC2 instance on your behalf. Role and ResourceRole can be the same role, but separately provide greater granularity in your security configuration.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html#dp-copydata-redshift-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-activity-cli",
"text": "Activity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html#dp-copydata-redshift-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-activity-cli",
"text": "Activity"
},
"links": [],
"text": "id\n\nThe user-defined ID, which is a label for your reference only.\n\ninput\n\nA reference to the Amazon S3 source file.\n\nschedule\n\nThe schedule on which to run this activity.\n\ninsertMode\n\nThe insert type (KEEP_EXISTING, OVERWRITE_EXISTING, or TRUNCATE).\n\nname\n\nThe user-defined name, which is a label for your reference only.\n\nrunsOn\n\nThe computational resource that performs the work that this activity defines.\n\noutput\n\nA reference to the Amazon Redshift destination table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html#dp-copydata-redshift-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "div",
"children_tags": [
"dl"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-expressions-functions",
"main_header": "Pipeline Expressions and Functions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-datetime",
"title": "DateTime"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-numeric",
"title": "Numeric"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-object-reference",
"title": "Object References"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-period",
"title": "Period"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-section",
"title": "String"
}
],
"text": "TypesDateTimeNumericObject ReferencesPeriodString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-pipeline-datatypes",
"main_header": "Simple Data Types",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"links": [],
"text": "Types",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-pipeline-datatypes",
"main_header": "Simple Data Types",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions",
"main_header": "Expressions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "NoteYou can create pipelines that have dependencies, such as tasks in your pipeline that depend on the work of other systems or tasks. If your pipeline requires certain resources, add those dependencies to the pipeline using preconditions that you associate with data nodes and tasks. This makes your pipelines easier to debug and more resilient. Additionally, keep your dependencies within a single pipeline when possible, because cross-pipeline troubleshooting is difficult.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "You can create pipelines that have dependencies, such as tasks in your pipeline that depend on the work of other systems or tasks. If your pipeline requires certain resources, add those dependencies to the pipeline using preconditions that you associate with data nodes and tasks. This makes your pipelines easier to debug and more resilient. Additionally, keep your dependencies within a single pipeline when possible, because cross-pipeline troubleshooting is difficult.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "You can create pipelines that have dependencies, such as tasks in your pipeline that depend on the work of other systems or tasks. If your pipeline requires certain resources, add those dependencies to the pipeline using preconditions that you associate with data nodes and tasks. This makes your pipelines easier to debug and more resilient. Additionally, keep your dependencies within a single pipeline when possible, because cross-pipeline troubleshooting is difficult.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-node",
"text": "Node Expression"
},
"links": [],
"text": "NoteThe fields preceded by the AT (@) sign indicate those fields are runtime fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-node",
"main_header": "Node Expression",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-node",
"text": "Node Expression"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-node",
"main_header": "Node Expression",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-node",
"text": "Node Expression"
},
"links": [],
"text": "The fields preceded by the AT (@) sign indicate those fields are runtime fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-node",
"main_header": "Node Expression",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-node",
"text": "Node Expression"
},
"links": [],
"text": "The fields preceded by the AT (@) sign indicate those fields are runtime fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-node",
"main_header": "Node Expression",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-math",
"text": "Mathematical Functions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html#dp-pipeline-reference-functions-math",
"main_header": "Mathematical Functions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-math",
"text": "Mathematical Functions"
},
"links": [],
"text": "Function\nDescription + Addition.\nExample: #{1 + 2}\nResult: 3 - Subtraction.\nExample: #{1 - 2}\nResult: -1 * Multiplication.\nExample: #{1 * 2}\nResult: 2 / Division. If you divide two integers, the result is truncated.\nExample: #{1 / 2}, Result: 0\nExample: #{1.0 / 2}, Result: .5 ^ Exponent.\nExample: #{2 ^ 2}\nResult: 4.0",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html#dp-pipeline-reference-functions-math",
"main_header": "Mathematical Functions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-math",
"text": "Mathematical Functions"
},
"links": [],
"text": "Function\nDescription + Addition.\nExample: #{1 + 2}\nResult: 3 - Subtraction.\nExample: #{1 - 2}\nResult: -1 * Multiplication.\nExample: #{1 * 2}\nResult: 2 / Division. If you divide two integers, the result is truncated.\nExample: #{1 / 2}, Result: 0\nExample: #{1.0 / 2}, Result: .5 ^ Exponent.\nExample: #{2 ^ 2}\nResult: 4.0",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html#dp-pipeline-reference-functions-math",
"main_header": "Mathematical Functions",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-string",
"text": "String Functions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html#dp-pipeline-reference-functions-string",
"main_header": "String Functions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-string",
"text": "String Functions"
},
"links": [],
"text": "Function\nDescription + Concatenation. Non-string values are first converted to strings.\nExample: #{\"hel\" + \"lo\"}\nResult: \"hello\"",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html#dp-pipeline-reference-functions-string",
"main_header": "String Functions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-string",
"text": "String Functions"
},
"links": [],
"text": "Function\nDescription + Concatenation. Non-string values are first converted to strings.\nExample: #{\"hel\" + \"lo\"}\nResult: \"hello\"",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html#dp-pipeline-reference-functions-string",
"main_header": "String Functions",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [
{
"url": "http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html",
"title": "Joda Time - Class DateTimeFormat"
}
],
"text": "NoteThe date/time format for AWS Data Pipeline is Joda Time, which is a replacement for the Java date and time classes. For more information, see Joda Time - Class DateTimeFormat.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [
{
"url": "http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html",
"title": "Joda Time - Class DateTimeFormat"
}
],
"text": "The date/time format for AWS Data Pipeline is Joda Time, which is a replacement for the Java date and time classes. For more information, see Joda Time - Class DateTimeFormat.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [
{
"url": "http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html",
"title": "Joda Time - Class DateTimeFormat"
}
],
"text": "The date/time format for AWS Data Pipeline is Joda Time, which is a replacement for the Java date and time classes. For more information, see Joda Time - Class DateTimeFormat.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [],
"text": "Function\nDescription int day(DateTime myDateTime) Gets the day of the DateTime value as an integer.\nExample: #{day(myDateTime)}\nResult: 24 int dayOfYear(DateTime myDateTime) Gets the day of the year of the DateTime value as an integer.\nExample: #{dayOfYear(myDateTime)}\nResult: 144 DateTime firstOfMonth(DateTime myDateTime) Creates a DateTime object for the start of the month in the specified DateTime.\nExample: #{firstOfMonth(myDateTime)}\nResult: \"2011-05-01T17:10:00z\" String format(DateTime myDateTime,String format) Creates a String object that is the result of converting the specified DateTime using the specified format string.\nExample: #{format(myDateTime,'YYYY-MM-dd HH:mm:ss z')}\nResult: \"2011-05-24T17:10:00 UTC\" int hour(DateTime myDateTime) Gets the hour of the DateTime value as an integer.\nExample: #{hour(myDateTime)}\nResult: 17 DateTime makeDate(int year,int month,int day) Creates a DateTime object, in UTC, with the specified year, month, and day, at midnight.\nExample: #{makeDate(2011,5,24)}\nResult: \"2011-05-24T0:00:00z\" DateTime makeDateTime(int year,int month,int day,int hour,int minute) Creates a DateTime object, in UTC, with the specified year, month, day, hour, and minute.\nExample: #{makeDateTime(2011,5,24,14,21)}\nResult: \"2011-05-24T14:21:00z\" DateTime midnight(DateTime myDateTime) Creates a DateTime object for the current midnight, relative to the specified DateTime. For example, where MyDateTime is 2011-05-25T17:10:00z, the result is as follows. \nExample: #{midnight(myDateTime)}\nResult: \"2011-05-25T0:00:00z\" DateTime minusDays(DateTime myDateTime,int daysToSub) Creates a DateTime object that is the result of subtracting the specified number of days from the specified DateTime.\nExample: #{minusDays(myDateTime,1)}\nResult: \"2011-05-23T17:10:00z\" DateTime minusHours(DateTime myDateTime,int hoursToSub) Creates a DateTime object that is the result of subtracting the specified number of hours from the specified DateTime.\nExample: #{minusHours(myDateTime,1)}\nResult: \"2011-05-24T16:10:00z\" DateTime minusMinutes(DateTime myDateTime,int minutesToSub) Creates a DateTime object that is the result of subtracting the specified number of minutes from the specified DateTime.\nExample: #{minusMinutes(myDateTime,1)}\nResult: \"2011-05-24T17:09:00z\" DateTime minusMonths(DateTime myDateTime,int monthsToSub) Creates a DateTime object that is the result of subtracting the specified number of months from the specified DateTime.\nExample: #{minusMonths(myDateTime,1)}\nResult: \"2011-04-24T17:10:00z\" DateTime minusWeeks(DateTime myDateTime,int weeksToSub) Creates a DateTime object that is the result of subtracting the specified number of weeks from the specified DateTime.\nExample: #{minusWeeks(myDateTime,1)}\nResult: \"2011-05-17T17:10:00z\" DateTime minusYears(DateTime myDateTime,int yearsToSub) Creates a DateTime object that is the result of subtracting the specified number of years from the specified DateTime.\nExample: #{minusYears(myDateTime,1)}\nResult: \"2010-05-24T17:10:00z\" int minute(DateTime myDateTime) Gets the minute of the DateTime value as an integer.\nExample: #{minute(myDateTime)}\nResult: 10 int month(DateTime myDateTime) Gets the month of the DateTime value as an integer.\nExample: #{month(myDateTime)}\nResult: 5 DateTime plusDays(DateTime myDateTime,int daysToAdd) Creates a DateTime object that is the result of adding the specified number of days to the specified DateTime.\nExample: #{plusDays(myDateTime,1)}\nResult: \"2011-05-25T17:10:00z\" DateTime plusHours(DateTime myDateTime,int hoursToAdd) Creates a DateTime object that is the result of adding the specified number of hours to the specified DateTime.\nExample: #{plusHours(myDateTime,1)}\nResult: \"2011-05-24T18:10:00z\" DateTime plusMinutes(DateTime myDateTime,int minutesToAdd) Creates a DateTime object that is the result of adding the specified number of minutes to the specified DateTime.\nExample: #{plusMinutes(myDateTime,1)}\nResult: \"2011-05-24 17:11:00z\" DateTime plusMonths(DateTime myDateTime,int monthsToAdd) Creates a DateTime object that is the result of adding the specified number of months to the specified DateTime.\nExample: #{plusMonths(myDateTime,1)}\nResult: \"2011-06-24T17:10:00z\" DateTime plusWeeks(DateTime myDateTime,int weeksToAdd) Creates a DateTime object that is the result of adding the specified number of weeks to the specified DateTime.\nExample: #{plusWeeks(myDateTime,1)}\nResult: \"2011-05-31T17:10:00z\" DateTime plusYears(DateTime myDateTime,int yearsToAdd) Creates a DateTime object that is the result of adding the specified number of years to the specified DateTime.\nExample: #{plusYears(myDateTime,1)}\nResult: \"2012-05-24T17:10:00z\" DateTime sunday(DateTime myDateTime) Creates a DateTime object for the previous Sunday, relative to the specified DateTime. If the specified DateTime is a Sunday, the result is the specified DateTime.\nExample: #{sunday(myDateTime)}\nResult: \"2011-05-22 17:10:00 UTC\" int year(DateTime myDateTime) Gets the year of the DateTime value as an integer.\nExample: #{year(myDateTime)}\nResult: 2011 DateTime yesterday(DateTime myDateTime) Creates a DateTime object for the previous day, relative to the specified DateTime. The result is the same as minusDays(1).\nExample: #{yesterday(myDateTime)}\nResult: \"2011-05-23T17:10:00z\"",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [],
"text": "Function\nDescription int day(DateTime myDateTime) Gets the day of the DateTime value as an integer.\nExample: #{day(myDateTime)}\nResult: 24 int dayOfYear(DateTime myDateTime) Gets the day of the year of the DateTime value as an integer.\nExample: #{dayOfYear(myDateTime)}\nResult: 144 DateTime firstOfMonth(DateTime myDateTime) Creates a DateTime object for the start of the month in the specified DateTime.\nExample: #{firstOfMonth(myDateTime)}\nResult: \"2011-05-01T17:10:00z\" String format(DateTime myDateTime,String format) Creates a String object that is the result of converting the specified DateTime using the specified format string.\nExample: #{format(myDateTime,'YYYY-MM-dd HH:mm:ss z')}\nResult: \"2011-05-24T17:10:00 UTC\" int hour(DateTime myDateTime) Gets the hour of the DateTime value as an integer.\nExample: #{hour(myDateTime)}\nResult: 17 DateTime makeDate(int year,int month,int day) Creates a DateTime object, in UTC, with the specified year, month, and day, at midnight.\nExample: #{makeDate(2011,5,24)}\nResult: \"2011-05-24T0:00:00z\" DateTime makeDateTime(int year,int month,int day,int hour,int minute) Creates a DateTime object, in UTC, with the specified year, month, day, hour, and minute.\nExample: #{makeDateTime(2011,5,24,14,21)}\nResult: \"2011-05-24T14:21:00z\" DateTime midnight(DateTime myDateTime) Creates a DateTime object for the current midnight, relative to the specified DateTime. For example, where MyDateTime is 2011-05-25T17:10:00z, the result is as follows. \nExample: #{midnight(myDateTime)}\nResult: \"2011-05-25T0:00:00z\" DateTime minusDays(DateTime myDateTime,int daysToSub) Creates a DateTime object that is the result of subtracting the specified number of days from the specified DateTime.\nExample: #{minusDays(myDateTime,1)}\nResult: \"2011-05-23T17:10:00z\" DateTime minusHours(DateTime myDateTime,int hoursToSub) Creates a DateTime object that is the result of subtracting the specified number of hours from the specified DateTime.\nExample: #{minusHours(myDateTime,1)}\nResult: \"2011-05-24T16:10:00z\" DateTime minusMinutes(DateTime myDateTime,int minutesToSub) Creates a DateTime object that is the result of subtracting the specified number of minutes from the specified DateTime.\nExample: #{minusMinutes(myDateTime,1)}\nResult: \"2011-05-24T17:09:00z\" DateTime minusMonths(DateTime myDateTime,int monthsToSub) Creates a DateTime object that is the result of subtracting the specified number of months from the specified DateTime.\nExample: #{minusMonths(myDateTime,1)}\nResult: \"2011-04-24T17:10:00z\" DateTime minusWeeks(DateTime myDateTime,int weeksToSub) Creates a DateTime object that is the result of subtracting the specified number of weeks from the specified DateTime.\nExample: #{minusWeeks(myDateTime,1)}\nResult: \"2011-05-17T17:10:00z\" DateTime minusYears(DateTime myDateTime,int yearsToSub) Creates a DateTime object that is the result of subtracting the specified number of years from the specified DateTime.\nExample: #{minusYears(myDateTime,1)}\nResult: \"2010-05-24T17:10:00z\" int minute(DateTime myDateTime) Gets the minute of the DateTime value as an integer.\nExample: #{minute(myDateTime)}\nResult: 10 int month(DateTime myDateTime) Gets the month of the DateTime value as an integer.\nExample: #{month(myDateTime)}\nResult: 5 DateTime plusDays(DateTime myDateTime,int daysToAdd) Creates a DateTime object that is the result of adding the specified number of days to the specified DateTime.\nExample: #{plusDays(myDateTime,1)}\nResult: \"2011-05-25T17:10:00z\" DateTime plusHours(DateTime myDateTime,int hoursToAdd) Creates a DateTime object that is the result of adding the specified number of hours to the specified DateTime.\nExample: #{plusHours(myDateTime,1)}\nResult: \"2011-05-24T18:10:00z\" DateTime plusMinutes(DateTime myDateTime,int minutesToAdd) Creates a DateTime object that is the result of adding the specified number of minutes to the specified DateTime.\nExample: #{plusMinutes(myDateTime,1)}\nResult: \"2011-05-24 17:11:00z\" DateTime plusMonths(DateTime myDateTime,int monthsToAdd) Creates a DateTime object that is the result of adding the specified number of months to the specified DateTime.\nExample: #{plusMonths(myDateTime,1)}\nResult: \"2011-06-24T17:10:00z\" DateTime plusWeeks(DateTime myDateTime,int weeksToAdd) Creates a DateTime object that is the result of adding the specified number of weeks to the specified DateTime.\nExample: #{plusWeeks(myDateTime,1)}\nResult: \"2011-05-31T17:10:00z\" DateTime plusYears(DateTime myDateTime,int yearsToAdd) Creates a DateTime object that is the result of adding the specified number of years to the specified DateTime.\nExample: #{plusYears(myDateTime,1)}\nResult: \"2012-05-24T17:10:00z\" DateTime sunday(DateTime myDateTime) Creates a DateTime object for the previous Sunday, relative to the specified DateTime. If the specified DateTime is a Sunday, the result is the specified DateTime.\nExample: #{sunday(myDateTime)}\nResult: \"2011-05-22 17:10:00 UTC\" int year(DateTime myDateTime) Gets the year of the DateTime value as an integer.\nExample: #{year(myDateTime)}\nResult: 2011 DateTime yesterday(DateTime myDateTime) Creates a DateTime object for the previous day, relative to the specified DateTime. The result is the same as minusDays(1).\nExample: #{yesterday(myDateTime)}\nResult: \"2011-05-23T17:10:00z\"",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-characters",
"text": "Special Characters"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html#dp-pipeline-characters",
"main_header": "Special Characters",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-characters",
"text": "Special Characters"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"title": "Expressions"
}
],
"text": "Special Character\nDescription\nExamples @\nRuntime field. This character is a field name prefix for a field that is only available when a pipeline runs.\n@actualStartTime @failureReason @resourceStatus #\nExpression. Expressions are delimited by: \"#{\" and \"}\" and the contents of the braces are evaluated by AWS Data Pipeline. For more information, see Expressions.\n#{format(myDateTime,'YYYY-MM-dd hh:mm:ss')} s3://mybucket/#{id}.csv *\nEncrypted field. This character is a field name prefix to indicate that AWS Data Pipeline should encrypt the contents of this field in transit between the console or CLI and the AWS Data Pipeline service.\n*password",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html#dp-pipeline-characters",
"main_header": "Special Characters",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-characters",
"text": "Special Characters"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"title": "Expressions"
}
],
"text": "Special Character\nDescription\nExamples @\nRuntime field. This character is a field name prefix for a field that is only available when a pipeline runs.\n@actualStartTime @failureReason @resourceStatus #\nExpression. Expressions are delimited by: \"#{\" and \"}\" and the contents of the braces are evaluated by AWS Data Pipeline. For more information, see Expressions.\n#{format(myDateTime,'YYYY-MM-dd hh:mm:ss')} s3://mybucket/#{id}.csv *\nEncrypted field. This character is a field name prefix to indicate that AWS Data Pipeline should encrypt the contents of this field in transit between the console or CLI and the AWS Data Pipeline service.\n*password",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html#dp-pipeline-characters",
"main_header": "Special Characters",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [
{
"url": "https://github.com/awslabs/data-pipeline-samples/tree/master/samples/DynamoDBExportJava",
"title": "Data Pipeline DynamoDB Export Java Sample"
}
],
"text": "NoteFor an example application that uses the AWS Data Pipeline Java SDK, see Data Pipeline DynamoDB Export Java Sample on GitHub.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [
{
"url": "https://github.com/awslabs/data-pipeline-samples/tree/master/samples/DynamoDBExportJava",
"title": "Data Pipeline DynamoDB Export Java Sample"
}
],
"text": "For an example application that uses the AWS Data Pipeline Java SDK, see Data Pipeline DynamoDB Export Java Sample on GitHub.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [
{
"url": "https://github.com/awslabs/data-pipeline-samples/tree/master/samples/DynamoDBExportJava",
"title": "Data Pipeline DynamoDB Export Java Sample"
}
],
"text": "For an example application that uses the AWS Data Pipeline Java SDK, see Data Pipeline DynamoDB Export Java Sample on GitHub.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/object_hierarchy.png",
"alt": "\n                AWS Data Pipeline object hierarchy\n            "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-object-datanodes",
"text": "Data Nodes"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html#dp-object-datanodes",
"main_header": "Data Nodes",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dp-object-dynamodbdatanode",
"main_header": "DynamoDBDataNode",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"links": [],
"text": "NoteThe DynamoDBDataNode object does not support the Exists precondition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dp-object-dynamodbdatanode",
"main_header": "DynamoDBDataNode",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dp-object-dynamodbdatanode",
"main_header": "DynamoDBDataNode",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"links": [],
"text": "The DynamoDBDataNode object does not support the Exists precondition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dp-object-dynamodbdatanode",
"main_header": "DynamoDBDataNode",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"links": [],
"text": "The DynamoDBDataNode object does not support the Exists precondition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dp-object-dynamodbdatanode",
"main_header": "DynamoDBDataNode",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type tableName\nThe DynamoDB table.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type tableName\nThe DynamoDB table.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see Schedule.\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see Schedule.\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If this field is set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dataFormat\nDataFormat for the data described by this data node. Currently supported for HiveActivity and HiveCopyActivity.\nReference Object,  \"dataFormat\":{\"ref\":\"myDynamoDBDataFormatId\"} dependsOn\nSpecify dependency on another runnable object\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} readThroughputPercent\nSets the rate of read operations to keep your DynamoDB provisioned throughput rate in the allocated range for your table. The value is a double between 0.1 and 1.0, inclusively.\nDouble region\nThe code for the region where the DynamoDB table exists. For example, us-east-1. This is used by HiveActivity when it performs staging for DynamoDB tables in Hive.\nEnumeration reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString writeThroughputPercent\nSets the rate of write operations to keep your DynamoDB provisioned throughput rate in the allocated range for your table. The value is a double between .1 and 1.0, inclusively.\nDouble",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If this field is set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dataFormat\nDataFormat for the data described by this data node. Currently supported for HiveActivity and HiveCopyActivity.\nReference Object,  \"dataFormat\":{\"ref\":\"myDynamoDBDataFormatId\"} dependsOn\nSpecify dependency on another runnable object\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} readThroughputPercent\nSets the rate of read operations to keep your DynamoDB provisioned throughput rate in the allocated range for your table. The value is a double between 0.1 and 1.0, inclusively.\nDouble region\nThe code for the region where the DynamoDB table exists. For example, us-east-1. This is used by HiveActivity when it performs staging for DynamoDB tables in Hive.\nEnumeration reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString writeThroughputPercent\nSets the rate of write operations to keep your DynamoDB provisioned throughput rate in the allocated range for your table. The value is a double between .1 and 1.0, inclusively.\nDouble",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#dp-object-mysqldatanode",
"main_header": "MySqlDataNode",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
}
],
"text": "NoteThe MySqlDataNode type is deprecated. We recommend that you use SqlDataNode instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#dp-object-mysqldatanode",
"main_header": "MySqlDataNode",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#dp-object-mysqldatanode",
"main_header": "MySqlDataNode",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
}
],
"text": "The MySqlDataNode type is deprecated. We recommend that you use SqlDataNode instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#dp-object-mysqldatanode",
"main_header": "MySqlDataNode",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
}
],
"text": "The MySqlDataNode type is deprecated. We recommend that you use SqlDataNode instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#dp-object-mysqldatanode",
"main_header": "MySqlDataNode",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type table\nThe name of the table in the MySQL database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type table\nThe name of the table in the MySQL database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod createTableSql\nAn SQL create table expression that creates the table.\nString database\nThe name of the database.\nReference Object, e.g. \"database\":{\"ref\":\"myDatabaseId\"} dependsOn\nSpecifies dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration insertQuery\nAn SQL statement to insert data into the table.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration schemaName\nThe name of the schema holding the table\nString selectQuery\nA SQL statement to fetch data from the table.\nString workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod createTableSql\nAn SQL create table expression that creates the table.\nString database\nThe name of the database.\nReference Object, e.g. \"database\":{\"ref\":\"myDatabaseId\"} dependsOn\nSpecifies dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration insertQuery\nAn SQL statement to insert data into the table.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration schemaName\nThe name of the schema holding the table\nString selectQuery\nA SQL statement to fetch data from the table.\nString workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "S3DataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#dp-object-redshiftdatanode",
"main_header": "RedshiftDataNode",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type database\nThe database on which the table resides.\nReference Object, e.g. \"database\":{\"ref\":\"myRedshiftDatabaseId\"} tableName\nThe name of the Amazon Redshift table. The table is created if it doesn't already exist and you've provided createTableSql.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type database\nThe database on which the table resides.\nReference Object, e.g. \"database\":{\"ref\":\"myRedshiftDatabaseId\"} tableName\nThe name of the Amazon Redshift table. The table is created if it doesn't already exist and you've provided createTableSql.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod createTableSql\nAn SQL expression to create the table in the database. We recommend that you specify the schema where the table should be created, for example: CREATE TABLE mySchema.myTable (bestColumn varchar(25) primary key distkey, numberOfWins integer sortKey). AWS Data Pipeline runs the script in the createTableSql field if the table, specified by tableName, does not exist in the schema, specified by the schemaName field. For example, if you specify schemaName as mySchema but do not include mySchema in the createTableSql field, the table is created in the wrong schema (by default, it would be created in PUBLIC). This occurs because AWS Data Pipeline does not parse your CREATE TABLE statements. \nString dependsOn\nSpecify dependency on another runnable object\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} primaryKeys\nIf you do not specify primaryKeys for a destination table in RedShiftCopyActivity, you can specify a list of columns using primaryKeys which will act as a mergeKey. However, if you have an existing primaryKey defined in an Amazon Redshift table, this setting overrides the existing key.\nString reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration schemaName\nThis optional field specifies the name of the schema for the Amazon Redshift table. If not specified, the schema name is PUBLIC, which is the default schema in Amazon Redshift. For more information, see the Amazon Redshift Database Developer Guide. \nString workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod createTableSql\nAn SQL expression to create the table in the database. We recommend that you specify the schema where the table should be created, for example: CREATE TABLE mySchema.myTable (bestColumn varchar(25) primary key distkey, numberOfWins integer sortKey). AWS Data Pipeline runs the script in the createTableSql field if the table, specified by tableName, does not exist in the schema, specified by the schemaName field. For example, if you specify schemaName as mySchema but do not include mySchema in the createTableSql field, the table is created in the wrong schema (by default, it would be created in PUBLIC). This occurs because AWS Data Pipeline does not parse your CREATE TABLE statements. \nString dependsOn\nSpecify dependency on another runnable object\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} primaryKeys\nIf you do not specify primaryKeys for a destination table in RedShiftCopyActivity, you can specify a list of columns using primaryKeys which will act as a mergeKey. However, if you have an existing primaryKey defined in an Amazon Redshift table, this setting overrides the existing key.\nString reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration schemaName\nThis optional field specifies the name of the schema for the Amazon Redshift table. If not specified, the schema name is PUBLIC, which is the default schema in Amazon Redshift. For more information, see the Amazon Redshift Database Developer Guide. \nString workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#dp-object-s3datanode",
"main_header": "S3DataNode",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"links": [],
"text": "NoteWhen you use an S3DataNode as input to CopyActivity, only the CSV and TSV data formats are supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#dp-object-s3datanode",
"main_header": "S3DataNode",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#dp-object-s3datanode",
"main_header": "S3DataNode",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"links": [],
"text": "When you use an S3DataNode as input to CopyActivity, only the CSV and TSV data formats are supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#dp-object-s3datanode",
"main_header": "S3DataNode",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"links": [],
"text": "When you use an S3DataNode as input to CopyActivity, only the CSV and TSV data formats are supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#dp-object-s3datanode",
"main_header": "S3DataNode",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod compression\nThe type of compression for the data described by the S3DataNode. \"none\" is no compression and \"gzip\" is compressed with the gzip algorithm. This field is only supported for use with Amazon Redshift and when you use S3DataNode with CopyActivity.\nEnumeration dataFormat\nDataFormat for the data described by this S3DataNode.\nReference Object, e.g. \"dataFormat\":{\"ref\":\"myDataFormatId\"} dependsOn\nSpecify dependency on another runnable object\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} directoryPath\nAmazon S3 directory path as a URI: s3://my-bucket/my-key-for-directory. You must provide either a filePath or directoryPath value.\nString failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration filePath\nThe path to the object in Amazon S3 as a URI, for example: s3://my-bucket/my-key-for-file. You must provide either a filePath or directoryPath value. These represent a folder and a file name. Use the directoryPath value to accommodate multiple files in a directory.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod manifestFilePath\nThe Amazon S3 path to a manifest file in the format supported by Amazon Redshift. AWS Data Pipeline uses the manifest file to copy the specified Amazon S3 files into the table. This field is valid only when a RedShiftCopyActivity references the S3DataNode.\nString maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} s3EncryptionType\nOverrides the Amazon S3 encryption type. Values are SERVER_SIDE_ENCRYPTION or NONE. Server-side encryption is enabled by default. \nEnumeration scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod compression\nThe type of compression for the data described by the S3DataNode. \"none\" is no compression and \"gzip\" is compressed with the gzip algorithm. This field is only supported for use with Amazon Redshift and when you use S3DataNode with CopyActivity.\nEnumeration dataFormat\nDataFormat for the data described by this S3DataNode.\nReference Object, e.g. \"dataFormat\":{\"ref\":\"myDataFormatId\"} dependsOn\nSpecify dependency on another runnable object\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} directoryPath\nAmazon S3 directory path as a URI: s3://my-bucket/my-key-for-directory. You must provide either a filePath or directoryPath value.\nString failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration filePath\nThe path to the object in Amazon S3 as a URI, for example: s3://my-bucket/my-key-for-file. You must provide either a filePath or directoryPath value. These represent a folder and a file name. Use the directoryPath value to accommodate multiple files in a directory.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod manifestFilePath\nThe Amazon S3 path to a manifest file in the format supported by Amazon Redshift. AWS Data Pipeline uses the manifest file to copy the specified Amazon S3 files into the table. This field is valid only when a RedShiftCopyActivity references the S3DataNode.\nString maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} s3EncryptionType\nOverrides the Amazon S3 encryption type. Values are SERVER_SIDE_ENCRYPTION or NONE. Server-side encryption is enabled by default. \nEnumeration scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"title": "MySqlDataNode"
}
],
"text": "MySqlDataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#dp-object-sqldatanode",
"main_header": "SqlDataNode",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type table\nThe name of the table in the SQL database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type table\nThe name of the table in the SQL database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod createTableSql\nAn SQL create table expression that creates the table.\nString database\nThe name of the database.\nReference Object, e.g. \"database\":{\"ref\":\"myDatabaseId\"} dependsOn\nSpecifies the dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration insertQuery\nAn SQL statement to insert data into the table.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration schemaName\nThe name of the schema holding the table\nString selectQuery\nA SQL statement to fetch data from the table.\nString workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod createTableSql\nAn SQL create table expression that creates the table.\nString database\nThe name of the database.\nReference Object, e.g. \"database\":{\"ref\":\"myDatabaseId\"} dependsOn\nSpecifies the dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration insertQuery\nAn SQL statement to insert data into the table.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration schemaName\nThe name of the schema holding the table\nString selectQuery\nA SQL statement to fetch data from the table.\nString workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-see-also",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "S3DataNode",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-see-also",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-activities",
"text": "Activities"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html#dp-object-activities",
"main_header": "Activities",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "The input and output are S3DataNodes\n\nThe dataFormat field is the same for input and output",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "The separator must be the \",\" (comma) character.\n\nThe records are not quoted.\n\nThe default escape character is ASCII value 92 (backslash).\n\nThe end of record identifier is ASCII value 10 (or \"\\n\").",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration input\nThe input data source.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data source.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration input\nThe input data source.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data source.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
],
"text": "ShellCommandActivity\n\nEmrActivity\n\nExport MySQL Data to Amazon S3 Using AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#dp-object-emractivity",
"main_header": "EmrActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "NoteIn this example, you can replace the step field with your desired cluster string, which could be a Pig script, Hadoop streaming cluster, your own custom JAR including its parameters, or so on.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "In this example, you can replace the step field with your desired cluster string, which could be a Pig script, Hadoop streaming cluster, your own custom JAR including its parameters, or so on.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "In this example, you can replace the step field with your desired cluster string, which could be a Pig script, Hadoop streaming cluster, your own custom JAR including its parameters, or so on.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "NoteTo pass arguments to an application in a step, you need to specify the Region in the path of the script, as in the following example. In addition, you may need to escape the arguments that you pass. For example, if you use script-runner.jar to run a shell script and want to pass arguments to the script, you must escape the commas that separate them. The following step slot illustrates how to do this: \"step\" : \"s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar,s3://datapipeline/echo.sh,a\\\\\\\\,b\\\\\\\\,c\"This step uses script-runner.jar to run the echo.sh shell script and passes a, b, and c as a single argument to the script. The first escape character is removed from the resultant argument so you may need to escape again. For example, if you had File\\.gz as an argument in JSON, you could escape it using File\\\\\\\\.gz. However, because the first escape is discarded, you must use File\\\\\\\\\\\\\\\\.gz .",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "To pass arguments to an application in a step, you need to specify the Region in the path of the script, as in the following example. In addition, you may need to escape the arguments that you pass. For example, if you use script-runner.jar to run a shell script and want to pass arguments to the script, you must escape the commas that separate them. The following step slot illustrates how to do this: \"step\" : \"s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar,s3://datapipeline/echo.sh,a\\\\\\\\,b\\\\\\\\,c\"This step uses script-runner.jar to run the echo.sh shell script and passes a, b, and c as a single argument to the script. The first escape character is removed from the resultant argument so you may need to escape again. For example, if you had File\\.gz as an argument in JSON, you could escape it using File\\\\\\\\.gz. However, because the first escape is discarded, you must use File\\\\\\\\\\\\\\\\.gz .",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "To pass arguments to an application in a step, you need to specify the Region in the path of the script, as in the following example. In addition, you may need to escape the arguments that you pass. For example, if you use script-runner.jar to run a shell script and want to pass arguments to the script, you must escape the commas that separate them. The following step slot illustrates how to do this:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "This step uses script-runner.jar to run the echo.sh shell script and passes a, b, and c as a single argument to the script. The first escape character is removed from the resultant argument so you may need to escape again. For example, if you had File\\.gz as an argument in JSON, you could escape it using File\\\\\\\\.gz. However, because the first escape is discarded, you must use File\\\\\\\\\\\\\\\\.gz .",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Specify a schedule reference to another object to set the dependency execution order for this object. You can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), you can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Specify a schedule reference to another object to set the dependency execution order for this object. You can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), you can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe Amazon EMR cluster on which this job will run.\nReference Object, for example, \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe Amazon EMR cluster on which this job will run.\nReference Object, for example, \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, for example, \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration input\nThe location of the input data.\nReference Object, for example, \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number of attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, for example, \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, for example, \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when the current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe location of the output data.\nReference Object, for example, \"output\":{\"ref\":\"myDataNodeId\"} parent\nThe parent of the current object from which slots will be inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI, such as 's3://BucketName/Prefix/' for uploading logs for the pipeline.\nString postStepCommand\nShell scripts to be run after all steps are finished. To specify multiple scripts, up to 255, add multiple postStepCommand fields.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, for example, \"precondition\":{\"ref\":\"myPreconditionId\"} preStepCommand\nShell scripts to be run before any steps are run. To specify multiple scripts, up to 255, add multiple preStepCommand fields.\nString reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\n\nResize the cluster before performing this activity to accommodate DynamoDB tables specified as inputs or outputs. \nNoteIf your EmrActivity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\n\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm.\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of the interval, or end of the interval. Values are: cron, ondemand, and timeseries. The timeseries scheduling means that instances are scheduled at the end of each interval. The cron scheduling means that instances are scheduled at the beginning of each interval. An ondemand schedule allows you to run a pipeline one time per activation. You do not have to clone or re-create the pipeline to run it again. If you use an ondemand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run. \nEnumeration step\nOne or more steps for the cluster to run. To specify multiple steps, up to 255, add multiple step fields. Use comma-separated arguments after the JAR name; for example, \"s3://example-bucket/MyWork.jar,arg1,arg2,arg3\".\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, for example, \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration input\nThe location of the input data.\nReference Object, for example, \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number of attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, for example, \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, for example, \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when the current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe location of the output data.\nReference Object, for example, \"output\":{\"ref\":\"myDataNodeId\"} parent\nThe parent of the current object from which slots will be inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI, such as 's3://BucketName/Prefix/' for uploading logs for the pipeline.\nString postStepCommand\nShell scripts to be run after all steps are finished. To specify multiple scripts, up to 255, add multiple postStepCommand fields.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, for example, \"precondition\":{\"ref\":\"myPreconditionId\"} preStepCommand\nShell scripts to be run before any steps are run. To specify multiple scripts, up to 255, add multiple preStepCommand fields.\nString reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\n\nResize the cluster before performing this activity to accommodate DynamoDB tables specified as inputs or outputs. \nNoteIf your EmrActivity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\n\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm.\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of the interval, or end of the interval. Values are: cron, ondemand, and timeseries. The timeseries scheduling means that instances are scheduled at the end of each interval. The cron scheduling means that instances are scheduled at the beginning of each interval. An ondemand schedule allows you to run a pipeline one time per activation. You do not have to clone or re-create the pipeline to run it again. If you use an ondemand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run. \nEnumeration step\nOne or more steps for the cluster to run. To specify multiple steps, up to 255, add multiple step fields. Use comma-separated arguments after the JAR name; for example, \"s3://example-bucket/MyWork.jar,arg1,arg2,arg3\".\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for the object.\nDateTime @scheduledStartTime\nSchedule start time for the object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version that the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for the object.\nDateTime @scheduledStartTime\nSchedule start time for the object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version that the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
],
"text": "ShellCommandActivity\n\nCopyActivity\n\nEmrCluster",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#dp-object-hadoopactivity",
"main_header": "HadoopActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"title": "Executing Work on Existing Resources Using Task Runner"
}
],
"text": "Run a MapReduce program only on myWorkerGroup resources. For more information about worker groups, see Executing Work on Existing Resources Using Task Runner.\n\nRun a preActivityTaskConfig and postActivityTaskConfig",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type jarUri\nLocation of a JAR in Amazon S3 or the local file system of the cluster to run with HadoopActivity.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type jarUri\nLocation of a JAR in Amazon S3 or the local file system of the cluster to run with HadoopActivity.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nEMR Cluster on which this job will run.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nEMR Cluster on which this job will run.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type argument\nArguments to pass to the JAR.\nString attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration hadoopQueue\nThe Hadoop scheduler queue name on which the activity will be submitted.\nString input\nLocation of the input data.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod mainClass\nThe main class of the JAR you are executing with HadoopActivity.\nString maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nLocation of the output data.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString postActivityTaskConfig\nPost-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nPre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type argument\nArguments to pass to the JAR.\nString attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration hadoopQueue\nThe Hadoop scheduler queue name on which the activity will be submitted.\nString input\nLocation of the input data.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod mainClass\nThe main class of the JAR you are executing with HadoopActivity.\nString maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nLocation of the output data.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString postActivityTaskConfig\nPost-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nPre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
],
"text": "ShellCommandActivity\n\nCopyActivity\n\nEmrCluster",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [
{
"url": "https://cwiki.apache.org/confluence/display/Hive/CSV+Serde",
"title": "CSV Serde"
}
],
"text": "NoteThis activity uses the Hive CSV Serde.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [
{
"url": "https://cwiki.apache.org/confluence/display/Hive/CSV+Serde",
"title": "CSV Serde"
}
],
"text": "This activity uses the Hive CSV Serde.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [
{
"url": "https://cwiki.apache.org/confluence/display/Hive/CSV+Serde",
"title": "CSV Serde"
}
],
"text": "This activity uses the Hive CSV Serde.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Specify a schedule reference to another object to set the dependency execution order for this object. You can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), you can create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html.\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Specify a schedule reference to another object to set the dependency execution order for this object. You can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), you can create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html.\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type hiveScript\nThe Hive script to run.\nString scriptUri\nThe location of the Hive script to run (for example, s3://scriptLocation).\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type hiveScript\nThe Hive script to run.\nString scriptUri\nThe location of the Hive script to run (for example, s3://scriptLocation).\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group\nDescription\nSlot Type runsOn\nThe EMR cluster on which this HiveActivity runs.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString input\nThe input data source.\nReference Object, such as \"input\":{\"ref\":\"myDataNodeId\"} output\nThe output data source.\nReference Object, such as \"output\":{\"ref\":\"myDataNodeId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group\nDescription\nSlot Type runsOn\nThe EMR cluster on which this HiveActivity runs.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString input\nThe input data source.\nReference Object, such as \"input\":{\"ref\":\"myDataNodeId\"} output\nThe output data source.\nReference Object, such as \"output\":{\"ref\":\"myDataNodeId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, such as \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration hadoopQueue\nThe Hadoop scheduler queue name on which the job will be submitted.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number of attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, such as \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, such as \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, such as \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, such as \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString postActivityTaskConfig\nPost-activity configuration script to be run. This consists of a URI of the shell script in\nAmazon S3 and a list of arguments.\nReference Object, such as \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nPre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, such as \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, such as  \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\nResize the cluster before performing this activity to accommodate DynamoDB data nodes specified as inputs or outputs. NoteIf your activity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm.\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration scriptVariable\nSpecifies script variables for Amazon EMR to pass to Hive while running a script. For example, the following example script variables would pass a SAMPLE and FILTER_DATE variable to Hive: SAMPLE=s3://elasticmapreduce/samples/hive-ads and FILTER_DATE=#{format(@scheduledStartTime,'YYYY-MM-dd')}%. This field accepts multiple values and works with both script and scriptUri fields. In addition, scriptVariable functions regardless of whether stage is set to true or false. This field is especially useful to send dynamic values to Hive using\nAWS Data Pipeline expressions and functions.\nString stage\nDetermines whether staging is enabled before or after running the script. Not permitted with Hive 11, so use an Amazon EMR AMI version 3.2.0 or greater.\nBoolean",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, such as \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration hadoopQueue\nThe Hadoop scheduler queue name on which the job will be submitted.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number of attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, such as \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, such as \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, such as \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, such as \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString postActivityTaskConfig\nPost-activity configuration script to be run. This consists of a URI of the shell script in\nAmazon S3 and a list of arguments.\nReference Object, such as \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nPre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, such as \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, such as  \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\nResize the cluster before performing this activity to accommodate DynamoDB data nodes specified as inputs or outputs. NoteIf your activity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm.\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration scriptVariable\nSpecifies script variables for Amazon EMR to pass to Hive while running a script. For example, the following example script variables would pass a SAMPLE and FILTER_DATE variable to Hive: SAMPLE=s3://elasticmapreduce/samples/hive-ads and FILTER_DATE=#{format(@scheduledStartTime,'YYYY-MM-dd')}%. This field accepts multiple values and works with both script and scriptUri fields. In addition, scriptVariable functions regardless of whether stage is set to true or false. This field is especially useful to send dynamic values to Hive using\nAWS Data Pipeline expressions and functions.\nString stage\nDetermines whether staging is enabled before or after running the script. Not permitted with Hive 11, so use an Amazon EMR AMI version 3.2.0 or greater.\nBoolean",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, such as  \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, such as \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for an object.\nDateTime @scheduledStartTime\nSchedule start time for an object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, such as  \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, such as  \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, such as \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for an object.\nDateTime @scheduledStartTime\nSchedule start time for an object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, such as  \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "ShellCommandActivity\n\nEmrActivity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#dp-object-hivecopyactivity",
"main_header": "HiveCopyActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object. Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying  \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nSpecify cluster to run on.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nSpecify cluster to run on.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nThe timeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecifies the dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration filterSql\nA Hive SQL statement fragment that filters a subset of DynamoDB or Amazon S3 data to copy. The filter should only contain predicates and not begin with a WHERE clause, because AWS Data Pipeline adds it automatically.\nString input\nThe input data source. This must be a S3DataNode or DynamoDBDataNode. If you use DynamoDBNode, specify a DynamoDBExportDataFormat.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data source. If input is S3DataNode, this must be DynamoDBDataNode. Otherwise, this can be S3DataNode or DynamoDBDataNode. If you use DynamoDBNode, specify a DynamoDBExportDataFormat.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nThe parent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI, such as  's3://BucketName/Key/', for uploading logs for the pipeline.\nString postActivityTaskConfig\nThe post-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nThe pre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally defines a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\nResize the cluster before performing this activity to accommodate DynamoDB data nodes  specified as inputs or outputs. NoteIf your activity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nThe timeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecifies the dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration filterSql\nA Hive SQL statement fragment that filters a subset of DynamoDB or Amazon S3 data to copy. The filter should only contain predicates and not begin with a WHERE clause, because AWS Data Pipeline adds it automatically.\nString input\nThe input data source. This must be a S3DataNode or DynamoDBDataNode. If you use DynamoDBNode, specify a DynamoDBExportDataFormat.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data source. If input is S3DataNode, this must be DynamoDBDataNode. Otherwise, this can be S3DataNode or DynamoDBDataNode. If you use DynamoDBNode, specify a DynamoDBExportDataFormat.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nThe parent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI, such as  's3://BucketName/Key/', for uploading logs for the pipeline.\nString postActivityTaskConfig\nThe post-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nThe pre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, e.g. \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally defines a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\nResize the cluster before performing this activity to accommodate DynamoDB data nodes  specified as inputs or outputs. NoteIf your activity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nThe most recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nThe most recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "ShellCommandActivity\n\nEmrActivity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#dp-object-pigactivity",
"main_header": "PigActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-example",
"text": "Example"
},
"links": [],
"text": "MyPigActivity1 loads data from Amazon S3 and runs a Pig script that selects a few columns of data and uploads it to Amazon S3.\n\nMyPigActivity2 loads the first output, selects a few columns and three rows of data, and uploads it to Amazon S3 as a second output.\n\nMyPigActivity3 loads the second output data, inserts two rows of data and only the column named \"fifth\" to Amazon RDS.\n\nMyPigActivity4 loads Amazon RDS data, selects the first row of data, and uploads it to Amazon S3.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-example",
"main_header": "Example",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object.  Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Users must specify a schedule reference to another object to set the dependency execution order for this object.  Users can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), users can create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type script\nThe Pig script to run.\nString scriptUri\nThe location of the Pig script to run (for example, s3://scriptLocation).\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type script\nThe Pig script to run.\nString scriptUri\nThe location of the Pig script to run (for example, s3://scriptLocation).\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nEMR Cluster on which this PigActivity runs.\nReference Object, for example, \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nEMR Cluster on which this PigActivity runs.\nReference Object, for example, \"runsOn\":{\"ref\":\"myEmrClusterId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nThe timeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecifies the dependency on another runnable object.\nReference Object, for example, \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration input\nThe input data source.\nReference Object, for example, \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, for example, \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, for example, \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data source.\nReference Object, for example, \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString postActivityTaskConfig\nPost-activity configuration script to be run. This consists of a URI of the shell script in Amazon S33 and a list of arguments.\nReference Object, for example, \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nPre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, for example, \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, for example, \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\nResize the cluster before performing this activity to accommodate DynamoDB  data nodes specified as inputs or outputs.NoteIf your activity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm.\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration scriptVariable\nThe arguments to pass to the Pig script. You can use scriptVariable with script or scriptUri.\nString stage\nDetermines whether staging is enabled and allows your Pig script to have access to the staged-data tables, such as ${INPUT1} and ${OUTPUT1}.\nBoolean",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nThe timeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecifies the dependency on another runnable object.\nReference Object, for example, \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration input\nThe input data source.\nReference Object, for example, \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, for example, \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, for example, \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data source.\nReference Object, for example, \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString postActivityTaskConfig\nPost-activity configuration script to be run. This consists of a URI of the shell script in Amazon S33 and a list of arguments.\nReference Object, for example, \"postActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} preActivityTaskConfig\nPre-activity configuration script to be run. This consists of a URI of the shell script in Amazon S3 and a list of arguments.\nReference Object, for example, \"preActivityTaskConfig\":{\"ref\":\"myShellScriptConfigId\"} precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, for example, \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resizeClusterBeforeRunning\nResize the cluster before performing this activity to accommodate DynamoDB  data nodes specified as inputs or outputs.NoteIf your activity uses a DynamoDBDataNode as either an input or output data node, and if you set the resizeClusterBeforeRunning to TRUE, AWS Data Pipeline starts using m3.xlarge instance types. This overwrites your instance type choices with m3.xlarge, which could increase your monthly costs.\nBoolean resizeClusterMaxInstances\nA limit on the maximum number of instances that can be requested by the resize algorithm.\nInteger retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Time Series Style Scheduling means instances are scheduled at the end of each interval and Cron Style Scheduling means instances are scheduled at the beginning of each interval. An on-demand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, you simply call the ActivatePipeline operation for each subsequent run. Values are: cron, ondemand, and timeseries.\nEnumeration scriptVariable\nThe arguments to pass to the Pig script. You can use scriptVariable with script or scriptUri.\nString stage\nDetermines whether staging is enabled and allows your Pig script to have access to the staged-data tables, such as ${INPUT1} and ${OUTPUT1}.\nBoolean",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, for example, \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for the object.\nDateTime @scheduledStartTime\nSchedule start time for the object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version that the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, for example, \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for the object.\nDateTime @scheduledStartTime\nSchedule start time for the object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version that the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "ShellCommandActivity\n\nEmrActivity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#dp-object-redshiftcopyactivity",
"main_header": "RedshiftCopyActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"title": "SqlActivity"
}
],
"text": "Start by using AWS Data Pipeline to stage your data in Amazon S3. Use RedshiftCopyActivity to move the data from Amazon RDS and Amazon EMR to Amazon Redshift.\nThis lets you load your data into Amazon Redshift where you can analyze it.\n\nUse SqlActivity to perform SQL queries on the data that you've loaded into Amazon Redshift.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#dp-object-redshiftcopyactivity",
"main_header": "RedshiftCopyActivity",
"images": [],
"container_type": "div",
"children_tags": [
"ol"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "NoteIf an APPEND operation is interrupted and retried, the resulting rerun pipeline potentially appends from the beginning. This may cause further duplication, so you should be aware of this behavior, especially if you have any logic that counts the number of rows.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "If an APPEND operation is interrupted and retried, the resulting rerun pipeline potentially appends from the beginning. This may cause further duplication, so you should be aware of this behavior, especially if you have any logic that counts the number of rows.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "If an APPEND operation is interrupted and retried, the resulting rerun pipeline potentially appends from the beginning. This may cause further duplication, so you should be aware of this behavior, especially if you have any logic that counts the number of rows.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/t_updating-inserting-using-staging-tables-.html",
"title": "Updating and Inserting New Data"
}
],
"text": "Required Fields\nDescription\nSlot Type insertMode Determines what AWS Data Pipeline does with pre-existing data in the target table that overlaps with rows in the data to be loaded.\nValid values are: KEEP_EXISTING, OVERWRITE_EXISTING, TRUNCATE and APPEND.\nKEEP_EXISTING adds new rows to the table, while leaving any existing rows unmodified.\nKEEP_EXISTING and OVERWRITE_EXISTING use the primary key, sort, and distribution keys to identify which incoming rows to match with existing rows. See Updating and Inserting New Data in the Amazon Redshift Database Developer Guide. \nTRUNCATE deletes all the data in the destination table before writing the new data.\n\nAPPEND adds all records to the end of the Redshift table. APPEND does not require a primary, distribution key, or sort key so items that may be potential duplicates may be appended.\n\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/t_updating-inserting-using-staging-tables-.html",
"title": "Updating and Inserting New Data"
}
],
"text": "Required Fields\nDescription\nSlot Type insertMode Determines what AWS Data Pipeline does with pre-existing data in the target table that overlaps with rows in the data to be loaded.\nValid values are: KEEP_EXISTING, OVERWRITE_EXISTING, TRUNCATE and APPEND.\nKEEP_EXISTING adds new rows to the table, while leaving any existing rows unmodified.\nKEEP_EXISTING and OVERWRITE_EXISTING use the primary key, sort, and distribution keys to identify which incoming rows to match with existing rows. See Updating and Inserting New Data in the Amazon Redshift Database Developer Guide. \nTRUNCATE deletes all the data in the destination table before writing the new data.\n\nAPPEND adds all records to the end of the Redshift table. APPEND does not require a primary, distribution key, or sort key so items that may be potential duplicates may be appended.\n\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval. \nSpecify a schedule reference to another object to set the dependency execution order for this object. \nIn most cases, we recommend to put the schedule reference on the default pipeline object so that all objects inherit that schedule. For example, you can explicitly set a schedule on the object by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. \nIf the master schedule in your pipeline contains  nested schedules, create a parent object that has a schedule reference. \nFor more information about example optional schedule configurations, see Schedule. Reference Object, such as: \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval. \nSpecify a schedule reference to another object to set the dependency execution order for this object. \nIn most cases, we recommend to put the schedule reference on the default pipeline object so that all objects inherit that schedule. For example, you can explicitly set a schedule on the object by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. \nIf the master schedule in your pipeline contains  nested schedules, create a parent object that has a schedule reference. \nFor more information about example optional schedule configurations, see Schedule. Reference Object, such as: \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html",
"title": "COPY"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html",
"title": "Data Conversion Parameters"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html",
"title": "UNLOAD"
},
{
"url": "https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/cm-c-executing-queries.html",
"title": "Assigning Queries to Queues"
}
],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod commandOptions\n\nTakes parameters to pass to the Amazon Redshift data node during the COPY operation. For information on parameters, see COPY in the Amazon Redshift Database Developer Guide.\nAs it loads the table, COPY attempts to implicitly convert the strings to the data type of the target column. In addition to the default data conversions that happen automatically, if you receive errors or have other conversion needs, you can specify additional conversion parameters. For information, see Data Conversion Parameters in the Amazon Redshift Database Developer Guide.\nIf a data format is associated with the input or output data node, then the provided parameters are ignored. \nBecause the copy operation first uses COPY to insert data into a staging table, and then uses an INSERT command to copy the data from the staging table into the destination table, some COPY parameters do not apply, such as the COPY command's ability to enable automatic compression of the table. If compression is required, add column encoding details to the CREATE TABLE statement. \nAlso, in some cases when it needs to unload data from the Amazon Redshift cluster and create files in Amazon S3, the RedshiftCopyActivity relies on the UNLOAD operation from Amazon Redshift.\nTo improve performance during copying and unloading, specify PARALLEL OFF parameter from the UNLOAD command. For information on parameters, see UNLOAD in the Amazon Redshift Database Developer Guide.\n\nString dependsOn\nSpecify dependency on another runnable object.\nReference Object: \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration input\nThe input data node. The data source can be Amazon S3, DynamoDB, or Amazon Redshift.\nReference Object: \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object: \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object:  \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object: \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data node. The output location can be Amazon S3 or Amazon Redshift.\nReference Object: \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object: \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object: \"precondition\":{\"ref\":\"myPreconditionId\"} queue\n\nCorresponds to the query_group setting in Amazon Redshift, which allows you to assign and prioritize concurrent activities based on their placement in queues. \nAmazon Redshift limits the number of simultaneous connections to 15. For more information, see Assigning Queries to Queues in the Amazon RDS Database Developer Guide.\n\nString reportProgressTimeout\n\nTimeout for remote work successive calls to reportProgress. \nIf set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\n\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\n\nAllows you to specify whether the schedule for objects in your pipeline. Values are: cron, ondemand, and timeseries.\nThe timeseries scheduling means instances are scheduled at the end of each interval.\nThe Cron scheduling means instances are scheduled at the beginning of each interval. \nAn ondemand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again.\n To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run. \nIf you use an ondemand schedule, you must specify it in the default object, and it must be the only scheduleType specified for objects in the pipeline.\n\nEnumeration transformSql\n\nThe SQL SELECT expression used to transform the input data. \nRun the transformSql expression on the table named staging. \nWhen you copy data from DynamoDB or Amazon S3, AWS Data Pipeline creates a table called \"staging\" and initially loads data in there. Data from this table is used to update the target table. \nThe output schema of transformSql must match the final target table's schema.\nIf you specify the transformSql option, a second staging table is created from the specified SQL statement. The data from this second staging table is then updated in the final target table.\n\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html",
"title": "COPY"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html",
"title": "Data Conversion Parameters"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html",
"title": "UNLOAD"
},
{
"url": "https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/cm-c-executing-queries.html",
"title": "Assigning Queries to Queues"
}
],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod commandOptions\n\nTakes parameters to pass to the Amazon Redshift data node during the COPY operation. For information on parameters, see COPY in the Amazon Redshift Database Developer Guide.\nAs it loads the table, COPY attempts to implicitly convert the strings to the data type of the target column. In addition to the default data conversions that happen automatically, if you receive errors or have other conversion needs, you can specify additional conversion parameters. For information, see Data Conversion Parameters in the Amazon Redshift Database Developer Guide.\nIf a data format is associated with the input or output data node, then the provided parameters are ignored. \nBecause the copy operation first uses COPY to insert data into a staging table, and then uses an INSERT command to copy the data from the staging table into the destination table, some COPY parameters do not apply, such as the COPY command's ability to enable automatic compression of the table. If compression is required, add column encoding details to the CREATE TABLE statement. \nAlso, in some cases when it needs to unload data from the Amazon Redshift cluster and create files in Amazon S3, the RedshiftCopyActivity relies on the UNLOAD operation from Amazon Redshift.\nTo improve performance during copying and unloading, specify PARALLEL OFF parameter from the UNLOAD command. For information on parameters, see UNLOAD in the Amazon Redshift Database Developer Guide.\n\nString dependsOn\nSpecify dependency on another runnable object.\nReference Object: \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration input\nThe input data node. The data source can be Amazon S3, DynamoDB, or Amazon Redshift.\nReference Object: \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object: \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object:  \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object: \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe output data node. The output location can be Amazon S3 or Amazon Redshift.\nReference Object: \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object: \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object: \"precondition\":{\"ref\":\"myPreconditionId\"} queue\n\nCorresponds to the query_group setting in Amazon Redshift, which allows you to assign and prioritize concurrent activities based on their placement in queues. \nAmazon Redshift limits the number of simultaneous connections to 15. For more information, see Assigning Queries to Queues in the Amazon RDS Database Developer Guide.\n\nString reportProgressTimeout\n\nTimeout for remote work successive calls to reportProgress. \nIf set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\n\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\n\nAllows you to specify whether the schedule for objects in your pipeline. Values are: cron, ondemand, and timeseries.\nThe timeseries scheduling means instances are scheduled at the end of each interval.\nThe Cron scheduling means instances are scheduled at the beginning of each interval. \nAn ondemand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again.\n To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run. \nIf you use an ondemand schedule, you must specify it in the default object, and it must be the only scheduleType specified for objects in the pipeline.\n\nEnumeration transformSql\n\nThe SQL SELECT expression used to transform the input data. \nRun the transformSql expression on the table named staging. \nWhen you copy data from DynamoDB or Amazon S3, AWS Data Pipeline creates a table called \"staging\" and initially loads data in there. Data from this table is used to update the target table. \nThe output schema of transformSql must match the final target table's schema.\nIf you specify the transformSql option, a second staging table is created from the specified SQL statement. The data from this second staging table is then updated in the final target table.\n\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object: \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object: \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object: \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object: \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object: \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object: \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object. Denotes its place in the life cycle. For example, Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object. Denotes its place in the life cycle. For example, Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval.\nTo set the dependency execution order for this object, specify a schedule reference to another object. \nTo satisfy this requirement, explicitly set a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. \nIn most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. If the pipeline consists of a tree of schedules (schedules within the master schedule), create a parent object that has a schedule reference. \nTo spread the load, AWS Data Pipeline creates physical objects slightly ahead of schedule, but runs them on schedule. \nFor more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\n\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval.\nTo set the dependency execution order for this object, specify a schedule reference to another object. \nTo satisfy this requirement, explicitly set a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. \nIn most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. If the pipeline consists of a tree of schedules (schedules within the master schedule), create a parent object that has a schedule reference. \nTo spread the load, AWS Data Pipeline creates physical objects slightly ahead of schedule, but runs them on schedule. \nFor more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\n\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type command\nThe command to run. Use $ to reference positional parameters and scriptArgument to specify the parameters for the command. This value and any associated parameters must function in the environment from which you are running the Task Runner.\nString scriptUri\nAn Amazon S3 URI path for a file to download and run as a shell command. Specify only one scriptUri, or command field. scriptUri cannot use parameters, use command instead.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type command\nThe command to run. Use $ to reference positional parameters and scriptArgument to specify the parameters for the command. This value and any associated parameters must function in the environment from which you are running the Task Runner.\nString scriptUri\nAn Amazon S3 URI path for a file to download and run as a shell command. Specify only one scriptUri, or command field. scriptUri cannot use parameters, use command instead.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command, for example, an Amazon EC2 instance or an Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nUsed for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command, for example, an Amazon EC2 instance or an Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nUsed for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nThe timeout for the remote work completion. If set, then a remote activity that does not complete within the specified starting time may be retried.\nPeriod dependsOn\nSpecifies a dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration input\nThe location of the input data.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or is not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe location of the output data.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nThe parent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI, such as 's3://BucketName/Key/' for uploading logs for the pipeline.\nString precondition\nOptionally defines a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nThe timeout for successive calls to reportProgress by remote activities. If set, then remote activities that do not report progress for the specified period may be considered stalled and are retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\n\nAllows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of the interval or at the end of the interval. \nThe values are: cron, ondemand, and timeseries.\nIf set to timeseries, instances are scheduled at the end of each interval. \nIf set to Cron, instances are scheduled at the beginning of each interval. \nIf set to ondemand, you can run a pipeline one time, per activation. This means you do not have to clone or recreate the pipeline to run it again. If you use an ondemand schedule, specify it in the default object as the  only scheduleType  for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run. Enumeration scriptArgument\nA JSON-formatted array of strings to pass to the command specified by the command. For example, if command is echo $1 $2, specify scriptArgument as \"param1\", \"param2\". For multiple arguments and parameters, pass the scriptArgument as follows: \"scriptArgument\":\"arg1\",\"scriptArgument\":\"param1\",\"scriptArgument\":\"arg2\",\"scriptArgument\":\"param2\". The scriptArgument can only be used with command; Using it with scriptUri causes an error.\nString stage\nDetermines whether staging is enabled and allows your shell commands to have access to the staged-data variables, such as ${INPUT1_STAGING_DIR} and ${OUTPUT1_STAGING_DIR}.\nBoolean stderr\nThe path that receives redirected system error messages from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString stdout\nThe Amazon S3 path that receives redirected output from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nThe timeout for the remote work completion. If set, then a remote activity that does not complete within the specified starting time may be retried.\nPeriod dependsOn\nSpecifies a dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration input\nThe location of the input data.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number attempt retries on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or is not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nThe location of the output data.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nThe parent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI, such as 's3://BucketName/Key/' for uploading logs for the pipeline.\nString precondition\nOptionally defines a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} reportProgressTimeout\nThe timeout for successive calls to reportProgress by remote activities. If set, then remote activities that do not report progress for the specified period may be considered stalled and are retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\n\nAllows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of the interval or at the end of the interval. \nThe values are: cron, ondemand, and timeseries.\nIf set to timeseries, instances are scheduled at the end of each interval. \nIf set to Cron, instances are scheduled at the beginning of each interval. \nIf set to ondemand, you can run a pipeline one time, per activation. This means you do not have to clone or recreate the pipeline to run it again. If you use an ondemand schedule, specify it in the default object as the  only scheduleType  for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run. Enumeration scriptArgument\nA JSON-formatted array of strings to pass to the command specified by the command. For example, if command is echo $1 $2, specify scriptArgument as \"param1\", \"param2\". For multiple arguments and parameters, pass the scriptArgument as follows: \"scriptArgument\":\"arg1\",\"scriptArgument\":\"param1\",\"scriptArgument\":\"arg2\",\"scriptArgument\":\"param2\". The scriptArgument can only be used with command; Using it with scriptUri causes an error.\nString stage\nDetermines whether staging is enabled and allows your shell commands to have access to the staged-data variables, such as ${INPUT1_STAGING_DIR} and ${OUTPUT1_STAGING_DIR}.\nBoolean stderr\nThe path that receives redirected system error messages from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString stdout\nThe Amazon S3 path that receives redirected output from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nThe list of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nThe time when the execution of this object finished.\nDateTime @actualStartTime\nThe time when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nThe description of the dependency chain that caused the object failure.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on Amazon EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which the object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for Amazon EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nThe Id of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nThe time at which the health status was updated last time.\nDateTime hostname\nThe host name of the client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nThe time of the latest run for which the execution completed.\nDateTime @latestRunTime\nThe time of the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nThe time of the run to be scheduled next.\nDateTime reportProgressTime\nThe most recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nThe schedule end time for object.\nDateTime @scheduledStartTime\nThe schedule start time for object.\nDateTime @status\nThe status of the object.\nString @version\nThe AWS Data Pipeline version used to create the object.\nString @waitingOn\nThe description of the list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nThe list of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nThe time when the execution of this object finished.\nDateTime @actualStartTime\nThe time when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nThe description of the dependency chain that caused the object failure.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nAmazon EMR step logs available only on Amazon EMR activity attempts.\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which the object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for Amazon EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nThe Id of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nThe time at which the health status was updated last time.\nDateTime hostname\nThe host name of the client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nThe time of the latest run for which the execution completed.\nDateTime @latestRunTime\nThe time of the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nThe time of the run to be scheduled next.\nDateTime reportProgressTime\nThe most recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nThe schedule end time for object.\nDateTime @scheduledStartTime\nThe schedule start time for object.\nDateTime @status\nThe status of the object.\nString @version\nThe AWS Data Pipeline version used to create the object.\nString @waitingOn\nThe description of the list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nThe error describing the ill-formed object.\nString @pipelineId\nThe Id of the pipeline to which this object belongs.\nString @sphere\nThe place of an object in the lifecycle. Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nThe error describing the ill-formed object.\nString @pipelineId\nThe Id of the pipeline to which this object belongs.\nString @sphere\nThe place of an object in the lifecycle. Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"title": "CopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "CopyActivity\n\nEmrActivity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#dp-object-sqlactivity",
"main_header": "SqlActivity",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type database\nThe database on which to run the supplied SQL script.\nReference Object, e.g. \"database\":{\"ref\":\"myDatabaseId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type database\nThe database on which to run the supplied SQL script.\nReference Object, e.g. \"database\":{\"ref\":\"myDatabaseId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval. You must specify a schedule reference to another object to set the dependency execution order for this object. You can set a schedule explicitly on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. \nIn most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. \nIf the pipeline has a tree of schedules nested within the master schedule, create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\n\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval. You must specify a schedule reference to another object to set the dependency execution order for this object. You can set a schedule explicitly on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. \nIn most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. \nIf the pipeline has a tree of schedules nested within the master schedule, create a parent object that has a schedule reference.  For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\n\nReference Object, e.g. \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type script\nThe SQL script to run. You must specify script or scriptUri. When the script is stored in Amazon S3, then script is not evaluated as an expression. Specifying multiple values for scriptArgument is helpful when the script is stored in Amazon S3.\nString scriptUri\nA URI specifying the location of an SQL script to execute in this activity.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type script\nThe SQL script to run. You must specify script or scriptUri. When the script is stored in Amazon S3, then script is not evaluated as an expression. Specifying multiple values for scriptArgument is helpful when the script is stored in Amazon S3.\nString scriptUri\nA URI specifying the location of an SQL script to execute in this activity.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type runsOn\nThe computational resource to run the activity or command. For example, an Amazon EC2 instance or Amazon EMR cluster.\nReference Object, e.g. \"runsOn\":{\"ref\":\"myResourceId\"} workerGroup\nThe worker group. This is used for routing tasks. If you provide a runsOn value and workerGroup exists, workerGroup is ignored.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/cm-c-executing-queries.html",
"title": "Assigning Queries to Queues"
}
],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration input\nLocation of the input data.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe time period since the scheduled start of the pipeline within which the object run must start.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed in the time period since the scheduled start of the pipeline as specified by 'lateAfterTimeout'.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nLocation of the output data. This is only useful for referencing from within a script (for example #{output.tablename}) and for creating the output table by setting 'createTableSql' in the output data node. The output of the SQL query is not written to the output data node.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} queue\n[Amazon Redshift only] Corresponds to the query_group setting in Amazon Redshift, which allows you to assign and prioritize concurrent activities based on their placement in queues. Amazon Redshift limits the number of simultaneous connections to 15. For more information, see Assigning Queries to Queues in the Amazon Redshift Database Developer Guide.\nString reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\n\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Values are: cron, ondemand, and timeseries.\n\ntimeseries scheduling means instances are scheduled at the end of each interval.\ncron scheduling means instances are scheduled at the beginning of each interval. \nAn ondemand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an ondemand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run.\n\nEnumeration scriptArgument\nA list of variables for the script. You can alternatively put expressions directly into the script field. Multiple values for scriptArgument are helpful when the script is stored in Amazon S3. Example: #{format(@scheduledStartTime, \"YY-MM-DD HH:MM:SS\"}\\n#{format(plusPeriod(@scheduledStartTime, \"1 day\"), \"YY-MM-DD HH:MM:SS\"}\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/cm-c-executing-queries.html",
"title": "Assigning Queries to Queues"
}
],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod dependsOn\nSpecify dependency on another runnable object.\nReference Object, e.g. \"dependsOn\":{\"ref\":\"myActivityId\"} failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration input\nLocation of the input data.\nReference Object, e.g. \"input\":{\"ref\":\"myDataNodeId\"} lateAfterTimeout\nThe time period since the scheduled start of the pipeline within which the object run must start.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed in the time period since the scheduled start of the pipeline as specified by 'lateAfterTimeout'.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} output\nLocation of the output data. This is only useful for referencing from within a script (for example #{output.tablename}) and for creating the output table by setting 'createTableSql' in the output data node. The output of the SQL query is not written to the output data node.\nReference Object, e.g. \"output\":{\"ref\":\"myDataNodeId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString precondition\nOptionally define a precondition. A data node is not marked \"READY\" until all preconditions have been met.\nReference Object, e.g. \"precondition\":{\"ref\":\"myPreconditionId\"} queue\n[Amazon Redshift only] Corresponds to the query_group setting in Amazon Redshift, which allows you to assign and prioritize concurrent activities based on their placement in queues. Amazon Redshift limits the number of simultaneous connections to 15. For more information, see Assigning Queries to Queues in the Amazon Redshift Database Developer Guide.\nString reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scheduleType\n\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of interval or end of the interval. Values are: cron, ondemand, and timeseries.\n\ntimeseries scheduling means instances are scheduled at the end of each interval.\ncron scheduling means instances are scheduled at the beginning of each interval. \nAn ondemand schedule allows you to run a pipeline one time per activation. This means you do not have to clone or re-create the pipeline to run it again. If you use an ondemand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run.\n\nEnumeration scriptArgument\nA list of variables for the script. You can alternatively put expressions directly into the script field. Multiple values for scriptArgument are helpful when the script is stored in Amazon S3. Example: #{format(@scheduledStartTime, \"YY-MM-DD HH:MM:SS\"}\\n#{format(plusPeriod(@scheduledStartTime, \"1 day\"), \"YY-MM-DD HH:MM:SS\"}\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString @healthStatus\nThe health status of the object which reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-resources",
"text": "Resources"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html#dp-object-resources",
"main_header": "Resources",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#dp-object-ec2resource",
"main_header": "Ec2Resource",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "ImportantOnly AWS accounts created before December 4, 2013 support the EC2-Classic platform. If you have one of these accounts, you may have the option to create EC2Resource objects for a pipeline in an EC2-Classic network rather than a VPC. We strongly recommend that you create resources for all your pipelines in VPCs. In addition, if you have existing resources in EC2-Classic, we strongly recommend that you migrate them to a VPC. For more information, see Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "Only AWS accounts created before December 4, 2013 support the EC2-Classic platform. If you have one of these accounts, you may have the option to create EC2Resource objects for a pipeline in an EC2-Classic network rather than a VPC. We strongly recommend that you create resources for all your pipelines in VPCs. In addition, if you have existing resources in EC2-Classic, we strongly recommend that you migrate them to a VPC. For more information, see Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"title": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
}
],
"text": "Only AWS accounts created before December 4, 2013 support the EC2-Classic platform. If you have one of these accounts, you may have the option to create EC2Resource objects for a pipeline in an EC2-Classic network rather than a VPC. We strongly recommend that you create resources for all your pipelines in VPCs. In addition, if you have existing resources in EC2-Classic, we strongly recommend that you migrate them to a VPC. For more information, see Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type resourceRole\nThe IAM role that controls the resources that the Amazon EC2 instance can access.\nString role\nThe IAM role that AWS Data Pipeline uses to create the EC2 instance.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type resourceRole\nThe IAM role that controls the resources that the Amazon EC2 instance can access.\nString role\nThe IAM role that AWS Data Pipeline uses to create the EC2 instance.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval. \nTo set the dependency execution order for this object,specify a schedule reference to another object. You can do this in one of the following ways: To ensure that all objects in the pipeline inherit the schedule, set a schedule on the object explicitly: \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is useful to put the schedule reference on the default pipeline object, so that all objects inherit that schedule.\n\nIf the pipeline has schedules nested within the master schedule, you can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html Reference Object, for example \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\n\nThis object is invoked within the execution of a schedule interval. \nTo set the dependency execution order for this object,specify a schedule reference to another object. You can do this in one of the following ways: To ensure that all objects in the pipeline inherit the schedule, set a schedule on the object explicitly: \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is useful to put the schedule reference on the default pipeline object, so that all objects inherit that schedule.\n\nIf the pipeline has schedules nested within the master schedule, you can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html Reference Object, for example \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html",
"title": "Linux AMI Virtualization Types"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html",
"title": "Finding a Linux AMI"
}
],
"text": "Optional Fields\nDescription\nSlot Type actionOnResourceFailure\nThe action taken after a resource failure for this resource. Valid values are \"retryall\" and \"retrynone\".\nString actionOnTaskFailure\nThe action taken after a task failure for this resource. Valid values are \"continue\" or \"terminate\".\nString associatePublicIpAddress\nIndicates whether to assign a public IP address to the instance. If the instance is in  Amazon EC2 or Amazon VPC, the default value is true. Otherwise, the default value is false.\nBoolean attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for the remote work completion. If set, then a remote activity that does not complete within the specified starting time may be retried.\nPeriod availabilityZone\nThe Availability Zone in which to launch the Amazon EC2 instance.\nString failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration httpProxy\nThe proxy host that clients use to connect to AWS services.\nReference Object, for example, \"httpProxy\":{\"ref\":\"myHttpProxyId\"} imageId\nThe ID of the AMI to use for the instance. By default, AWS Data Pipeline uses the HVM AMI virtualization type. The specific AMI IDs used are based on a Region. You can overwrite the default AMI by specifying the HVM AMI of your choice. For more information about AMI types, see Linux AMI Virtualization Types and Finding a Linux AMI in the Amazon EC2 User Guide for Linux Instances.\n\nString initTimeout\nThe amount of time to wait for the resource to start. \nPeriod instanceCount\nDeprecated.\nInteger instanceType\nThe type of Amazon EC2 instance to start.\nString keyPair\nThe name of the key pair. If you launch an Amazon EC2 instance without specifying a key pair, you cannot log on to it.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number of attempt retries on failure.\nInteger minInstanceCount\nDeprecated.\nInteger onFail\nAn action to run when the current object fails.\nReference Object, for example \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or is still running.\nReference Object, for example\"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when the current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} parent\nThe parent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString region\n The code for the Region in which the Amazon EC2 instance should run. By default, the instance runs in the same Region as the pipeline. You can run the instance in the same Region as a dependent dataset.\nEnumeration reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and will be retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runAsUser\nThe user to run the TaskRunner.\nString runsOn\nThis field is not allowed on this object.\nReference Object, for example, \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\n\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of an interval, at the end of the interval, or on demand.\nValues are: timeseries. Instances are scheduled at the end of each interval.\n\ncron. Instances are scheduled at the beginning of each interval.\n\nondemand. Allows you to run a pipeline one time per activation. You do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, call the ActivatePipeline operation for each subsequent run. Enumeration securityGroupIds\nThe IDs of one or more Amazon EC2 security groups to use for the instances in the resource pool.\nString securityGroups\nOne or more Amazon EC2 security groups to use for the instances in the resource pool.\nString spotBidPrice\nThe maximum amount per hour for your Spot Instance in dollars, which is a decimal value between 0 and 20.00, exclusive.\nString subnetId\nThe ID of the Amazon EC2 subnet in which to start the instance.\nString terminateAfter\nThe number of hours after which to terminate the resource.\nPeriod useOnDemandOnLastAttempt\nOn the last attempt to request a Spot Instance, make a request for On-Demand Instances rather than a Spot Instance. This ensures that if all previous attempts have failed, the last attempt is not interrupted.\nBoolean workerGroup\nThis field is not allowed on this object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html",
"title": "Linux AMI Virtualization Types"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html",
"title": "Finding a Linux AMI"
}
],
"text": "Optional Fields\nDescription\nSlot Type actionOnResourceFailure\nThe action taken after a resource failure for this resource. Valid values are \"retryall\" and \"retrynone\".\nString actionOnTaskFailure\nThe action taken after a task failure for this resource. Valid values are \"continue\" or \"terminate\".\nString associatePublicIpAddress\nIndicates whether to assign a public IP address to the instance. If the instance is in  Amazon EC2 or Amazon VPC, the default value is true. Otherwise, the default value is false.\nBoolean attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for the remote work completion. If set, then a remote activity that does not complete within the specified starting time may be retried.\nPeriod availabilityZone\nThe Availability Zone in which to launch the Amazon EC2 instance.\nString failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration httpProxy\nThe proxy host that clients use to connect to AWS services.\nReference Object, for example, \"httpProxy\":{\"ref\":\"myHttpProxyId\"} imageId\nThe ID of the AMI to use for the instance. By default, AWS Data Pipeline uses the HVM AMI virtualization type. The specific AMI IDs used are based on a Region. You can overwrite the default AMI by specifying the HVM AMI of your choice. For more information about AMI types, see Linux AMI Virtualization Types and Finding a Linux AMI in the Amazon EC2 User Guide for Linux Instances.\n\nString initTimeout\nThe amount of time to wait for the resource to start. \nPeriod instanceCount\nDeprecated.\nInteger instanceType\nThe type of Amazon EC2 instance to start.\nString keyPair\nThe name of the key pair. If you launch an Amazon EC2 instance without specifying a key pair, you cannot log on to it.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nThe maximum number of attempt retries on failure.\nInteger minInstanceCount\nDeprecated.\nInteger onFail\nAn action to run when the current object fails.\nReference Object, for example \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or is still running.\nReference Object, for example\"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when the current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} parent\nThe parent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString region\n The code for the Region in which the Amazon EC2 instance should run. By default, the instance runs in the same Region as the pipeline. You can run the instance in the same Region as a dependent dataset.\nEnumeration reportProgressTimeout\nThe timeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and will be retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod runAsUser\nThe user to run the TaskRunner.\nString runsOn\nThis field is not allowed on this object.\nReference Object, for example, \"runsOn\":{\"ref\":\"myResourceId\"} scheduleType\n\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of an interval, at the end of the interval, or on demand.\nValues are: timeseries. Instances are scheduled at the end of each interval.\n\ncron. Instances are scheduled at the beginning of each interval.\n\nondemand. Allows you to run a pipeline one time per activation. You do not have to clone or re-create the pipeline to run it again. If you use an on-demand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use on-demand pipelines, call the ActivatePipeline operation for each subsequent run. Enumeration securityGroupIds\nThe IDs of one or more Amazon EC2 security groups to use for the instances in the resource pool.\nString securityGroups\nOne or more Amazon EC2 security groups to use for the instances in the resource pool.\nString spotBidPrice\nThe maximum amount per hour for your Spot Instance in dollars, which is a decimal value between 0 and 20.00, exclusive.\nString subnetId\nThe ID of the Amazon EC2 subnet in which to start the instance.\nString terminateAfter\nThe number of hours after which to terminate the resource.\nPeriod useOnDemandOnLastAttempt\nOn the last attempt to request a Spot Instance, make a request for On-Demand Instances rather than a Spot Instance. This ensures that if all previous attempts have failed, the last attempt is not interrupted.\nBoolean workerGroup\nThis field is not allowed on this object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, for example, \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain on which the object failed.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nStep logs are available only on Amazon EMR activity attempts.\nString errorId\nThe error ID if this object failed.\nString errorMessage\nThe error message if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @failureReason\nThe reason for the resource failure.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for Amazon EMR activities.\nString @healthStatus\nThe health status of the object that reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nThe most recent time that the remote activity reported progress.\nDateTime @scheduledEndTime\nThe schedule end time for the object.\nDateTime @scheduledStartTime\nThe schedule start time for the object.\nDateTime @status\nThe status of this object.\nString @version\nThe pipeline version with which the object was created.\nString @waitingOn\nDescription of the list of dependencies on which this object is waiting.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, for example, \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain on which the object failed.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nStep logs are available only on Amazon EMR activity attempts.\nString errorId\nThe error ID if this object failed.\nString errorMessage\nThe error message if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @failureReason\nThe reason for the resource failure.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for Amazon EMR activities.\nString @healthStatus\nThe health status of the object that reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nId of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nThe most recent time that the remote activity reported progress.\nDateTime @scheduledEndTime\nThe schedule end time for the object.\nDateTime @scheduledStartTime\nThe schedule start time for the object.\nDateTime @status\nThe status of this object.\nString @version\nThe pipeline version with which the object was created.\nString @waitingOn\nDescription of the list of dependencies on which this object is waiting.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe place of an object in the lifecycle. Component objects give rise to instance objects, which execute attempt objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe place of an object in the lifecycle. Component objects give rise to instance objects, which execute attempt objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-object-emrcluster",
"main_header": "EmrCluster",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-schedulers",
"text": "Schedulers"
},
"links": [
{
"url": "https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/FairScheduler.html",
"title": "FairScheduler"
},
{
"url": "https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html",
"title": "CapacityScheduler"
}
],
"text": "FairScheduler \u00e2\u0080\u0094 Attempts to schedule resources evenly over a significant period of time.\n\nCapacityScheduler \u00e2\u0080\u0094 Uses queues to allow cluster administrators to assign users to queues of varying priority and resource allocation. Default \u00e2\u0080\u0094 Used by the cluster, which could be configured by your site.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-schedulers",
"main_header": "Schedulers",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html",
"title": "Signature Version 4"
},
{
"url": "http://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/",
"title": "Amazon S3 Update \u00e2\u0080\u0093 SigV2 Deprecation Period Extended and Modified"
}
],
"text": "ImportantAll Amazon EMR clusters created using release version 5.22.0 or later use Signature Version 4 to authenticate requests to Amazon S3. Some earlier release versions use Signature Version 2. Signature Version 2 support is being discontinued. For more information, see Amazon S3 Update \u00e2\u0080\u0093 SigV2 Deprecation Period Extended and Modified. We strongly recommend that you use an Amazon EMR release version that supports Signature Version 4. For earlier version releases, beginning with EMR 4.7.x, the most recent release in the series has been updated to support Signature Version 4. When using an earlier version EMR release, we recommend that you use the latest release in the series. In addition, avoid releases earlier than EMR 4.7.0.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emrcluster-release-versions",
"main_header": "Amazon EMR Release Versions",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"links": [],
"text": "Important",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emrcluster-release-versions",
"main_header": "Amazon EMR Release Versions",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html",
"title": "Signature Version 4"
},
{
"url": "http://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/",
"title": "Amazon S3 Update \u00e2\u0080\u0093 SigV2 Deprecation Period Extended and Modified"
}
],
"text": "All Amazon EMR clusters created using release version 5.22.0 or later use Signature Version 4 to authenticate requests to Amazon S3. Some earlier release versions use Signature Version 2. Signature Version 2 support is being discontinued. For more information, see Amazon S3 Update \u00e2\u0080\u0093 SigV2 Deprecation Period Extended and Modified. We strongly recommend that you use an Amazon EMR release version that supports Signature Version 4. For earlier version releases, beginning with EMR 4.7.x, the most recent release in the series has been updated to support Signature Version 4. When using an earlier version EMR release, we recommend that you use the latest release in the series. In addition, avoid releases earlier than EMR 4.7.0.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emrcluster-release-versions",
"main_header": "Amazon EMR Release Versions",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html",
"title": "Signature Version 4"
},
{
"url": "http://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/",
"title": "Amazon S3 Update \u00e2\u0080\u0093 SigV2 Deprecation Period Extended and Modified"
}
],
"text": "All Amazon EMR clusters created using release version 5.22.0 or later use Signature Version 4 to authenticate requests to Amazon S3. Some earlier release versions use Signature Version 2. Signature Version 2 support is being discontinued. For more information, see Amazon S3 Update \u00e2\u0080\u0093 SigV2 Deprecation Period Extended and Modified. We strongly recommend that you use an Amazon EMR release version that supports Signature Version 4. For earlier version releases, beginning with EMR 4.7.x, the most recent release in the series has been updated to support Signature Version 4. When using an earlier version EMR release, we recommend that you use the latest release in the series. In addition, avoid releases earlier than EMR 4.7.0.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emrcluster-release-versions",
"main_header": "Amazon EMR Release Versions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Specify a schedule reference to another object to set the dependency execution order for this object. You can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), you can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html"
}
],
"text": "Object Invocation Fields\nDescription\nSlot Type schedule\nThis object is invoked within the execution of a schedule interval. Specify a schedule reference to another object to set the dependency execution order for this object. You can satisfy this requirement by explicitly setting a schedule on the object, for example, by specifying \"schedule\": {\"ref\": \"DefaultSchedule\"}. In most cases, it is better to put the schedule reference on the default pipeline object so that all objects inherit that schedule. Or, if the pipeline has a tree of schedules (schedules within the master schedule), you can create a parent object that has a schedule reference. For more information about example optional schedule configurations, see https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html\nReference Object, for example, \"schedule\":{\"ref\":\"myScheduleId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-additional-sec-groups.html",
"title": "Amazon EMR Additional Security Groups"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/",
"title": "Amazon EMR Management Guide"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"title": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",
"title": "Instance Types That Support EBS Optimization"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html",
"title": "Using a custom AMI"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-security-groups.html",
"title": "Configure Security Groups"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"title": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",
"title": "Instance Types That Support EBS Optimization"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",
"title": "Instance Types That Support EBS Optimization"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html",
"title": "EBS I/O Characteristics"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html",
"title": "EBS Volume Types"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html",
"title": "EBS Volume Types"
}
],
"text": "Optional Fields\nDescription\nSlot Type actionOnResourceFailure\nThe action taken after a resource failure for this resource. Valid values are \"retryall\", which retries all tasks to the cluster for the specified duration, and \"retrynone\".\nString actionOnTaskFailure\nThe action taken after task failure for this resource. Valid values are \"continue\", meaning do not terminate the cluster, and \"terminate.\"\nString additionalMasterSecurityGroupIds\nThe identifier of additional master security groups of the EMR cluster, which follows the form sg-01XXXX6a. For more information, see Amazon EMR Additional Security Groups in the Amazon EMR Management Guide.\nString additionalSlaveSecurityGroupIds\nThe identifier of additional slave security groups of the EMR cluster, which follows the form sg-01XXXX6a.\nString amiVersion\nThe Amazon Machine Image (AMI) version that Amazon EMR uses to install the cluster nodes. For more information, see the Amazon EMR Management Guide.\nString applications\nApplications to install in the cluster with comma-separated arguments. By default, Hive and Pig are installed. This parameter is applicable only for Amazon EMR version 4.0 and later.\nString attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod availabilityZone\nThe Availability Zone in which to run the cluster.\nString bootstrapAction\nAn action to run when the cluster starts. You can specify comma-separated arguments. To specify multiple actions, up to 255, add multiple bootstrapAction fields. The default behavior is to start the cluster without any bootstrap actions.\nString configuration\nConfiguration for the Amazon EMR cluster. This parameter is applicable only for Amazon EMR version 4.0 and later.\nReference Object, for example, \"configuration\":{\"ref\":\"myEmrConfigurationId\"} coreInstanceBidPrice\nThe maximum Spot price your are willing to pay for Amazon EC2 instances. If a bid price is specified, Amazon EMR uses Spot Instances for the instance group. Specified in USD.\nString coreInstanceCount\nThe number of core nodes to use for the cluster.\nInteger coreInstanceType\nThe type of Amazon EC2 instance to use for core nodes. See Supported Amazon EC2 Instances for Amazon EMR Clusters .\nString coreGroupConfiguration\nThe configuration for the Amazon EMR cluster core instance group. This parameter is applicable only for Amazon EMR version 4.0 and later.\nReference Object, for example \u00e2\u0080\u009cconfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEmrConfigurationId\u00e2\u0080\u009d} coreEbsConfiguration\nThe configuration for Amazon EBS volumes that will be attached to each of the core nodes in the core group in the Amazon EMR cluster. For more information, see Instance Types That Support EBS Optimization in the Amazon EC2 User Guide for Linux Instances.\nReference Object, for example \u00e2\u0080\u009ccoreEbsConfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsConfiguration\u00e2\u0080\u009d} customAmiId\nApplies only to Amazon EMR release version 5.7.0 and later. Specifies the AMI ID of a custom AMI to use when Amazon EMR provisions Amazon EC2 instances. It can also be used instead of bootstrap actions to customize cluster node configurations. For more information, see the following topic in the Amazon EMR Management Guide. Using a custom AMI\nString EbsBlockDeviceConfig\n\nThe configuration of a requested Amazon EBS block device associated with the instance group. Includes a specified number of volumes that will be associated with each instance in the instance group. Includes volumesPerInstance and volumeSpecification, where: volumesPerInstance is the number of EBS volumes with a specific volume configuration that will be associated with each instance in the instance group. volumeSpecification is the Amazon EBS volume specifications, such as volume type, IOPS, and size in Gigibytes (GiB) that will be requested for the EBS volume attached to an EC2 instance in the Amazon EMR cluster. Reference Object, for example \u00e2\u0080\u009cEbsBlockDeviceConfig\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsBlockDeviceConfig\u00e2\u0080\u009d} emrManagedMasterSecurityGroupId\nThe identifier of the master security group of the Amazon EMR cluster, which follows the form of sg-01XXXX6a. For more information, see Configure Security Groups in the Amazon EMR Management Guide.\nString emrManagedSlaveSecurityGroupId\nThe identifier of the slave security group of the Amazon EMR cluster, which follows the form sg-01XXXX6a.\nString enableDebugging\nEnables debugging on the Amazon EMR cluster.\nString failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration hadoopSchedulerType\nThe scheduler type of the cluster. Valid types are: PARALLEL_FAIR_SCHEDULING, PARALLEL_CAPACITY_SCHEDULING, and DEFAULT_SCHEDULER.\nEnumeration httpProxy\nThe proxy host that clients use to connect to AWS services.\nReference Object, for example, \"httpProxy\":{\"ref\":\"myHttpProxyId\"} initTimeout\nThe amount of time to wait for the resource to start. \nPeriod keyPair\nThe Amazon EC2 key pair to use to log on to the master node of the Amazon EMR cluster.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod masterInstanceBidPrice\nThe maximum Spot price your are willing to pay for Amazon EC2 instances. It is a decimal value between 0 and 20.00, exclusive. Specified in USD. Setting this value enables Spot Instances for the Amazon EMR cluster master node. If a bid price is specified, Amazon EMR uses Spot Instances for the instance group.\nString masterInstanceType\nThe type of Amazon EC2 instance to use for the master node. See Supported Amazon EC2 Instances for Amazon EMR Clusters .\nString masterGroupConfiguration\nThe configuration for the Amazon EMR cluster master instance group. This parameter is applicable only for Amazon EMR version 4.0 and later.\nReference Object, for example \u00e2\u0080\u009cconfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEmrConfigurationId\u00e2\u0080\u009d} masterEbsConfiguration\nThe configuration for Amazon EBS volumes that will be attached to each of the master nodes in the master group in the Amazon EMR cluster. For more information, see Instance Types That Support EBS Optimization in the Amazon EC2 User Guide for Linux Instances.\nReference Object, for example \u00e2\u0080\u009cmasterEbsConfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsConfiguration\u00e2\u0080\u009d} maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure.\nInteger onFail\nAn action to run when the current object fails.\nReference Object, for example, \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or is still not completed.\nReference Object, for example, \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when the current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots are inherited.\nReference Object, for example. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString region\nThe code for the region that the Amazon EMR cluster should run in. By default, the cluster runs in the same region as the pipeline. You can run the cluster in the same region as a dependent dataset. \nEnumeration releaseLabel\nRelease label for the EMR cluster.\nString reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resourceRole\nThe IAM role that AWS Data Pipeline uses to create the Amazon EMR cluster. The default role is DataPipelineDefaultRole. \nString retryDelay\nThe timeout duration between two retry attempts.\nPeriod role\nThe IAM role passed to Amazon EMR to create EC2 nodes.\nString runsOn\nThis field is not allowed on this object.\nReference Object, for example, \"runsOn\":{\"ref\":\"myResourceId\"} serviceAccessSecurityGroupId\nThe identifier for the service access security group of the Amazon EMR cluster. \nString. It follows the form of sg-01XXXX6a, for example, sg-1234abcd. scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of the interval, or end of the interval. Values are: cron, ondemand, and timeseries. The timeseries scheduling means that instances are scheduled at the end of each interval. The cron scheduling means that instances are scheduled at the beginning of each interval. An ondemand schedule allows you to run a pipeline one time per activation. You do not have to clone or re-create the pipeline to run it again. If you use an ondemand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run.\nEnumeration subnetId\nThe identifier of the subnet into which to launch the Amazon EMR cluster.\nString supportedProducts\nA parameter that installs third-party software on an Amazon EMR cluster, for example, a third-party distribution of Hadoop.\nString taskInstanceBidPrice\nThe maximum Spot price your are willing to pay for EC2 instances. A decimal value between 0 and 20.00, exclusive. Specified in USD. If a bid price is specified, Amazon EMR uses Spot Instances for the instance group.\nString taskInstanceCount\nThe number of task nodes to use for the Amazon EMR cluster.\nInteger taskInstanceType\nThe type of Amazon EC2 instance to use for task nodes.\nString taskGroupConfiguration\nThe configuration for the Amazon EMR cluster task instance group. This parameter is applicable only for Amazon EMR version 4.0 and later. \nReference Object, for example \u00e2\u0080\u009cconfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEmrConfigurationId\u00e2\u0080\u009d} taskEbsConfiguration\nThe configuration for Amazon EBS volumes that will be attached to each of the task nodes in the task group in the Amazon EMR cluster. For more information, see Instance Types That Support EBS Optimization in the Amazon EC2 User Guide for Linux Instances.\nReference Object, for example \u00e2\u0080\u009ctaskEbsConfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsConfiguration\u00e2\u0080\u009d} terminateAfter\nTerminate the resource after these many hours.\nInteger VolumeSpecification The Amazon EBS volume specifications, such as volume type, IOPS, and size in Gigibytes (GiB) that will be requested for the Amazon EBS volume attached to an Amazon EC2 instance in the Amazon EMR cluster. The node can be a core, master or task node. \nThe VolumeSpecification includes: iops() Integer. The number of I/O operations per second (IOPS) that the Amazon EBS volume supports, for example, 1000. For more information, see EBS I/O Characteristics in the Amazon EC2 User Guide for Linux Instances.\n\nsizeinGB(). Integer. The Amazon EBS volume size, in gibibytes (GiB), for example 500. For information about valid combinations of volume types and hard drive sizes, see EBS Volume Types in the Amazon EC2 User Guide for Linux Instances.\n\nvolumetType. String. The Amazon EBS volume type, for example, gp2. The supported volume types include standard, gp2, io1, st1, sc1, and others. For more information, see EBS Volume Types in the Amazon EC2 User Guide for Linux Instances. Reference Object, for example \u00e2\u0080\u009cVolumeSpecification\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyVolumeSpecification\u00e2\u0080\u009d} useOnDemandOnLastAttempt\nOn the last attempt to request a resource, make a request for On-Demand Instances rather than Spot Instances. This ensures that if all previous attempts have failed, the last attempt is not interrupted. \nBoolean workerGroup\nField not allowed on this object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-additional-sec-groups.html",
"title": "Amazon EMR Additional Security Groups"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/",
"title": "Amazon EMR Management Guide"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"title": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",
"title": "Instance Types That Support EBS Optimization"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html",
"title": "Using a custom AMI"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-security-groups.html",
"title": "Configure Security Groups"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"title": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",
"title": "Instance Types That Support EBS Optimization"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",
"title": "Instance Types That Support EBS Optimization"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html",
"title": "EBS I/O Characteristics"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html",
"title": "EBS Volume Types"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html",
"title": "EBS Volume Types"
}
],
"text": "Optional Fields\nDescription\nSlot Type actionOnResourceFailure\nThe action taken after a resource failure for this resource. Valid values are \"retryall\", which retries all tasks to the cluster for the specified duration, and \"retrynone\".\nString actionOnTaskFailure\nThe action taken after task failure for this resource. Valid values are \"continue\", meaning do not terminate the cluster, and \"terminate.\"\nString additionalMasterSecurityGroupIds\nThe identifier of additional master security groups of the EMR cluster, which follows the form sg-01XXXX6a. For more information, see Amazon EMR Additional Security Groups in the Amazon EMR Management Guide.\nString additionalSlaveSecurityGroupIds\nThe identifier of additional slave security groups of the EMR cluster, which follows the form sg-01XXXX6a.\nString amiVersion\nThe Amazon Machine Image (AMI) version that Amazon EMR uses to install the cluster nodes. For more information, see the Amazon EMR Management Guide.\nString applications\nApplications to install in the cluster with comma-separated arguments. By default, Hive and Pig are installed. This parameter is applicable only for Amazon EMR version 4.0 and later.\nString attemptStatus\nThe most recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set, then a remote activity that does not complete within the set time of starting may be retried.\nPeriod availabilityZone\nThe Availability Zone in which to run the cluster.\nString bootstrapAction\nAn action to run when the cluster starts. You can specify comma-separated arguments. To specify multiple actions, up to 255, add multiple bootstrapAction fields. The default behavior is to start the cluster without any bootstrap actions.\nString configuration\nConfiguration for the Amazon EMR cluster. This parameter is applicable only for Amazon EMR version 4.0 and later.\nReference Object, for example, \"configuration\":{\"ref\":\"myEmrConfigurationId\"} coreInstanceBidPrice\nThe maximum Spot price your are willing to pay for Amazon EC2 instances. If a bid price is specified, Amazon EMR uses Spot Instances for the instance group. Specified in USD.\nString coreInstanceCount\nThe number of core nodes to use for the cluster.\nInteger coreInstanceType\nThe type of Amazon EC2 instance to use for core nodes. See Supported Amazon EC2 Instances for Amazon EMR Clusters .\nString coreGroupConfiguration\nThe configuration for the Amazon EMR cluster core instance group. This parameter is applicable only for Amazon EMR version 4.0 and later.\nReference Object, for example \u00e2\u0080\u009cconfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEmrConfigurationId\u00e2\u0080\u009d} coreEbsConfiguration\nThe configuration for Amazon EBS volumes that will be attached to each of the core nodes in the core group in the Amazon EMR cluster. For more information, see Instance Types That Support EBS Optimization in the Amazon EC2 User Guide for Linux Instances.\nReference Object, for example \u00e2\u0080\u009ccoreEbsConfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsConfiguration\u00e2\u0080\u009d} customAmiId\nApplies only to Amazon EMR release version 5.7.0 and later. Specifies the AMI ID of a custom AMI to use when Amazon EMR provisions Amazon EC2 instances. It can also be used instead of bootstrap actions to customize cluster node configurations. For more information, see the following topic in the Amazon EMR Management Guide. Using a custom AMI\nString EbsBlockDeviceConfig\n\nThe configuration of a requested Amazon EBS block device associated with the instance group. Includes a specified number of volumes that will be associated with each instance in the instance group. Includes volumesPerInstance and volumeSpecification, where: volumesPerInstance is the number of EBS volumes with a specific volume configuration that will be associated with each instance in the instance group. volumeSpecification is the Amazon EBS volume specifications, such as volume type, IOPS, and size in Gigibytes (GiB) that will be requested for the EBS volume attached to an EC2 instance in the Amazon EMR cluster. Reference Object, for example \u00e2\u0080\u009cEbsBlockDeviceConfig\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsBlockDeviceConfig\u00e2\u0080\u009d} emrManagedMasterSecurityGroupId\nThe identifier of the master security group of the Amazon EMR cluster, which follows the form of sg-01XXXX6a. For more information, see Configure Security Groups in the Amazon EMR Management Guide.\nString emrManagedSlaveSecurityGroupId\nThe identifier of the slave security group of the Amazon EMR cluster, which follows the form sg-01XXXX6a.\nString enableDebugging\nEnables debugging on the Amazon EMR cluster.\nString failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration hadoopSchedulerType\nThe scheduler type of the cluster. Valid types are: PARALLEL_FAIR_SCHEDULING, PARALLEL_CAPACITY_SCHEDULING, and DEFAULT_SCHEDULER.\nEnumeration httpProxy\nThe proxy host that clients use to connect to AWS services.\nReference Object, for example, \"httpProxy\":{\"ref\":\"myHttpProxyId\"} initTimeout\nThe amount of time to wait for the resource to start. \nPeriod keyPair\nThe Amazon EC2 key pair to use to log on to the master node of the Amazon EMR cluster.\nString lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod masterInstanceBidPrice\nThe maximum Spot price your are willing to pay for Amazon EC2 instances. It is a decimal value between 0 and 20.00, exclusive. Specified in USD. Setting this value enables Spot Instances for the Amazon EMR cluster master node. If a bid price is specified, Amazon EMR uses Spot Instances for the instance group.\nString masterInstanceType\nThe type of Amazon EC2 instance to use for the master node. See Supported Amazon EC2 Instances for Amazon EMR Clusters .\nString masterGroupConfiguration\nThe configuration for the Amazon EMR cluster master instance group. This parameter is applicable only for Amazon EMR version 4.0 and later.\nReference Object, for example \u00e2\u0080\u009cconfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEmrConfigurationId\u00e2\u0080\u009d} masterEbsConfiguration\nThe configuration for Amazon EBS volumes that will be attached to each of the master nodes in the master group in the Amazon EMR cluster. For more information, see Instance Types That Support EBS Optimization in the Amazon EC2 User Guide for Linux Instances.\nReference Object, for example \u00e2\u0080\u009cmasterEbsConfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsConfiguration\u00e2\u0080\u009d} maxActiveInstances\nThe maximum number of concurrent active instances of a component. Re-runs do not count toward the number of active instances.\nInteger maximumRetries\nMaximum number attempt retries on failure.\nInteger onFail\nAn action to run when the current object fails.\nReference Object, for example, \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or is still not completed.\nReference Object, for example, \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when the current object succeeds.\nReference Object, for example, \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots are inherited.\nReference Object, for example. \"parent\":{\"ref\":\"myBaseObjectId\"} pipelineLogUri\nThe Amazon S3 URI (such as 's3://BucketName/Key/') for uploading logs for the pipeline.\nString region\nThe code for the region that the Amazon EMR cluster should run in. By default, the cluster runs in the same region as the pipeline. You can run the cluster in the same region as a dependent dataset. \nEnumeration releaseLabel\nRelease label for the EMR cluster.\nString reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod resourceRole\nThe IAM role that AWS Data Pipeline uses to create the Amazon EMR cluster. The default role is DataPipelineDefaultRole. \nString retryDelay\nThe timeout duration between two retry attempts.\nPeriod role\nThe IAM role passed to Amazon EMR to create EC2 nodes.\nString runsOn\nThis field is not allowed on this object.\nReference Object, for example, \"runsOn\":{\"ref\":\"myResourceId\"} serviceAccessSecurityGroupId\nThe identifier for the service access security group of the Amazon EMR cluster. \nString. It follows the form of sg-01XXXX6a, for example, sg-1234abcd. scheduleType\nSchedule type allows you to specify whether the objects in your pipeline definition should be scheduled at the beginning of the interval, or end of the interval. Values are: cron, ondemand, and timeseries. The timeseries scheduling means that instances are scheduled at the end of each interval. The cron scheduling means that instances are scheduled at the beginning of each interval. An ondemand schedule allows you to run a pipeline one time per activation. You do not have to clone or re-create the pipeline to run it again. If you use an ondemand schedule, it must be specified in the default object and must be the only scheduleType specified for objects in the pipeline. To use ondemand pipelines, call the ActivatePipeline operation for each subsequent run.\nEnumeration subnetId\nThe identifier of the subnet into which to launch the Amazon EMR cluster.\nString supportedProducts\nA parameter that installs third-party software on an Amazon EMR cluster, for example, a third-party distribution of Hadoop.\nString taskInstanceBidPrice\nThe maximum Spot price your are willing to pay for EC2 instances. A decimal value between 0 and 20.00, exclusive. Specified in USD. If a bid price is specified, Amazon EMR uses Spot Instances for the instance group.\nString taskInstanceCount\nThe number of task nodes to use for the Amazon EMR cluster.\nInteger taskInstanceType\nThe type of Amazon EC2 instance to use for task nodes.\nString taskGroupConfiguration\nThe configuration for the Amazon EMR cluster task instance group. This parameter is applicable only for Amazon EMR version 4.0 and later. \nReference Object, for example \u00e2\u0080\u009cconfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEmrConfigurationId\u00e2\u0080\u009d} taskEbsConfiguration\nThe configuration for Amazon EBS volumes that will be attached to each of the task nodes in the task group in the Amazon EMR cluster. For more information, see Instance Types That Support EBS Optimization in the Amazon EC2 User Guide for Linux Instances.\nReference Object, for example \u00e2\u0080\u009ctaskEbsConfiguration\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyEbsConfiguration\u00e2\u0080\u009d} terminateAfter\nTerminate the resource after these many hours.\nInteger VolumeSpecification The Amazon EBS volume specifications, such as volume type, IOPS, and size in Gigibytes (GiB) that will be requested for the Amazon EBS volume attached to an Amazon EC2 instance in the Amazon EMR cluster. The node can be a core, master or task node. \nThe VolumeSpecification includes: iops() Integer. The number of I/O operations per second (IOPS) that the Amazon EBS volume supports, for example, 1000. For more information, see EBS I/O Characteristics in the Amazon EC2 User Guide for Linux Instances.\n\nsizeinGB(). Integer. The Amazon EBS volume size, in gibibytes (GiB), for example 500. For information about valid combinations of volume types and hard drive sizes, see EBS Volume Types in the Amazon EC2 User Guide for Linux Instances.\n\nvolumetType. String. The Amazon EBS volume type, for example, gp2. The supported volume types include standard, gp2, io1, st1, sc1, and others. For more information, see EBS Volume Types in the Amazon EC2 User Guide for Linux Instances. Reference Object, for example \u00e2\u0080\u009cVolumeSpecification\u00e2\u0080\u009d: {\u00e2\u0080\u009cref\u00e2\u0080\u009d: \u00e2\u0080\u009cmyVolumeSpecification\u00e2\u0080\u009d} useOnDemandOnLastAttempt\nOn the last attempt to request a resource, make a request for On-Demand Instances rather than Spot Instances. This ensures that if all previous attempts have failed, the last attempt is not interrupted. \nBoolean workerGroup\nField not allowed on this object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, for example, \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain on which the object failed.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nStep logs available only on Amazon EMR activity attempts.\nString errorId\nThe error ID if this object failed.\nString errorMessage\nThe error message if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @failureReason\nThe reason for the resource failure.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for Amazon EMR activities.\nString @healthStatus\nThe health status of the object that reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nID of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version with which the object was created.\nString @waitingOn\nDescription of the list of dependencies on which this object is waiting.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, for example, \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain on which the object failed.\nReference Object, for example, \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nStep logs available only on Amazon EMR activity attempts.\nString errorId\nThe error ID if this object failed.\nString errorMessage\nThe error message if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString @failureReason\nThe reason for the resource failure.\nString @finishedTime\nThe time at which this object finished its execution.\nDateTime hadoopJobLog\nHadoop job logs available on attempts for Amazon EMR activities.\nString @healthStatus\nThe health status of the object that reflects success or failure of the last object instance that reached a terminated state.\nString @healthStatusFromInstanceId\nID of the last instance object that reached a terminated state.\nString @healthStatusUpdatedTime\nTime at which the health status was updated last time.\nDateTime hostname\nThe host name of client that picked up the task attempt.\nString @lastDeactivatedTime\nThe time at which this object was last deactivated.\nDateTime @latestCompletedRunTime\nTime the latest run for which the execution completed.\nDateTime @latestRunTime\nTime the latest run for which the execution was scheduled.\nDateTime @nextRunTime\nTime of run to be scheduled next.\nDateTime reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version with which the object was created.\nString @waitingOn\nDescription of the list of dependencies on which this object is waiting.\nReference Object, for example, \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe place of an object in the lifecycle. Component objects give rise to instance objects, which execute attempt objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe place of an object in the lifecycle. Component objects give rise to instance objects, which execute attempt objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "EmrActivity",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example",
"text": "Examples"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html#emrcluster-example",
"main_header": "Examples",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-launch",
"text": "Launch an Amazon EMR cluster with hadoopVersion"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html#emrcluster-example-launch",
"main_header": "Launch an Amazon EMR cluster with hadoopVersion",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-launch",
"text": "Launch an Amazon EMR cluster with hadoopVersion"
},
"links": [],
"text": "The following example launches an Amazon EMR cluster using AMI version 1.0 and Hadoop 0.20.{ \"id\" : \"MyEmrCluster\", \"type\" : \"EmrCluster\", \"hadoopVersion\" : \"0.20\", \"keyPair\" : \"my-key-pair\", \"masterInstanceType\" : \"m3.xlarge\", \"coreInstanceType\" : \"m3.xlarge\", \"coreInstanceCount\" : \"10\", \"taskInstanceType\" : \"m3.xlarge\", \"taskInstanceCount\": \"10\", \"bootstrapAction\" : [\"s3://Region.elasticmapreduce/bootstrap-actions/configure-hadoop,arg1,arg2,arg3\",\"s3://Region.elasticmapreduce/bootstrap-actions/configure-hadoop/configure-other-stuff,arg1,arg2\"]\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html#emrcluster-example-launch",
"main_header": "Launch an Amazon EMR cluster with hadoopVersion",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-launch",
"text": "Launch an Amazon EMR cluster with hadoopVersion"
},
"links": [],
"text": "The following example launches an Amazon EMR cluster using AMI version 1.0 and Hadoop 0.20.{ \"id\" : \"MyEmrCluster\", \"type\" : \"EmrCluster\", \"hadoopVersion\" : \"0.20\", \"keyPair\" : \"my-key-pair\", \"masterInstanceType\" : \"m3.xlarge\", \"coreInstanceType\" : \"m3.xlarge\", \"coreInstanceCount\" : \"10\", \"taskInstanceType\" : \"m3.xlarge\", \"taskInstanceCount\": \"10\", \"bootstrapAction\" : [\"s3://Region.elasticmapreduce/bootstrap-actions/configure-hadoop,arg1,arg2,arg3\",\"s3://Region.elasticmapreduce/bootstrap-actions/configure-hadoop/configure-other-stuff,arg1,arg2\"]\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html#emrcluster-example-launch",
"main_header": "Launch an Amazon EMR cluster with hadoopVersion",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-launch",
"text": "Launch an Amazon EMR cluster with hadoopVersion"
},
"links": [],
"text": "The following example launches an Amazon EMR cluster using AMI version 1.0 and Hadoop 0.20.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-launch.html#emrcluster-example-launch",
"main_header": "Launch an Amazon EMR cluster with hadoopVersion",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-release-label",
"text": "Launch an Amazon EMR cluster with release label emr-4.x or greater"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html#emrcluster-example-release-label",
"main_header": "Launch an Amazon EMR cluster with release label emr-4.x or greater",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-release-label",
"text": "Launch an Amazon EMR cluster with release label emr-4.x or greater"
},
"links": [],
"text": "The following example launches an Amazon EMR cluster using the newer releaseLabel field:{ \"id\" : \"MyEmrCluster\", \"type\" : \"EmrCluster\", \"keyPair\" : \"my-key-pair\", \"masterInstanceType\" : \"m3.xlarge\", \"coreInstanceType\" : \"m3.xlarge\", \"coreInstanceCount\" : \"10\", \"taskInstanceType\" : \"m3.xlarge\", \"taskInstanceCount\": \"10\", \"releaseLabel\": \"emr-4.1.0\", \"applications\": [\"spark\", \"hive\", \"pig\"], \"configuration\": {\"ref\":\"myConfiguration\"} }",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html#emrcluster-example-release-label",
"main_header": "Launch an Amazon EMR cluster with release label emr-4.x or greater",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-release-label",
"text": "Launch an Amazon EMR cluster with release label emr-4.x or greater"
},
"links": [],
"text": "The following example launches an Amazon EMR cluster using the newer releaseLabel field:{ \"id\" : \"MyEmrCluster\", \"type\" : \"EmrCluster\", \"keyPair\" : \"my-key-pair\", \"masterInstanceType\" : \"m3.xlarge\", \"coreInstanceType\" : \"m3.xlarge\", \"coreInstanceCount\" : \"10\", \"taskInstanceType\" : \"m3.xlarge\", \"taskInstanceCount\": \"10\", \"releaseLabel\": \"emr-4.1.0\", \"applications\": [\"spark\", \"hive\", \"pig\"], \"configuration\": {\"ref\":\"myConfiguration\"} }",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html#emrcluster-example-release-label",
"main_header": "Launch an Amazon EMR cluster with release label emr-4.x or greater",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-release-label",
"text": "Launch an Amazon EMR cluster with release label emr-4.x or greater"
},
"links": [],
"text": "The following example launches an Amazon EMR cluster using the newer releaseLabel field:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-release-label.html#emrcluster-example-release-label",
"main_header": "Launch an Amazon EMR cluster with release label emr-4.x or greater",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-install-software",
"text": "Install additional software on your Amazon EMR cluster"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html#emrcluster-example-install-software",
"main_header": "Install additional software on your Amazon EMR cluster",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-install-software",
"text": "Install additional software on your Amazon EMR cluster"
},
"links": [],
"text": "EmrCluster provides the supportedProducts field that installs third-party software on an Amazon EMR cluster, for example, it lets you install a custom distribution of Hadoop, such as MapR. It accepts a comma-separated list of arguments for the third-party software to read and act on. The following example shows how to use the supportedProducts field of EmrCluster to create a custom MapR M3 edition cluster with Karmasphere Analytics installed, and run an EmrActivity object on it.{ \"id\": \"MyEmrActivity\", \"type\": \"EmrActivity\", \"schedule\": {\"ref\": \"ResourcePeriod\"}, \"runsOn\": {\"ref\": \"MyEmrCluster\"}, \"postStepCommand\": \"echo Ending job >> /mnt/var/log/stepCommand.txt\", \"preStepCommand\": \"echo Starting job > /mnt/var/log/stepCommand.txt\", \"step\": \"/home/hadoop/contrib/streaming/hadoop-streaming.jar,-input,s3n://elasticmapreduce/samples/wordcount/input,-output, \\ hdfs:///output32113/,-mapper,s3n://elasticmapreduce/samples/wordcount/wordSplitter.py,-reducer,aggregate\" }, { \"id\": \"MyEmrCluster\", \"type\": \"EmrCluster\", \"schedule\": {\"ref\": \"ResourcePeriod\"}, \"supportedProducts\": [\"mapr,--edition,m3,--version,1.2,--key1,value1\",\"karmasphere-enterprise-utility\"], \"masterInstanceType\": \"m3.xlarge\", \"taskInstanceType\": \"m3.xlarge\"\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html#emrcluster-example-install-software",
"main_header": "Install additional software on your Amazon EMR cluster",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-install-software",
"text": "Install additional software on your Amazon EMR cluster"
},
"links": [],
"text": "EmrCluster provides the supportedProducts field that installs third-party software on an Amazon EMR cluster, for example, it lets you install a custom distribution of Hadoop, such as MapR. It accepts a comma-separated list of arguments for the third-party software to read and act on. The following example shows how to use the supportedProducts field of EmrCluster to create a custom MapR M3 edition cluster with Karmasphere Analytics installed, and run an EmrActivity object on it.{ \"id\": \"MyEmrActivity\", \"type\": \"EmrActivity\", \"schedule\": {\"ref\": \"ResourcePeriod\"}, \"runsOn\": {\"ref\": \"MyEmrCluster\"}, \"postStepCommand\": \"echo Ending job >> /mnt/var/log/stepCommand.txt\", \"preStepCommand\": \"echo Starting job > /mnt/var/log/stepCommand.txt\", \"step\": \"/home/hadoop/contrib/streaming/hadoop-streaming.jar,-input,s3n://elasticmapreduce/samples/wordcount/input,-output, \\ hdfs:///output32113/,-mapper,s3n://elasticmapreduce/samples/wordcount/wordSplitter.py,-reducer,aggregate\" }, { \"id\": \"MyEmrCluster\", \"type\": \"EmrCluster\", \"schedule\": {\"ref\": \"ResourcePeriod\"}, \"supportedProducts\": [\"mapr,--edition,m3,--version,1.2,--key1,value1\",\"karmasphere-enterprise-utility\"], \"masterInstanceType\": \"m3.xlarge\", \"taskInstanceType\": \"m3.xlarge\"\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html#emrcluster-example-install-software",
"main_header": "Install additional software on your Amazon EMR cluster",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-install-software",
"text": "Install additional software on your Amazon EMR cluster"
},
"links": [],
"text": "EmrCluster provides the supportedProducts field that installs third-party software on an Amazon EMR cluster, for example, it lets you install a custom distribution of Hadoop, such as MapR. It accepts a comma-separated list of arguments for the third-party software to read and act on. The following example shows how to use the supportedProducts field of EmrCluster to create a custom MapR M3 edition cluster with Karmasphere Analytics installed, and run an EmrActivity object on it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-install-software.html#emrcluster-example-install-software",
"main_header": "Install additional software on your Amazon EMR cluster",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example1-disable-encryption",
"text": "Disable server-side encryption on 3.x releases"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html#emrcluster-example1-disable-encryption",
"main_header": "Disable server-side encryption on 3.x releases",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example1-disable-encryption",
"text": "Disable server-side encryption on 3.x releases"
},
"links": [],
"text": "An EmrCluster activity with a Hadoop version 2.x created by AWS Data Pipeline enables server-side encryption by default. If you would like to disable server-side encryption, you must specify a bootstrap action in the cluster object definition.The following example creates an EmrCluster activity with server-side encryption disabled:{ \"id\":\"NoSSEEmrCluster\", \"type\":\"EmrCluster\", \"hadoopVersion\":\"2.x\", \"keyPair\":\"my-key-pair\", \"masterInstanceType\":\"m3.xlarge\", \"coreInstanceType\":\"m3.large\", \"coreInstanceCount\":\"10\", \"taskInstanceType\":\"m3.large\", \"taskInstanceCount\":\"10\", \"bootstrapAction\":[\"s3://Region.elasticmapreduce/bootstrap-actions/configure-hadoop,-e, fs.s3.enableServerSideEncryption=false\"]\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html#emrcluster-example1-disable-encryption",
"main_header": "Disable server-side encryption on 3.x releases",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example1-disable-encryption",
"text": "Disable server-side encryption on 3.x releases"
},
"links": [],
"text": "An EmrCluster activity with a Hadoop version 2.x created by AWS Data Pipeline enables server-side encryption by default. If you would like to disable server-side encryption, you must specify a bootstrap action in the cluster object definition.The following example creates an EmrCluster activity with server-side encryption disabled:{ \"id\":\"NoSSEEmrCluster\", \"type\":\"EmrCluster\", \"hadoopVersion\":\"2.x\", \"keyPair\":\"my-key-pair\", \"masterInstanceType\":\"m3.xlarge\", \"coreInstanceType\":\"m3.large\", \"coreInstanceCount\":\"10\", \"taskInstanceType\":\"m3.large\", \"taskInstanceCount\":\"10\", \"bootstrapAction\":[\"s3://Region.elasticmapreduce/bootstrap-actions/configure-hadoop,-e, fs.s3.enableServerSideEncryption=false\"]\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html#emrcluster-example1-disable-encryption",
"main_header": "Disable server-side encryption on 3.x releases",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example1-disable-encryption",
"text": "Disable server-side encryption on 3.x releases"
},
"links": [],
"text": "An EmrCluster activity with a Hadoop version 2.x created by AWS Data Pipeline enables server-side encryption by default. If you would like to disable server-side encryption, you must specify a bootstrap action in the cluster object definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html#emrcluster-example1-disable-encryption",
"main_header": "Disable server-side encryption on 3.x releases",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example1-disable-encryption",
"text": "Disable server-side encryption on 3.x releases"
},
"links": [],
"text": "The following example creates an EmrCluster activity with server-side encryption disabled:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example1-disable-encryption.html#emrcluster-example1-disable-encryption",
"main_header": "Disable server-side encryption on 3.x releases",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example2-disable-encryption",
"text": "Disable server-side encryption on 4.x releases"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html#emrcluster-example2-disable-encryption",
"main_header": "Disable server-side encryption on 4.x releases",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example2-disable-encryption",
"text": "Disable server-side encryption on 4.x releases"
},
"links": [],
"text": "You must disable server-side encryption using a EmrConfiguration object.The following example creates an EmrCluster activity with server-side encryption disabled: { \"name\": \"ReleaseLabelCluster\", \"releaseLabel\": \"emr-4.1.0\", \"applications\": [\"spark\", \"hive\", \"pig\"], \"id\": \"myResourceId\", \"type\": \"EmrCluster\", \"configuration\": { \"ref\": \"disableSSE\" } }, { \"name\": \"disableSSE\", \"id\": \"disableSSE\", \"type\": \"EmrConfiguration\", \"classification\": \"emrfs-site\", \"property\": [{ \"ref\": \"enableServerSideEncryption\" } ] }, { \"name\": \"enableServerSideEncryption\", \"id\": \"enableServerSideEncryption\", \"type\": \"Property\", \"key\": \"fs.s3.enableServerSideEncryption\", \"value\": \"false\" }",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html#emrcluster-example2-disable-encryption",
"main_header": "Disable server-side encryption on 4.x releases",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example2-disable-encryption",
"text": "Disable server-side encryption on 4.x releases"
},
"links": [],
"text": "You must disable server-side encryption using a EmrConfiguration object.The following example creates an EmrCluster activity with server-side encryption disabled: { \"name\": \"ReleaseLabelCluster\", \"releaseLabel\": \"emr-4.1.0\", \"applications\": [\"spark\", \"hive\", \"pig\"], \"id\": \"myResourceId\", \"type\": \"EmrCluster\", \"configuration\": { \"ref\": \"disableSSE\" } }, { \"name\": \"disableSSE\", \"id\": \"disableSSE\", \"type\": \"EmrConfiguration\", \"classification\": \"emrfs-site\", \"property\": [{ \"ref\": \"enableServerSideEncryption\" } ] }, { \"name\": \"enableServerSideEncryption\", \"id\": \"enableServerSideEncryption\", \"type\": \"Property\", \"key\": \"fs.s3.enableServerSideEncryption\", \"value\": \"false\" }",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html#emrcluster-example2-disable-encryption",
"main_header": "Disable server-side encryption on 4.x releases",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example2-disable-encryption",
"text": "Disable server-side encryption on 4.x releases"
},
"links": [],
"text": "You must disable server-side encryption using a EmrConfiguration object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html#emrcluster-example2-disable-encryption",
"main_header": "Disable server-side encryption on 4.x releases",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example2-disable-encryption",
"text": "Disable server-side encryption on 4.x releases"
},
"links": [],
"text": "The following example creates an EmrCluster activity with server-side encryption disabled:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example2-disable-encryption.html#emrcluster-example2-disable-encryption",
"main_header": "Disable server-side encryption on 4.x releases",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-hadoop-kms",
"text": "Configure Hadoop KMS ACLs and create encryption zones in HDFS"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html#emrcluster-example-hadoop-kms",
"main_header": "Configure Hadoop KMS ACLs and create encryption zones in HDFS",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-hadoop-kms",
"text": "Configure Hadoop KMS ACLs and create encryption zones in HDFS"
},
"links": [],
"text": "The following objects create ACLs for Hadoop KMS and create encryption zones and corresponding encryption keys in HDFS:{ \"name\": \"kmsAcls\", \"id\": \"kmsAcls\", \"type\": \"EmrConfiguration\", \"classification\": \"hadoop-kms-acls\", \"property\": [ {\"ref\":\"kmsBlacklist\"}, {\"ref\":\"kmsAcl\"} ] }, { \"name\": \"hdfsEncryptionZone\", \"id\": \"hdfsEncryptionZone\", \"type\": \"EmrConfiguration\", \"classification\": \"hdfs-encryption-zones\", \"property\": [ {\"ref\":\"hdfsPath1\"}, {\"ref\":\"hdfsPath2\"} ] }, { \"name\": \"kmsBlacklist\", \"id\": \"kmsBlacklist\", \"type\": \"Property\", \"key\": \"hadoop.kms.blacklist.CREATE\", \"value\": \"foo,myBannedUser\" }, { \"name\": \"kmsAcl\", \"id\": \"kmsAcl\", \"type\": \"Property\", \"key\": \"hadoop.kms.acl.ROLLOVER\", \"value\": \"myAllowedUser\" }, { \"name\": \"hdfsPath1\", \"id\": \"hdfsPath1\", \"type\": \"Property\", \"key\": \"/myHDFSPath1\", \"value\": \"path1_key\" }, { \"name\": \"hdfsPath2\", \"id\": \"hdfsPath2\", \"type\": \"Property\", \"key\": \"/myHDFSPath2\", \"value\": \"path2_key\" }",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html#emrcluster-example-hadoop-kms",
"main_header": "Configure Hadoop KMS ACLs and create encryption zones in HDFS",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-hadoop-kms",
"text": "Configure Hadoop KMS ACLs and create encryption zones in HDFS"
},
"links": [],
"text": "The following objects create ACLs for Hadoop KMS and create encryption zones and corresponding encryption keys in HDFS:{ \"name\": \"kmsAcls\", \"id\": \"kmsAcls\", \"type\": \"EmrConfiguration\", \"classification\": \"hadoop-kms-acls\", \"property\": [ {\"ref\":\"kmsBlacklist\"}, {\"ref\":\"kmsAcl\"} ] }, { \"name\": \"hdfsEncryptionZone\", \"id\": \"hdfsEncryptionZone\", \"type\": \"EmrConfiguration\", \"classification\": \"hdfs-encryption-zones\", \"property\": [ {\"ref\":\"hdfsPath1\"}, {\"ref\":\"hdfsPath2\"} ] }, { \"name\": \"kmsBlacklist\", \"id\": \"kmsBlacklist\", \"type\": \"Property\", \"key\": \"hadoop.kms.blacklist.CREATE\", \"value\": \"foo,myBannedUser\" }, { \"name\": \"kmsAcl\", \"id\": \"kmsAcl\", \"type\": \"Property\", \"key\": \"hadoop.kms.acl.ROLLOVER\", \"value\": \"myAllowedUser\" }, { \"name\": \"hdfsPath1\", \"id\": \"hdfsPath1\", \"type\": \"Property\", \"key\": \"/myHDFSPath1\", \"value\": \"path1_key\" }, { \"name\": \"hdfsPath2\", \"id\": \"hdfsPath2\", \"type\": \"Property\", \"key\": \"/myHDFSPath2\", \"value\": \"path2_key\" }",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html#emrcluster-example-hadoop-kms",
"main_header": "Configure Hadoop KMS ACLs and create encryption zones in HDFS",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-hadoop-kms",
"text": "Configure Hadoop KMS ACLs and create encryption zones in HDFS"
},
"links": [],
"text": "The following objects create ACLs for Hadoop KMS and create encryption zones and corresponding encryption keys in HDFS:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-hadoop-kms.html#emrcluster-example-hadoop-kms",
"main_header": "Configure Hadoop KMS ACLs and create encryption zones in HDFS",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-custom-iam-roles",
"text": "Specify custom IAM roles"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html#emrcluster-example-custom-iam-roles",
"main_header": "Specify custom IAM roles",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-custom-iam-roles",
"text": "Specify custom IAM roles"
},
"links": [],
"text": "By default, AWS Data Pipeline passes DataPipelineDefaultRole as the Amazon EMR service role and DataPipelineDefaultResourceRole as the Amazon EC2 instance profile to create resources on your behalf. However, you can create a custom Amazon EMR service role and a custom instance profile and use them instead. AWS Data Pipeline should have sufficient permissions to create clusters using the custom role, and you must add AWS Data Pipeline as a trusted entity.The following example object specifies custom roles for the Amazon EMR cluster:{ \"id\":\"MyEmrCluster\", \"type\":\"EmrCluster\", \"hadoopVersion\":\"2.x\", \"keyPair\":\"my-key-pair\", \"masterInstanceType\":\"m3.xlarge\", \"coreInstanceType\":\"m3.large\", \"coreInstanceCount\":\"10\", \"taskInstanceType\":\"m3.large\", \"taskInstanceCount\":\"10\", \"role\":\"emrServiceRole\", \"resourceRole\":\"emrInstanceProfile\"\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html#emrcluster-example-custom-iam-roles",
"main_header": "Specify custom IAM roles",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-custom-iam-roles",
"text": "Specify custom IAM roles"
},
"links": [],
"text": "By default, AWS Data Pipeline passes DataPipelineDefaultRole as the Amazon EMR service role and DataPipelineDefaultResourceRole as the Amazon EC2 instance profile to create resources on your behalf. However, you can create a custom Amazon EMR service role and a custom instance profile and use them instead. AWS Data Pipeline should have sufficient permissions to create clusters using the custom role, and you must add AWS Data Pipeline as a trusted entity.The following example object specifies custom roles for the Amazon EMR cluster:{ \"id\":\"MyEmrCluster\", \"type\":\"EmrCluster\", \"hadoopVersion\":\"2.x\", \"keyPair\":\"my-key-pair\", \"masterInstanceType\":\"m3.xlarge\", \"coreInstanceType\":\"m3.large\", \"coreInstanceCount\":\"10\", \"taskInstanceType\":\"m3.large\", \"taskInstanceCount\":\"10\", \"role\":\"emrServiceRole\", \"resourceRole\":\"emrInstanceProfile\"\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html#emrcluster-example-custom-iam-roles",
"main_header": "Specify custom IAM roles",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-custom-iam-roles",
"text": "Specify custom IAM roles"
},
"links": [],
"text": "By default, AWS Data Pipeline passes DataPipelineDefaultRole as the Amazon EMR service role and DataPipelineDefaultResourceRole as the Amazon EC2 instance profile to create resources on your behalf. However, you can create a custom Amazon EMR service role and a custom instance profile and use them instead. AWS Data Pipeline should have sufficient permissions to create clusters using the custom role, and you must add AWS Data Pipeline as a trusted entity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html#emrcluster-example-custom-iam-roles",
"main_header": "Specify custom IAM roles",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-custom-iam-roles",
"text": "Specify custom IAM roles"
},
"links": [],
"text": "The following example object specifies custom roles for the Amazon EMR cluster:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-custom-iam-roles.html#emrcluster-example-custom-iam-roles",
"main_header": "Specify custom IAM roles",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-java",
"text": "Use EmrCluster Resource in AWS SDK for Java"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html#emrcluster-example-java",
"main_header": "Use EmrCluster Resource in AWS SDK for Java",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-java",
"text": "Use EmrCluster Resource in AWS SDK for Java"
},
"links": [],
"text": "The following example shows how to use an EmrCluster and EmrActivity to create an Amazon EMR 4.x cluster to run a Spark step using the Java SDK:public class dataPipelineEmr4 { public static void main(String[] args) { AWSCredentials credentials = null;\n\tcredentials = new ProfileCredentialsProvider(\"/path/to/AwsCredentials.properties\",\"default\").getCredentials();\n\tDataPipelineClient dp = new DataPipelineClient(credentials);\n\tCreatePipelineRequest createPipeline = new CreatePipelineRequest().withName(\"EMR4SDK\").withUniqueId(\"unique\");\n\tCreatePipelineResult createPipelineResult = dp.createPipeline(createPipeline);\n\tString pipelineId = createPipelineResult.getPipelineId(); PipelineObject emrCluster = new PipelineObject() .withName(\"EmrClusterObj\") .withId(\"EmrClusterObj\") .withFields( new Field().withKey(\"releaseLabel\").withStringValue(\"emr-4.1.0\"), new Field().withKey(\"coreInstanceCount\").withStringValue(\"3\"), new Field().withKey(\"applications\").withStringValue(\"spark\"), new Field().withKey(\"applications\").withStringValue(\"Presto-Sandbox\"), new Field().withKey(\"type\").withStringValue(\"EmrCluster\"), new Field().withKey(\"keyPair\").withStringValue(\"myKeyName\"), new Field().withKey(\"masterInstanceType\").withStringValue(\"m3.xlarge\"), new Field().withKey(\"coreInstanceType\").withStringValue(\"m3.xlarge\") ); PipelineObject emrActivity = new PipelineObject() .withName(\"EmrActivityObj\") .withId(\"EmrActivityObj\") .withFields( new Field().withKey(\"step\").withStringValue(\"command-runner.jar,spark-submit,--executor-memory,1g,--class,org.apache.spark.examples.SparkPi,/usr/lib/spark/lib/spark-examples.jar,10\"), new Field().withKey(\"runsOn\").withRefValue(\"EmrClusterObj\"), new Field().withKey(\"type\").withStringValue(\"EmrActivity\") ); PipelineObject schedule = new PipelineObject() .withName(\"Every 15 Minutes\") .withId(\"DefaultSchedule\") .withFields( new Field().withKey(\"type\").withStringValue(\"Schedule\"), new Field().withKey(\"period\").withStringValue(\"15 Minutes\"), new Field().withKey(\"startAt\").withStringValue(\"FIRST_ACTIVATION_DATE_TIME\") ); PipelineObject defaultObject = new PipelineObject() .withName(\"Default\") .withId(\"Default\") .withFields( new Field().withKey(\"failureAndRerunMode\").withStringValue(\"CASCADE\"), new Field().withKey(\"schedule\").withRefValue(\"DefaultSchedule\"), new Field().withKey(\"resourceRole\").withStringValue(\"DataPipelineDefaultResourceRole\"), new Field().withKey(\"role\").withStringValue(\"DataPipelineDefaultRole\"), new Field().withKey(\"pipelineLogUri\").withStringValue(\"s3://myLogUri\"), new Field().withKey(\"scheduleType\").withStringValue(\"cron\") ); List<PipelineObject> pipelineObjects = new ArrayList<PipelineObject>(); pipelineObjects.add(emrActivity);\n\tpipelineObjects.add(emrCluster);\n\tpipelineObjects.add(defaultObject);\n\tpipelineObjects.add(schedule); PutPipelineDefinitionRequest putPipelineDefintion = new PutPipelineDefinitionRequest() .withPipelineId(pipelineId) .withPipelineObjects(pipelineObjects); PutPipelineDefinitionResult putPipelineResult = dp.putPipelineDefinition(putPipelineDefintion);\n\tSystem.out.println(putPipelineResult); ActivatePipelineRequest activatePipelineReq = new ActivatePipelineRequest() .withPipelineId(pipelineId);\n\tActivatePipelineResult activatePipelineRes = dp.activatePipeline(activatePipelineReq); System.out.println(activatePipelineRes); System.out.println(pipelineId); }\n\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html#emrcluster-example-java",
"main_header": "Use EmrCluster Resource in AWS SDK for Java",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-java",
"text": "Use EmrCluster Resource in AWS SDK for Java"
},
"links": [],
"text": "The following example shows how to use an EmrCluster and EmrActivity to create an Amazon EMR 4.x cluster to run a Spark step using the Java SDK:public class dataPipelineEmr4 { public static void main(String[] args) { AWSCredentials credentials = null;\n\tcredentials = new ProfileCredentialsProvider(\"/path/to/AwsCredentials.properties\",\"default\").getCredentials();\n\tDataPipelineClient dp = new DataPipelineClient(credentials);\n\tCreatePipelineRequest createPipeline = new CreatePipelineRequest().withName(\"EMR4SDK\").withUniqueId(\"unique\");\n\tCreatePipelineResult createPipelineResult = dp.createPipeline(createPipeline);\n\tString pipelineId = createPipelineResult.getPipelineId(); PipelineObject emrCluster = new PipelineObject() .withName(\"EmrClusterObj\") .withId(\"EmrClusterObj\") .withFields( new Field().withKey(\"releaseLabel\").withStringValue(\"emr-4.1.0\"), new Field().withKey(\"coreInstanceCount\").withStringValue(\"3\"), new Field().withKey(\"applications\").withStringValue(\"spark\"), new Field().withKey(\"applications\").withStringValue(\"Presto-Sandbox\"), new Field().withKey(\"type\").withStringValue(\"EmrCluster\"), new Field().withKey(\"keyPair\").withStringValue(\"myKeyName\"), new Field().withKey(\"masterInstanceType\").withStringValue(\"m3.xlarge\"), new Field().withKey(\"coreInstanceType\").withStringValue(\"m3.xlarge\") ); PipelineObject emrActivity = new PipelineObject() .withName(\"EmrActivityObj\") .withId(\"EmrActivityObj\") .withFields( new Field().withKey(\"step\").withStringValue(\"command-runner.jar,spark-submit,--executor-memory,1g,--class,org.apache.spark.examples.SparkPi,/usr/lib/spark/lib/spark-examples.jar,10\"), new Field().withKey(\"runsOn\").withRefValue(\"EmrClusterObj\"), new Field().withKey(\"type\").withStringValue(\"EmrActivity\") ); PipelineObject schedule = new PipelineObject() .withName(\"Every 15 Minutes\") .withId(\"DefaultSchedule\") .withFields( new Field().withKey(\"type\").withStringValue(\"Schedule\"), new Field().withKey(\"period\").withStringValue(\"15 Minutes\"), new Field().withKey(\"startAt\").withStringValue(\"FIRST_ACTIVATION_DATE_TIME\") ); PipelineObject defaultObject = new PipelineObject() .withName(\"Default\") .withId(\"Default\") .withFields( new Field().withKey(\"failureAndRerunMode\").withStringValue(\"CASCADE\"), new Field().withKey(\"schedule\").withRefValue(\"DefaultSchedule\"), new Field().withKey(\"resourceRole\").withStringValue(\"DataPipelineDefaultResourceRole\"), new Field().withKey(\"role\").withStringValue(\"DataPipelineDefaultRole\"), new Field().withKey(\"pipelineLogUri\").withStringValue(\"s3://myLogUri\"), new Field().withKey(\"scheduleType\").withStringValue(\"cron\") ); List<PipelineObject> pipelineObjects = new ArrayList<PipelineObject>(); pipelineObjects.add(emrActivity);\n\tpipelineObjects.add(emrCluster);\n\tpipelineObjects.add(defaultObject);\n\tpipelineObjects.add(schedule); PutPipelineDefinitionRequest putPipelineDefintion = new PutPipelineDefinitionRequest() .withPipelineId(pipelineId) .withPipelineObjects(pipelineObjects); PutPipelineDefinitionResult putPipelineResult = dp.putPipelineDefinition(putPipelineDefintion);\n\tSystem.out.println(putPipelineResult); ActivatePipelineRequest activatePipelineReq = new ActivatePipelineRequest() .withPipelineId(pipelineId);\n\tActivatePipelineResult activatePipelineRes = dp.activatePipeline(activatePipelineReq); System.out.println(activatePipelineRes); System.out.println(pipelineId); }\n\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html#emrcluster-example-java",
"main_header": "Use EmrCluster Resource in AWS SDK for Java",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-java",
"text": "Use EmrCluster Resource in AWS SDK for Java"
},
"links": [],
"text": "The following example shows how to use an EmrCluster and EmrActivity to create an Amazon EMR 4.x cluster to run a Spark step using the Java SDK:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-java.html#emrcluster-example-java",
"main_header": "Use EmrCluster Resource in AWS SDK for Java",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-private-subnet",
"text": "Configure an Amazon EMR cluster in a private subnet"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html#emrcluster-example-private-subnet",
"main_header": "Configure an Amazon EMR cluster in a private subnet",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-private-subnet",
"text": "Configure an Amazon EMR cluster in a private subnet"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-vpc-launching-job-flows.html",
"title": "Launch Amazon EMR Clusters into a VPC"
}
],
"text": "This example includes a configuration that launches the cluster into a private subnet in a VPC. For more information, see Launch Amazon EMR Clusters into a VPC in the Amazon EMR Management Guide. This configuration is optional. You can use it in any pipeline that uses an EmrCluster object.To launch an Amazon EMR cluster in a private subnet, specify SubnetId, emrManagedMasterSecurityGroupId, emrManagedSlaveSecurityGroupId, and serviceAccessSecurityGroupId in your EmrCluster configuration.{ \"objects\": [ { \"output\": { \"ref\": \"S3BackupLocation\" }, \"input\": { \"ref\": \"DDBSourceTable\" }, \"maximumRetries\": \"2\", \"name\": \"TableBackupActivity\", \"step\": \"s3://dynamodb-emr-#{myDDBRegion}/emr-ddb-storage-handler/2.1.0/emr-ddb-2.1.0.jar,org.apache.hadoop.dynamodb.tools.DynamoDbExport,#{output.directoryPath},#{input.tableName},#{input.readThroughputPercent}\", \"id\": \"TableBackupActivity\", \"runsOn\": { \"ref\": \"EmrClusterForBackup\" }, \"type\": \"EmrActivity\", \"resizeClusterBeforeRunning\": \"false\" }, { \"readThroughputPercent\": \"#{myDDBReadThroughputRatio}\", \"name\": \"DDBSourceTable\", \"id\": \"DDBSourceTable\", \"type\": \"DynamoDBDataNode\", \"tableName\": \"#{myDDBTableName}\" }, { \"directoryPath\": \"#{myOutputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}\", \"name\": \"S3BackupLocation\", \"id\": \"S3BackupLocation\", \"type\": \"S3DataNode\" }, { \"name\": \"EmrClusterForBackup\", \"coreInstanceCount\": \"1\", \"taskInstanceCount\": \"1\", \"taskInstanceType\": \"m4.xlarge\", \"coreInstanceType\": \"m4.xlarge\", \"releaseLabel\": \"emr-4.7.0\", \"masterInstanceType\": \"m4.xlarge\", \"id\": \"EmrClusterForBackup\", \"subnetId\": \"#{mySubnetId}\", \"emrManagedMasterSecurityGroupId\": \"#{myMasterSecurityGroup}\", \"emrManagedSlaveSecurityGroupId\": \"#{mySlaveSecurityGroup}\", \"serviceAccessSecurityGroupId\": \"#{myServiceAccessSecurityGroup}\", \"region\": \"#{myDDBRegion}\", \"type\": \"EmrCluster\", \"keyPair\": \"user-key-pair\" }, { \"failureAndRerunMode\": \"CASCADE\", \"resourceRole\": \"DataPipelineDefaultResourceRole\", \"role\": \"DataPipelineDefaultRole\", \"pipelineLogUri\": \"#{myPipelineLogUri}\", \"scheduleType\": \"ONDEMAND\", \"name\": \"Default\", \"id\": \"Default\" } ], \"parameters\": [ { \"description\": \"Output S3 folder\", \"id\": \"myOutputS3Loc\", \"type\": \"AWS::S3::ObjectKey\" }, { \"description\": \"Source DynamoDB table name\", \"id\": \"myDDBTableName\", \"type\": \"String\" }, { \"default\": \"0.25\", \"watermark\": \"Enter value between 0.1-1.0\", \"description\": \"DynamoDB read throughput ratio\", \"id\": \"myDDBReadThroughputRatio\", \"type\": \"Double\" }, { \"default\": \"us-east-1\", \"watermark\": \"us-east-1\", \"description\": \"Region of the DynamoDB table\", \"id\": \"myDDBRegion\", \"type\": \"String\" } ], \"values\": { \"myDDBRegion\": \"us-east-1\", \"myDDBTableName\": \"ddb_table\", \"myDDBReadThroughputRatio\": \"0.25\", \"myOutputS3Loc\": \"s3://s3_path\", \"mySubnetId\": \"subnet_id\", \"myServiceAccessSecurityGroup\":  \"service access security group\", \"mySlaveSecurityGroup\": \"slave security group\", \"myMasterSecurityGroup\": \"master security group\", \"myPipelineLogUri\": \"s3://s3_path\" }\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html#emrcluster-example-private-subnet",
"main_header": "Configure an Amazon EMR cluster in a private subnet",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-private-subnet",
"text": "Configure an Amazon EMR cluster in a private subnet"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-vpc-launching-job-flows.html",
"title": "Launch Amazon EMR Clusters into a VPC"
}
],
"text": "This example includes a configuration that launches the cluster into a private subnet in a VPC. For more information, see Launch Amazon EMR Clusters into a VPC in the Amazon EMR Management Guide. This configuration is optional. You can use it in any pipeline that uses an EmrCluster object.To launch an Amazon EMR cluster in a private subnet, specify SubnetId, emrManagedMasterSecurityGroupId, emrManagedSlaveSecurityGroupId, and serviceAccessSecurityGroupId in your EmrCluster configuration.{ \"objects\": [ { \"output\": { \"ref\": \"S3BackupLocation\" }, \"input\": { \"ref\": \"DDBSourceTable\" }, \"maximumRetries\": \"2\", \"name\": \"TableBackupActivity\", \"step\": \"s3://dynamodb-emr-#{myDDBRegion}/emr-ddb-storage-handler/2.1.0/emr-ddb-2.1.0.jar,org.apache.hadoop.dynamodb.tools.DynamoDbExport,#{output.directoryPath},#{input.tableName},#{input.readThroughputPercent}\", \"id\": \"TableBackupActivity\", \"runsOn\": { \"ref\": \"EmrClusterForBackup\" }, \"type\": \"EmrActivity\", \"resizeClusterBeforeRunning\": \"false\" }, { \"readThroughputPercent\": \"#{myDDBReadThroughputRatio}\", \"name\": \"DDBSourceTable\", \"id\": \"DDBSourceTable\", \"type\": \"DynamoDBDataNode\", \"tableName\": \"#{myDDBTableName}\" }, { \"directoryPath\": \"#{myOutputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}\", \"name\": \"S3BackupLocation\", \"id\": \"S3BackupLocation\", \"type\": \"S3DataNode\" }, { \"name\": \"EmrClusterForBackup\", \"coreInstanceCount\": \"1\", \"taskInstanceCount\": \"1\", \"taskInstanceType\": \"m4.xlarge\", \"coreInstanceType\": \"m4.xlarge\", \"releaseLabel\": \"emr-4.7.0\", \"masterInstanceType\": \"m4.xlarge\", \"id\": \"EmrClusterForBackup\", \"subnetId\": \"#{mySubnetId}\", \"emrManagedMasterSecurityGroupId\": \"#{myMasterSecurityGroup}\", \"emrManagedSlaveSecurityGroupId\": \"#{mySlaveSecurityGroup}\", \"serviceAccessSecurityGroupId\": \"#{myServiceAccessSecurityGroup}\", \"region\": \"#{myDDBRegion}\", \"type\": \"EmrCluster\", \"keyPair\": \"user-key-pair\" }, { \"failureAndRerunMode\": \"CASCADE\", \"resourceRole\": \"DataPipelineDefaultResourceRole\", \"role\": \"DataPipelineDefaultRole\", \"pipelineLogUri\": \"#{myPipelineLogUri}\", \"scheduleType\": \"ONDEMAND\", \"name\": \"Default\", \"id\": \"Default\" } ], \"parameters\": [ { \"description\": \"Output S3 folder\", \"id\": \"myOutputS3Loc\", \"type\": \"AWS::S3::ObjectKey\" }, { \"description\": \"Source DynamoDB table name\", \"id\": \"myDDBTableName\", \"type\": \"String\" }, { \"default\": \"0.25\", \"watermark\": \"Enter value between 0.1-1.0\", \"description\": \"DynamoDB read throughput ratio\", \"id\": \"myDDBReadThroughputRatio\", \"type\": \"Double\" }, { \"default\": \"us-east-1\", \"watermark\": \"us-east-1\", \"description\": \"Region of the DynamoDB table\", \"id\": \"myDDBRegion\", \"type\": \"String\" } ], \"values\": { \"myDDBRegion\": \"us-east-1\", \"myDDBTableName\": \"ddb_table\", \"myDDBReadThroughputRatio\": \"0.25\", \"myOutputS3Loc\": \"s3://s3_path\", \"mySubnetId\": \"subnet_id\", \"myServiceAccessSecurityGroup\":  \"service access security group\", \"mySlaveSecurityGroup\": \"slave security group\", \"myMasterSecurityGroup\": \"master security group\", \"myPipelineLogUri\": \"s3://s3_path\" }\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html#emrcluster-example-private-subnet",
"main_header": "Configure an Amazon EMR cluster in a private subnet",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-private-subnet",
"text": "Configure an Amazon EMR cluster in a private subnet"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-vpc-launching-job-flows.html",
"title": "Launch Amazon EMR Clusters into a VPC"
}
],
"text": "This example includes a configuration that launches the cluster into a private subnet in a VPC. For more information, see Launch Amazon EMR Clusters into a VPC in the Amazon EMR Management Guide. This configuration is optional. You can use it in any pipeline that uses an EmrCluster object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html#emrcluster-example-private-subnet",
"main_header": "Configure an Amazon EMR cluster in a private subnet",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-private-subnet",
"text": "Configure an Amazon EMR cluster in a private subnet"
},
"links": [],
"text": "To launch an Amazon EMR cluster in a private subnet, specify SubnetId, emrManagedMasterSecurityGroupId, emrManagedSlaveSecurityGroupId, and serviceAccessSecurityGroupId in your EmrCluster configuration.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html#emrcluster-example-private-subnet",
"main_header": "Configure an Amazon EMR cluster in a private subnet",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html",
"title": "Amazon EBS volumes in Amazon EMR"
}
],
"text": "You can attach EBS volumes to any type of node in the EMR cluster within your pipeline. To attach EBS volumes to nodes, use coreEbsConfiguration, masterEbsConfiguration, and TaskEbsConfiguration in your EmrCluster configuration. This example of the Amazon EMR cluster uses Amazon EBS volumes for its master, task, and core nodes. For more information, see Amazon EBS volumes in Amazon EMR in the Amazon EMR Management Guide.These configurations are optional. You can use them in any pipeline that uses an EmrCluster object.In the pipeline, click the EmrCluster object configuration, choose Master EBS Configuration, Core EBS Configuration, or Task EBS Configuration, and enter the configuration details similar to the following example.{ \"objects\": [ { \"output\": { \"ref\": \"S3BackupLocation\" }, \"input\": { \"ref\": \"DDBSourceTable\" }, \"maximumRetries\": \"2\", \"name\": \"TableBackupActivity\", \"step\": \"s3://dynamodb-emr-#{myDDBRegion}/emr-ddb-storage-handler/2.1.0/emr-ddb-2.1.0.jar,org.apache.hadoop.dynamodb.tools.DynamoDbExport,#{output.directoryPath},#{input.tableName},#{input.readThroughputPercent}\", \"id\": \"TableBackupActivity\", \"runsOn\": { \"ref\": \"EmrClusterForBackup\" }, \"type\": \"EmrActivity\", \"resizeClusterBeforeRunning\": \"false\" }, { \"readThroughputPercent\": \"#{myDDBReadThroughputRatio}\", \"name\": \"DDBSourceTable\", \"id\": \"DDBSourceTable\", \"type\": \"DynamoDBDataNode\", \"tableName\": \"#{myDDBTableName}\" }, { \"directoryPath\": \"#{myOutputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}\", \"name\": \"S3BackupLocation\", \"id\": \"S3BackupLocation\", \"type\": \"S3DataNode\" }, { \"name\": \"EmrClusterForBackup\", \"coreInstanceCount\": \"1\", \"taskInstanceCount\": \"1\", \"taskInstanceType\": \"m4.xlarge\", \"coreInstanceType\": \"m4.xlarge\", \"releaseLabel\": \"emr-4.7.0\", \"masterInstanceType\": \"m4.xlarge\", \"id\": \"EmrClusterForBackup\", \"subnetId\": \"#{mySubnetId}\", \"emrManagedMasterSecurityGroupId\": \"#{myMasterSecurityGroup}\", \"emrManagedSlaveSecurityGroupId\": \"#{mySlaveSecurityGroup}\", \"region\": \"#{myDDBRegion}\", \"type\": \"EmrCluster\", \"coreEbsConfiguration\": { \"ref\": \"EBSConfiguration\" }, \"masterEbsConfiguration\": { \"ref\": \"EBSConfiguration\" }, \"taskEbsConfiguration\": { \"ref\": \"EBSConfiguration\" }, \"keyPair\": \"user-key-pair\" }, { \"name\": \"EBSConfiguration\", \"id\": \"EBSConfiguration\", \"ebsOptimized\": \"true\", \"ebsBlockDeviceConfig\" : [ { \"ref\": \"EbsBlockDeviceConfig\" } ], \"type\": \"EbsConfiguration\" }, { \"name\": \"EbsBlockDeviceConfig\", \"id\": \"EbsBlockDeviceConfig\", \"type\": \"EbsBlockDeviceConfig\", \"volumesPerInstance\" : \"2\", \"volumeSpecification\" : { \"ref\": \"VolumeSpecification\" } }, { \"name\": \"VolumeSpecification\", \"id\": \"VolumeSpecification\", \"type\": \"VolumeSpecification\", \"sizeInGB\": \"500\", \"volumeType\": \"io1\", \"iops\": \"1000\" }, { \"failureAndRerunMode\": \"CASCADE\", \"resourceRole\": \"DataPipelineDefaultResourceRole\", \"role\": \"DataPipelineDefaultRole\", \"pipelineLogUri\": \"#{myPipelineLogUri}\", \"scheduleType\": \"ONDEMAND\", \"name\": \"Default\", \"id\": \"Default\" } ], \"parameters\": [ { \"description\": \"Output S3 folder\", \"id\": \"myOutputS3Loc\", \"type\": \"AWS::S3::ObjectKey\" }, { \"description\": \"Source DynamoDB table name\", \"id\": \"myDDBTableName\", \"type\": \"String\" }, { \"default\": \"0.25\", \"watermark\": \"Enter value between 0.1-1.0\", \"description\": \"DynamoDB read throughput ratio\", \"id\": \"myDDBReadThroughputRatio\", \"type\": \"Double\" }, { \"default\": \"us-east-1\", \"watermark\": \"us-east-1\", \"description\": \"Region of the DynamoDB table\", \"id\": \"myDDBRegion\", \"type\": \"String\" } ], \"values\": { \"myDDBRegion\": \"us-east-1\", \"myDDBTableName\": \"ddb_table\", \"myDDBReadThroughputRatio\": \"0.25\", \"myOutputS3Loc\": \"s3://s3_path\", \"mySubnetId\": \"subnet_id\", \"mySlaveSecurityGroup\": \"slave security group\", \"myMasterSecurityGroup\": \"master security group\", \"myPipelineLogUri\": \"s3://s3_path\" }\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html",
"title": "Amazon EBS volumes in Amazon EMR"
}
],
"text": "You can attach EBS volumes to any type of node in the EMR cluster within your pipeline. To attach EBS volumes to nodes, use coreEbsConfiguration, masterEbsConfiguration, and TaskEbsConfiguration in your EmrCluster configuration. This example of the Amazon EMR cluster uses Amazon EBS volumes for its master, task, and core nodes. For more information, see Amazon EBS volumes in Amazon EMR in the Amazon EMR Management Guide.These configurations are optional. You can use them in any pipeline that uses an EmrCluster object.In the pipeline, click the EmrCluster object configuration, choose Master EBS Configuration, Core EBS Configuration, or Task EBS Configuration, and enter the configuration details similar to the following example.{ \"objects\": [ { \"output\": { \"ref\": \"S3BackupLocation\" }, \"input\": { \"ref\": \"DDBSourceTable\" }, \"maximumRetries\": \"2\", \"name\": \"TableBackupActivity\", \"step\": \"s3://dynamodb-emr-#{myDDBRegion}/emr-ddb-storage-handler/2.1.0/emr-ddb-2.1.0.jar,org.apache.hadoop.dynamodb.tools.DynamoDbExport,#{output.directoryPath},#{input.tableName},#{input.readThroughputPercent}\", \"id\": \"TableBackupActivity\", \"runsOn\": { \"ref\": \"EmrClusterForBackup\" }, \"type\": \"EmrActivity\", \"resizeClusterBeforeRunning\": \"false\" }, { \"readThroughputPercent\": \"#{myDDBReadThroughputRatio}\", \"name\": \"DDBSourceTable\", \"id\": \"DDBSourceTable\", \"type\": \"DynamoDBDataNode\", \"tableName\": \"#{myDDBTableName}\" }, { \"directoryPath\": \"#{myOutputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}\", \"name\": \"S3BackupLocation\", \"id\": \"S3BackupLocation\", \"type\": \"S3DataNode\" }, { \"name\": \"EmrClusterForBackup\", \"coreInstanceCount\": \"1\", \"taskInstanceCount\": \"1\", \"taskInstanceType\": \"m4.xlarge\", \"coreInstanceType\": \"m4.xlarge\", \"releaseLabel\": \"emr-4.7.0\", \"masterInstanceType\": \"m4.xlarge\", \"id\": \"EmrClusterForBackup\", \"subnetId\": \"#{mySubnetId}\", \"emrManagedMasterSecurityGroupId\": \"#{myMasterSecurityGroup}\", \"emrManagedSlaveSecurityGroupId\": \"#{mySlaveSecurityGroup}\", \"region\": \"#{myDDBRegion}\", \"type\": \"EmrCluster\", \"coreEbsConfiguration\": { \"ref\": \"EBSConfiguration\" }, \"masterEbsConfiguration\": { \"ref\": \"EBSConfiguration\" }, \"taskEbsConfiguration\": { \"ref\": \"EBSConfiguration\" }, \"keyPair\": \"user-key-pair\" }, { \"name\": \"EBSConfiguration\", \"id\": \"EBSConfiguration\", \"ebsOptimized\": \"true\", \"ebsBlockDeviceConfig\" : [ { \"ref\": \"EbsBlockDeviceConfig\" } ], \"type\": \"EbsConfiguration\" }, { \"name\": \"EbsBlockDeviceConfig\", \"id\": \"EbsBlockDeviceConfig\", \"type\": \"EbsBlockDeviceConfig\", \"volumesPerInstance\" : \"2\", \"volumeSpecification\" : { \"ref\": \"VolumeSpecification\" } }, { \"name\": \"VolumeSpecification\", \"id\": \"VolumeSpecification\", \"type\": \"VolumeSpecification\", \"sizeInGB\": \"500\", \"volumeType\": \"io1\", \"iops\": \"1000\" }, { \"failureAndRerunMode\": \"CASCADE\", \"resourceRole\": \"DataPipelineDefaultResourceRole\", \"role\": \"DataPipelineDefaultRole\", \"pipelineLogUri\": \"#{myPipelineLogUri}\", \"scheduleType\": \"ONDEMAND\", \"name\": \"Default\", \"id\": \"Default\" } ], \"parameters\": [ { \"description\": \"Output S3 folder\", \"id\": \"myOutputS3Loc\", \"type\": \"AWS::S3::ObjectKey\" }, { \"description\": \"Source DynamoDB table name\", \"id\": \"myDDBTableName\", \"type\": \"String\" }, { \"default\": \"0.25\", \"watermark\": \"Enter value between 0.1-1.0\", \"description\": \"DynamoDB read throughput ratio\", \"id\": \"myDDBReadThroughputRatio\", \"type\": \"Double\" }, { \"default\": \"us-east-1\", \"watermark\": \"us-east-1\", \"description\": \"Region of the DynamoDB table\", \"id\": \"myDDBRegion\", \"type\": \"String\" } ], \"values\": { \"myDDBRegion\": \"us-east-1\", \"myDDBTableName\": \"ddb_table\", \"myDDBReadThroughputRatio\": \"0.25\", \"myOutputS3Loc\": \"s3://s3_path\", \"mySubnetId\": \"subnet_id\", \"mySlaveSecurityGroup\": \"slave security group\", \"myMasterSecurityGroup\": \"master security group\", \"myPipelineLogUri\": \"s3://s3_path\" }\n}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"pre"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [],
"text": "You can attach EBS volumes to any type of node in the EMR cluster within your pipeline. To attach EBS volumes to nodes, use coreEbsConfiguration, masterEbsConfiguration, and TaskEbsConfiguration in your EmrCluster configuration.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html",
"title": "Amazon EBS volumes in Amazon EMR"
}
],
"text": "This example of the Amazon EMR cluster uses Amazon EBS volumes for its master, task, and core nodes. For more information, see Amazon EBS volumes in Amazon EMR in the Amazon EMR Management Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [],
"text": "These configurations are optional. You can use them in any pipeline that uses an EmrCluster object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example-ebs",
"text": "Attach EBS volumes to cluster nodes"
},
"links": [],
"text": "In the pipeline, click the EmrCluster object configuration, choose Master EBS Configuration, Core EBS Configuration, or Task EBS Configuration, and enter the configuration details similar to the following example.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html#emrcluster-example-ebs",
"main_header": "Attach EBS volumes to cluster nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"title": "Examples"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#dp-object-httpproxy",
"main_header": "HttpProxy",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type hostname\nHost of the proxy which clients will use to connect to AWS Services.\nString port\nPort of the proxy host which the clients will use to connect to AWS Services.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type hostname\nHost of the proxy which clients will use to connect to AWS Services.\nString port\nPort of the proxy host which the clients will use to connect to AWS Services.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} *password\nPassword for proxy.\nString s3NoProxy\nDisable the HTTP proxy when connecting to Amazon S3\nBoolean username\nUser name for proxy.\nString windowsDomain\nThe Windows domain name for NTLM Proxy.\nString windowsWorkgroup\nThe Windows workgroup name for NTLM Proxy.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} *password\nPassword for proxy.\nString s3NoProxy\nDisable the HTTP proxy when connecting to Amazon S3\nBoolean username\nUser name for proxy.\nString windowsDomain\nThe Windows domain name for NTLM Proxy.\nString windowsWorkgroup\nThe Windows workgroup name for NTLM Proxy.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-preconditions",
"text": "Preconditions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html#dp-object-preconditions",
"main_header": "Preconditions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists",
"main_header": "DynamoDBDataExists",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString tableName\nDynamoDB Table to check.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString tableName\nDynamoDB Table to check.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists",
"main_header": "DynamoDBTableExists",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString tableName\nDynamoDB Table to check.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString tableName\nDynamoDB Table to check.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#dp-object-exists",
"main_header": "Exists",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"title": "Preconditions"
}
],
"text": "NoteWe recommend that you use system-managed preconditions instead. For more information, see Preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#dp-object-exists",
"main_header": "Exists",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#dp-object-exists",
"main_header": "Exists",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"title": "Preconditions"
}
],
"text": "We recommend that you use system-managed preconditions instead. For more information, see Preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#dp-object-exists",
"main_header": "Exists",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"title": "Preconditions"
}
],
"text": "We recommend that you use system-managed preconditions instead. For more information, see Preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#dp-object-exists",
"main_header": "Exists",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString node\nThe node for which this precondition is being performed.\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString node\nThe node for which this precondition is being performed.\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"title": "ShellCommandPrecondition"
}
],
"text": "ShellCommandPrecondition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#dp-object-S3KeyExists",
"main_header": "S3KeyExists",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#dp-object-S3KeyExists-example",
"text": "Example"
},
"links": [],
"text": "Write a file to Amazon S3 at the end of the first pipeline's completion.\n\nCreate an S3KeyExists precondition on the second pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#dp-object-S3KeyExists-example",
"main_header": "Example",
"images": [],
"container_type": "div",
"children_tags": [
"ol"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString s3Key\nThe Amazon S3 key.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString s3Key\nThe Amazon S3 key.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout before attempting to complete remote work one more time. If set, then a remote activity that does not complete within the set time after starting is attempted again.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number of attempts that are initiated on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied.\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and are retried.\nPeriod retryDelay\nThe timeout duration between two successive attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout before attempting to complete remote work one more time. If set, then a remote activity that does not complete within the set time after starting is attempted again.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun.\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number of attempts that are initiated on failure.\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied.\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and are retried.\nPeriod retryDelay\nThe timeout duration between two successive attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"title": "ShellCommandPrecondition"
}
],
"text": "ShellCommandPrecondition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#dp-object-s3prefixnotempty",
"main_header": "S3PrefixNotEmpty",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString s3Prefix\nThe Amazon S3 prefix to check for existence of objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type role\nSpecifies the role to be used to execute the precondition.\nString s3Prefix\nThe Amazon S3 prefix to check for existence of objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed.\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} currentRetryCount\nNumber of times the precondition was tried in this attempt.\nString emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString lastRetryTime\nLast time when the precondition was tried within this attempt.\nString node\nThe node for which this precondition is being performed.\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object.\nDateTime @scheduledStartTime\nSchedule start time for object.\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"title": "ShellCommandPrecondition"
}
],
"text": "ShellCommandPrecondition",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#dp-object-shellcommandprecondition",
"main_header": "ShellCommandPrecondition",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type command\nThe command to run. This value and any associated parameters must function in the environment from which you are running the Task Runner.\nString scriptUri\nAn Amazon S3 URI path for a file to download and run as a shell command. Only one scriptUri or command field should be present. scriptUri cannot use parameters, use command instead.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type command\nThe command to run. This value and any associated parameters must function in the environment from which you are running the Task Runner.\nString scriptUri\nAn Amazon S3 URI path for a file to download and run as a shell command. Only one scriptUri or command field should be present. scriptUri cannot use parameters, use command instead.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scriptArgument\nArgument to be passed to shell script\nString stderr\nThe Amazon S3 path that receives redirected system error messages from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString stdout\nThe Amazon S3 path that receives redirected output from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type attemptStatus\nMost recently reported status from the remote activity.\nString attemptTimeout\nTimeout for remote work completion. If set then a remote activity that does not complete within the set time of starting may be retried.\nPeriod failureAndRerunMode\nDescribes consumer node behavior when dependencies fail or are rerun\nEnumeration lateAfterTimeout\nThe elapsed time after pipeline start within which the object must complete. It is triggered only when the schedule type is not set to ondemand.\nPeriod maximumRetries\nMaximum number attempt retries on failure\nInteger onFail\nAn action to run when current object fails.\nReference Object, e.g. \"onFail\":{\"ref\":\"myActionId\"} onLateAction\nActions that should be triggered if an object has not yet been scheduled or still not completed.\nReference Object, e.g. \"onLateAction\":{\"ref\":\"myActionId\"} onSuccess\nAn action to run when current object succeeds.\nReference Object, e.g. \"onSuccess\":{\"ref\":\"myActionId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} preconditionTimeout\nThe period from start after which precondition is marked as failed if still not satisfied\nPeriod reportProgressTimeout\nTimeout for remote work successive calls to reportProgress. If set, then remote activities that do not report progress for the specified period may be considered stalled and so retried.\nPeriod retryDelay\nThe timeout duration between two retry attempts.\nPeriod scriptArgument\nArgument to be passed to shell script\nString stderr\nThe Amazon S3 path that receives redirected system error messages from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString stdout\nThe Amazon S3 path that receives redirected output from the command. If you use the runsOn field, this must be an Amazon S3 path because of the transitory nature of the resource running your activity. However, if you specify the workerGroup field, a local file path is permitted.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @activeInstances\nList of the currently scheduled active instance objects.\nReference Object, e.g. \"activeInstances\":{\"ref\":\"myRunnableObjectId\"} @actualEndTime\nTime when the execution of this object finished.\nDateTime @actualStartTime\nTime when the execution of this object started.\nDateTime cancellationReason\nThe cancellationReason if this object was cancelled.\nString @cascadeFailedOn\nDescription of the dependency chain the object failed on.\nReference Object, e.g. \"cascadeFailedOn\":{\"ref\":\"myRunnableObjectId\"} emrStepLog\nEMR step logs available only on EMR activity attempts\nString errorId\nThe errorId if this object failed.\nString errorMessage\nThe errorMessage if this object failed.\nString errorStackTrace\nThe error stack trace if this object failed.\nString hadoopJobLog\nHadoop job logs available on attempts for EMR-based activities.\nString hostname\nThe host name of client that picked up the task attempt.\nString node\nThe node for which this precondition is being performed\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} reportProgressTime\nMost recent time that remote activity reported progress.\nDateTime @scheduledEndTime\nSchedule end time for object\nDateTime @scheduledStartTime\nSchedule start time for object\nDateTime @status\nThe status of this object.\nString @version\nPipeline version the object was created with.\nString @waitingOn\nDescription of list of dependencies this object is waiting on.\nReference Object, e.g. \"waitingOn\":{\"ref\":\"myRunnableObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"title": "ShellCommandActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"title": "Exists"
}
],
"text": "ShellCommandActivity\n\nExists",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-databases",
"text": "Databases"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html#dp-object-databases",
"main_header": "Databases",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#dp-object-jdbcdatabase",
"main_header": "JdbcDatabase",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type connectionString\nThe JDBC connection string to access the database.\nString jdbcDriverClass\nThe driver class to load before establishing the JDBC connection.\nString *password\nThe password to supply.\nString username\nThe user name to supply when connecting to the database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type connectionString\nThe JDBC connection string to access the database.\nString jdbcDriverClass\nThe driver class to load before establishing the JDBC connection.\nString *password\nThe password to supply.\nString username\nThe user name to supply when connecting to the database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type databaseName\nName of the logical database to attach to\nString jdbcDriverJarUri\nThe location in Amazon S3 of the JDBC driver JAR file used to connect to the database. AWS Data Pipeline must have permission to read this JAR file.\nString jdbcProperties\nPairs of the form A=B that will be set as properties on JDBC connections for this database.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type databaseName\nName of the logical database to attach to\nString jdbcDriverJarUri\nThe location in Amazon S3 of the JDBC driver JAR file used to connect to the database. AWS Data Pipeline must have permission to read this JAR file.\nString jdbcProperties\nPairs of the form A=B that will be set as properties on JDBC connections for this database.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#dp-object-rdsdatabase",
"main_header": "RdsDatabase",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"title": "JdbcDatabase"
}
],
"text": "NoteRdsDatabase does not support Aurora. Use JdbcDatabase for Aurora, instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#dp-object-rdsdatabase",
"main_header": "RdsDatabase",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#dp-object-rdsdatabase",
"main_header": "RdsDatabase",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"title": "JdbcDatabase"
}
],
"text": "RdsDatabase does not support Aurora. Use JdbcDatabase for Aurora, instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#dp-object-rdsdatabase",
"main_header": "RdsDatabase",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"title": "JdbcDatabase"
}
],
"text": "RdsDatabase does not support Aurora. Use JdbcDatabase for Aurora, instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#dp-object-rdsdatabase",
"main_header": "RdsDatabase",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type *password\nThe password to supply.\nString rdsInstanceId\nThe DBInstanceIdentifier property of the DB instance.\nString username\nThe user name to supply when connecting to the database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type *password\nThe password to supply.\nString rdsInstanceId\nThe DBInstanceIdentifier property of the DB instance.\nString username\nThe user name to supply when connecting to the database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type databaseName\nName of the logical database to attach to\nString jdbcDriverJarUri\nThe location in Amazon S3 of the JDBC driver JAR file used to connect to the database. AWS Data Pipeline must have permission to read this JAR file. For the MySQL and PostgreSQL engines, the default driver is used if this field is not specified, but you can override the default using this field. For the Oracle and SQL Server engines, this field is required.\nString jdbcProperties\nPairs of the form A=B that will be set as properties on JDBC connections for this database.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} region\nThe code for the region where the database exists. For example, us-east-1.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type databaseName\nName of the logical database to attach to\nString jdbcDriverJarUri\nThe location in Amazon S3 of the JDBC driver JAR file used to connect to the database. AWS Data Pipeline must have permission to read this JAR file. For the MySQL and PostgreSQL engines, the default driver is used if this field is not specified, but you can override the default using this field. For the Oracle and SQL Server engines, this field is required.\nString jdbcProperties\nPairs of the form A=B that will be set as properties on JDBC connections for this database.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} region\nThe code for the region where the database exists. For example, us-east-1.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#dp-object-redshiftdatabase",
"main_header": "RedshiftDatabase",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type *password\nThe password to supply.\nString username\nThe user name to supply when connecting to the database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type *password\nThe password to supply.\nString username\nThe user name to supply when connecting to the database.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type clusterId\nThe identifier provided by the user when the Amazon Redshift cluster was created. For example, if the endpoint for your Amazon Redshift cluster is mydb.example.us-east-1.redshift.amazonaws.com, the correct identifier is mydb. In the Amazon Redshift console, you can get this value from Cluster Identifier or Cluster Name.\nString connectionString\nThe JDBC endpoint for connecting to an Amazon Redshift instance owned by an account different than the pipeline. You can't specify both connectionString and clusterId.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type clusterId\nThe identifier provided by the user when the Amazon Redshift cluster was created. For example, if the endpoint for your Amazon Redshift cluster is mydb.example.us-east-1.redshift.amazonaws.com, the correct identifier is mydb. In the Amazon Redshift console, you can get this value from Cluster Identifier or Cluster Name.\nString connectionString\nThe JDBC endpoint for connecting to an Amazon Redshift instance owned by an account different than the pipeline. You can't specify both connectionString and clusterId.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type databaseName\nName of the logical database to attach to.\nString jdbcProperties\nPairs of the form A=B to be set as properties on JDBC connections for this database.\nString parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} region\nThe code for the region where the database exists. For example, us-east-1.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type databaseName\nName of the logical database to attach to.\nString jdbcProperties\nPairs of the form A=B to be set as properties on JDBC connections for this database.\nString parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} region\nThe code for the region where the database exists. For example, us-east-1.\nEnumeration",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-dataformats",
"text": "Data Formats"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html#dp-object-dataformats",
"main_header": "Data Formats",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#dp-object-csv",
"main_header": "CSV Data Format",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING For multiple values, use column names and data types separated by a space.\nString escapeChar\nA character, for example \"\\\", that instructs the parser to ignore the next character.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING For multiple values, use column names and data types separated by a space.\nString escapeChar\nA character, for example \"\\\", that instructs the parser to ignore the next character.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#dp-object-custom",
"main_header": "Custom Data Format",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type columnSeparator\nA character that indicates the end of a column in a data file.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type columnSeparator\nA character that indicates the end of a column in a data file.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING For multiple values, use column names and data types separated by a space.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} recordSeparator\nA character that indicates the end of a row in a data file, for example \"\\n\". Only single characters are supported.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING For multiple values, use column names and data types separated by a space.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} recordSeparator\nA character that indicates the end of a row in a data file, for example \"\\n\". Only single characters are supported.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dp-object-dynamodbdataformat",
"main_header": "DynamoDBDataFormat",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"links": [],
"text": "NoteDynamoDB Boolean types are not mapped to Hive Boolean types. However, it is possible to map DynamoDB integer values of 0 or 1 to Hive Boolean types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dp-object-dynamodbdataformat",
"main_header": "DynamoDBDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dp-object-dynamodbdataformat",
"main_header": "DynamoDBDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"links": [],
"text": "DynamoDB Boolean types are not mapped to Hive Boolean types. However, it is possible to map DynamoDB integer values of 0 or 1 to Hive Boolean types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dp-object-dynamodbdataformat",
"main_header": "DynamoDBDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"links": [],
"text": "DynamoDB Boolean types are not mapped to Hive Boolean types. However, it is possible to map DynamoDB integer values of 0 or 1 to Hive Boolean types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dp-object-dynamodbdataformat",
"main_header": "DynamoDBDataFormat",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nThe column name with data type specified by each field for the data described by this data node. For example, hostname STRING. For multiple values, use column names and data types separated by a space.\nString parent\nThe parent of the current object from which slots will be inherited.\nReference Object, such as  \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nThe column name with data type specified by each field for the data described by this data node. For example, hostname STRING. For multiple values, use column names and data types separated by a space.\nString parent\nThe parent of the current object from which slots will be inherited.\nReference Object, such as  \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nThe pipeline version uses to create the object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nThe pipeline version uses to create the object.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nThe error describing the ill-formed object.\nString @pipelineId\nThe Id of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nThe error describing the ill-formed object.\nString @pipelineId\nThe Id of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "Provides both DynamoDB and Amazon S3 support\n\nAllows you to filter data by certain columns in your Hive query\n\nExports all attributes from DynamoDB even if you have a sparse schema",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "NoteDynamoDB Boolean types are not mapped to Hive Boolean types. However, it is possible to map DynamoDB integer values of 0 or 1 to Hive Boolean types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "DynamoDB Boolean types are not mapped to Hive Boolean types. However, it is possible to map DynamoDB integer values of 0 or 1 to Hive Boolean types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "DynamoDB Boolean types are not mapped to Hive Boolean types. However, it is possible to map DynamoDB integer values of 0 or 1 to Hive Boolean types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#dp-object-regex",
"main_header": "RegEx Data Format",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING For multiple values, use column names and data types separated by a space.\nString inputRegEx\nThe regular expression to parse an S3 input file. inputRegEx provides a way to retrieve columns from relatively unstructured data in a file.\nString outputFormat\nThe column fields retrieved by inputRegEx, but referenced as %1$s %2$s using Java formatter syntax.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name with datatype specified by each field for the data described by this data node. Ex: hostname STRING For multiple values, use column names and data types separated by a space.\nString inputRegEx\nThe regular expression to parse an S3 input file. inputRegEx provides a way to retrieve columns from relatively unstructured data in a file.\nString outputFormat\nThe column fields retrieved by inputRegEx, but referenced as %1$s %2$s using Java formatter syntax.\nString parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#dp-object-tsv",
"main_header": "TSV Data Format",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name and data type for the data described by this data node. For example \"Name STRING\" denotes a column named Name with fields of data type STRING. Separate multiple column name and data type pairs with commas (as shown in the example).\nString columnSeparator\nThe character that separates fields in one column from fields in the next column. Defaults to '\\t'.\nString escapeChar\nA character, for example \"\\\", that instructs the parser to ignore the next character.\nString parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} recordSeparator\nThe character that separates records. Defaults to '\\n'.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type column\nColumn name and data type for the data described by this data node. For example \"Name STRING\" denotes a column named Name with fields of data type STRING. Separate multiple column name and data type pairs with commas (as shown in the example).\nString columnSeparator\nThe character that separates fields in one column from fields in the next column. Defaults to '\\t'.\nString escapeChar\nA character, for example \"\\\", that instructs the parser to ignore the next character.\nString parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} recordSeparator\nThe character that separates records. Defaults to '\\n'.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-actions",
"text": "Actions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html#dp-object-actions",
"main_header": "Actions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#dp-object-snsalarm",
"main_header": "SnsAlarm",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type message\nThe body text of the Amazon SNS notification.\nString role\nThe IAM role to use to create the Amazon SNS alarm.\nString subject\nThe subject line of the Amazon SNS notification message.\nString topicArn\nThe destination Amazon SNS topic ARN for the message.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type message\nThe body text of the Amazon SNS notification.\nString role\nThe IAM role to use to create the Amazon SNS alarm.\nString subject\nThe subject line of the Amazon SNS notification message.\nString topicArn\nThe destination Amazon SNS topic ARN for the message.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type node\nThe node for which this action is being performed.\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type node\nThe node for which this action is being performed.\nReference Object, e.g. \"node\":{\"ref\":\"myRunnableObjectId\"} @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nId of the pipeline to which this object belongs to.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#dp-object-terminate",
"main_header": "Terminate",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots are inherited.\nReference Object, for example \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots are inherited.\nReference Object, for example \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type node\nThe node for which this action is being performed.\nReference Object, for example \"node\":{\"ref\":\"myRunnableObjectId\"} @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type node\nThe node for which this action is being performed.\nReference Object, for example \"node\":{\"ref\":\"myRunnableObjectId\"} @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#dp-object-schedule",
"main_header": "Schedule",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"links": [],
"text": "NoteWhen a schedule's start time is in the past, AWS Data Pipeline backfills your pipeline and begins scheduling runs immediately beginning at the specified start time. For testing/development, use a relatively short interval. Otherwise, AWS Data Pipeline attempts to queue and schedule all runs of your pipeline for that interval. AWS Data Pipeline attempts to prevent accidental backfills if the pipeline component scheduledStartTime is earlier than 1 day ago by blocking pipeline activation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#dp-object-schedule",
"main_header": "Schedule",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#dp-object-schedule",
"main_header": "Schedule",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"links": [],
"text": "When a schedule's start time is in the past, AWS Data Pipeline backfills your pipeline and begins scheduling runs immediately beginning at the specified start time. For testing/development, use a relatively short interval. Otherwise, AWS Data Pipeline attempts to queue and schedule all runs of your pipeline for that interval. AWS Data Pipeline attempts to prevent accidental backfills if the pipeline component scheduledStartTime is earlier than 1 day ago by blocking pipeline activation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#dp-object-schedule",
"main_header": "Schedule",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"links": [],
"text": "When a schedule's start time is in the past, AWS Data Pipeline backfills your pipeline and begins scheduling runs immediately beginning at the specified start time. For testing/development, use a relatively short interval. Otherwise, AWS Data Pipeline attempts to queue and schedule all runs of your pipeline for that interval. AWS Data Pipeline attempts to prevent accidental backfills if the pipeline component scheduledStartTime is earlier than 1 day ago by blocking pipeline activation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#dp-object-schedule",
"main_header": "Schedule",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type period\nHow often the pipeline should run. The format is \"N [minutes|hours|days|weeks|months]\", where N is a number followed by one of the time specifiers. For example, \"15 minutes\", runs the pipeline every 15 minutes. The minimum period is 15 minutes and the maximum period is 3 years.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type period\nHow often the pipeline should run. The format is \"N [minutes|hours|days|weeks|months]\", where N is a number followed by one of the time specifiers. For example, \"15 minutes\", runs the pipeline every 15 minutes. The minimum period is 15 minutes and the maximum period is 3 years.\nPeriod",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type startAt\nThe date and time at which to start the scheduled pipeline runs. Valid value is FIRST_ACTIVATION_DATE_TIME, which is deprecated in favor of creating an on-demand pipeline.\nEnumeration startDateTime\nThe date and time to start the scheduled runs. You must use either startDateTime or startAt but not both.\nDateTime",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Group (One of the following is required)\nDescription\nSlot Type startAt\nThe date and time at which to start the scheduled pipeline runs. Valid value is FIRST_ACTIVATION_DATE_TIME, which is deprecated in favor of creating an on-demand pipeline.\nEnumeration startDateTime\nThe date and time to start the scheduled runs. You must use either startDateTime or startAt but not both.\nDateTime",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type endDateTime\nThe date and time to end the scheduled runs. Must be a date and time later than the value of startDateTime or startAt. The default behavior is to schedule runs until the pipeline is shut down. \nDateTime occurrences\nThe number of times to execute the pipeline after it's activated. You can't use occurrences with endDateTime.\nInteger parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type endDateTime\nThe date and time to end the scheduled runs. Must be a date and time later than the value of startDateTime or startAt. The default behavior is to schedule runs until the pipeline is shut down. \nDateTime occurrences\nThe number of times to execute the pipeline after it's activated. You can't use occurrences with endDateTime.\nInteger parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @firstActivationTime\nThe time of object creation.\nDateTime @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @firstActivationTime\nThe time of object creation.\nDateTime @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-utilities",
"text": "Utilities"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html#dp-object-utilities",
"main_header": "Utilities",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#dp-object-shellscriptconfig",
"main_header": "ShellScriptConfig",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} scriptArgument\nA list of arguments to use with the shell script.\nString scriptUri\nThe script URI in Amazon S3 that should be downloaded and run.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"} scriptArgument\nA list of arguments to use with the shell script.\nString scriptUri\nThe script URI in Amazon S3 that should be downloaded and run.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#dp-object-emrconfiguration",
"main_header": "EmrConfiguration",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type classification\nClassification for the configuration.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type classification\nClassification for the configuration.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type configuration\nSub-configuration for this configuration.\nReference Object, e.g. \"configuration\":{\"ref\":\"myEmrConfigurationId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} property\nConfiguration property.\nReference Object, e.g. \"property\":{\"ref\":\"myPropertyId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type configuration\nSub-configuration for this configuration.\nReference Object, e.g. \"configuration\":{\"ref\":\"myEmrConfigurationId\"} parent\nParent of the current object from which slots will be inherited.\nReference Object, e.g. \"parent\":{\"ref\":\"myBaseObjectId\"} property\nConfiguration property.\nReference Object, e.g. \"property\":{\"ref\":\"myPropertyId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object\nString @pipelineId\nId of the pipeline to which this object belongs to\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects which execute Attempt Objects\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"title": "Property"
},
{
"url": "http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/",
"title": "Amazon EMR Release Guide"
}
],
"text": "EmrCluster\n\nProperty\n\nAmazon EMR Release Guide",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#dp-object-property",
"main_header": "Property",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type key\nkey\nString value\nvalue\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "Required Fields\nDescription\nSlot Type key\nkey\nString value\nvalue\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "Optional Fields\nDescription\nSlot Type parent\nParent of the current object from which slots are inherited.\nReference Object, for example, \"parent\":{\"ref\":\"myBaseObjectId\"}",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "Runtime Fields\nDescription\nSlot Type @version\nPipeline version that the object was created with.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "System Fields\nDescription\nSlot Type @error\nError describing the ill-formed object.\nString @pipelineId\nID of the pipeline to which this object belongs.\nString @sphere\nThe sphere of an object denotes its place in the lifecycle: Component Objects give rise to Instance Objects, which execute Attempt Objects.\nString",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-seealso",
"text": "See Also"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"title": "EmrConfiguration"
},
{
"url": "http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/",
"title": "Amazon EMR Release Guide"
}
],
"text": "EmrCluster\n\nEmrConfiguration\n\nAmazon EMR Release Guide",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-seealso",
"main_header": "See Also",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-using-task-runner",
"text": "Working with Task Runner"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html#dp-using-task-runner",
"main_header": "Working with Task Runner",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-using-task-runner",
"text": "Working with Task Runner"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"title": "Executing Work on Existing Resources Using Task Runner"
}
],
"text": "Allow AWS Data Pipeline to install and manage one or more Task Runner applications for you. When a pipeline is activated, the default Ec2Instance or EmrCluster object referenced by an activity runsOn field is automatically created. AWS Data Pipeline takes care of installing Task Runner on an EC2 instance or on the master node of an EMR cluster. In this pattern, AWS Data Pipeline can do most of the instance or cluster management for you. Run all or parts of a pipeline on resources that you manage. The potential resources include a long-running Amazon EC2 instance, an Amazon EMR cluster, or a physical server. You can install a task runner (which can be either Task Runner or a custom task agent of your own devise) almost anywhere, provided that it can communicate with the AWS Data Pipeline web service. In this pattern, you assume almost complete control over which resources are used and how they are managed, and you must manually install and configure Task Runner. To do so, use the procedures in this section, as described in Executing Work on Existing Resources Using Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html#dp-using-task-runner",
"main_header": "Working with Task Runner",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-dp-managed",
"text": "Task Runner on AWS Data Pipeline-Managed Resources"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html#dp-how-task-runner-dp-managed",
"main_header": "Task Runner on AWS Data Pipeline-Managed Resources",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-dp-managed",
"text": "Task Runner on AWS Data Pipeline-Managed Resources"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html#dp-how-task-runner-dp-managed",
"main_header": "Task Runner on AWS Data Pipeline-Managed Resources",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-task-runner-managed-emr-jobflow.png",
"alt": "\n                    Task runner life cycle on an AWS Data Pipeline-managed resource\n                "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "Note You can only install Task Runner on Linux, UNIX, or macOS. Task Runner is not supported on the Windows operating system.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "You can only install Task Runner on Linux, UNIX, or macOS. Task Runner is not supported on the Windows operating system.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "You can only install Task Runner on Linux, UNIX, or macOS. Task Runner is not supported on the Windows operating system.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [
{
"src": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/images/dp-task-runner-user-emr-jobflow.png",
"alt": "\n                    \n                "
}
],
"container_type": "div",
"children_tags": [
"img"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-installing-taskrunner",
"text": "Installing Task Runner"
},
"links": [
{
"url": "http://www.oracle.com/technetwork/java/index.html",
"title": "http://www.oracle.com/technetwork/java/index.html"
},
{
"url": "https://s3.amazonaws.com/datapipeline-us-east-1/us-east-1/software/latest/TaskRunner/TaskRunner-1.0.jar",
"title": "https://s3.amazonaws.com/datapipeline-us-east-1/us-east-1/software/latest/TaskRunner/TaskRunner-1.0.jar"
},
{
"url": "https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html",
"title": "Understanding and getting your security credentials"
}
],
"text": "To install Task Runner\nTask Runner requires Java versions 1.6 or 1.8. To determine whether Java is installed, and the version that is running, use the following command:\njava -version\n If you do not have Java 1.6 or 1.8 installed on your computer, download one of these versions from http://www.oracle.com/technetwork/java/index.html. Download and install Java, and then proceed to the next step.\n\nDownload TaskRunner-1.0.jar from https://s3.amazonaws.com/datapipeline-us-east-1/us-east-1/software/latest/TaskRunner/TaskRunner-1.0.jar and then copy it into a folder on the target computing resource. For Amazon EMR clusters running EmrActivity tasks, install Task Runner on the master node of the cluster. Task Runner needs to connect to the AWS Data Pipeline web service to process your commands. In this step, you configure Task Runner with an AWS account that has permissions to create or manage data pipelines. \nCreate a JSON file named credentials.json (you can use a different name if you prefer), which specifies an access key ID and secret access key using the format { \"access-id\": \"MyAccessKeyID\", \"private-key\": \"MySecretAccessKey\" } . Copy the file to the directory where you installed Task Runner. \nFor CLI access, you need an access key ID and secret access key. Use IAM user access keys instead of AWS account root user access keys. IAM lets you securely control access to AWS services and resources in your AWS account. For more information about creating access keys, see Understanding and getting your security credentials in the AWS General Reference.\n\nTask Runner connects to the AWS Data Pipeline web service using HTTPS. If you are using an AWS resource, ensure that HTTPS is enabled in the appropriate routing table and subnet ACL. If you are using a firewall or proxy, ensure that port 443 is open.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-installing-taskrunner",
"main_header": "Installing Task Runner",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-installing-taskrunner",
"text": "Installing Task Runner"
},
"links": [],
"text": "To install Task Runner",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-installing-taskrunner",
"main_header": "Installing Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-rdssecurity",
"text": "(Optional) Granting Task Runner Access to Amazon RDS"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html#dp-taskrunner-rdssecurity",
"main_header": "(Optional) Granting Task Runner Access to Amazon RDS",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-rdssecurity",
"text": "(Optional) Granting Task Runner Access to Amazon RDS"
},
"links": [],
"text": "To grant access to Task Runner in EC2-Classic\nOpen the Amazon RDS console.\n\nIn the navigation pane, choose Instances, and then select your DB instance.\n\nUnder Security and Network, select the security group, which opens the Security Groups page with this DB security group selected. Select the details icon for the DB security group.\n\nUnder Security Group Details, create a rule with the appropriate Connection Type and Details. These fields depend on where Task Runner is running, as described here: Ec2Resource Connection Type: EC2 Security Group\nDetails: my-security-group-name (the name of the security group you created for the EC2 instance) EmrResource Connection Type: EC2 Security Group\nDetails: ElasticMapReduce-master\n\nConnection Type: EC2 Security Group\nDetails: ElasticMapReduce-slave Your local environment (on-premises) Connection Type: CIDR/IP:\nDetails: my-ip-address (the IP address of your computer or the IP address range of your network, if your computer is behind a firewall) Click Add.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html#dp-taskrunner-rdssecurity",
"main_header": "(Optional) Granting Task Runner Access to Amazon RDS",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-rdssecurity",
"text": "(Optional) Granting Task Runner Access to Amazon RDS"
},
"links": [],
"text": "To grant access to Task Runner in EC2-Classic",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html#dp-taskrunner-rdssecurity",
"main_header": "(Optional) Granting Task Runner Access to Amazon RDS",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-rdssecurity",
"text": "(Optional) Granting Task Runner Access to Amazon RDS"
},
"links": [],
"text": "To grant access to Task Runner in EC2-VPC\nOpen the Amazon RDS console.\n\nIn the navigation pane, choose Instances.\n\nSelect the details icon for the DB instance. Under Security and Network, open the link to the security group, which takes you to the Amazon EC2 console. If you're using the old console design for security groups, switch to the new console design by selecting the icon that's displayed at the top of the console page.\n\nOn the Inbound tab, choose Edit, Add Rule. Specify the database port that you used when you launched the DB instance. The source depends on where Task Runner is running, as described here: Ec2Resource my-security-group-id (the ID of the security group you created for the EC2 instance) EmrResource master-security-group-id (the ID of the ElasticMapReduce-master security group)\n\nslave-security-group-id (the ID of the ElasticMapReduce-slave security group) Your local environment (on-premises) ip-address (the IP address of your computer or the IP address range of your network, if your computer is behind a firewall) Click Save.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html#dp-taskrunner-rdssecurity",
"main_header": "(Optional) Granting Task Runner Access to Amazon RDS",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-rdssecurity",
"text": "(Optional) Granting Task Runner Access to Amazon RDS"
},
"links": [],
"text": "To grant access to Task Runner in EC2-VPC",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html#dp-taskrunner-rdssecurity",
"main_header": "(Optional) Granting Task Runner Access to Amazon RDS",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-threading",
"text": "Task Runner Threads and Preconditions"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html#dp-taskrunner-threading",
"main_header": "Task Runner Threads and Preconditions",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-config-options",
"text": "Task Runner Configuration Options"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html#dp-taskrunner-config-options",
"main_header": "Task Runner Configuration Options",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-config-options",
"text": "Task Runner Configuration Options"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html#datapipeline_region",
"title": "AWS Data Pipeline Regions and Endpoints"
}
],
"text": "Command Line Parameter\nDescription --help\nCommand line help. Example: Java -jar TaskRunner-1.0.jar --help --config\nThe path and file name of your credentials.json file. --accessId\nYour AWS access key ID for Task Runner to use when making requests. The --accessID and --secretKey options provide an alternative to using a credentials.json file. If a credentials.json file is also provided, the --accessID and --secretKey options take precedence. --secretKey\nYour AWS secret key for Task Runner to use when making requests. For more information, see --accessID. --endpoint\nAn endpoint is a URL that is the entry point for a web service. The AWS Data Pipeline service endpoint in the region where you are making requests. Optional. In general, it is sufficient to specify a region, and you do not need to set the endpoint. For a listing of AWS Data Pipeline regions and endpoints, see AWS Data Pipeline Regions and Endpoints in the AWS General Reference. --workerGroup\nThe name of the worker group for which Task Runner retrieves work. Required.When Task Runner polls the web service, it uses the credentials you supplied and the value of workerGroup to select which (if any) tasks to retrieve. You can use any name that is meaningful to you; the only requirement is that the string must match between the Task Runner and its corresponding pipeline activities. The worker group name is bound to a region. Even if there are identical worker group names in other regions, Task Runner always get tasks from the region specified in --region. --taskrunnerId\nThe ID of the task runner to use when reporting progress. Optional. --output\nThe Task Runner directory for log output files. Optional. Log files are stored in a local directory until they are pushed to Amazon S3. This option overrides the default directory. --region\nThe region to use. Optional, but it is recommended to always set the region. If you do not specify the region, Task Runner retrieves tasks from the default service region, us-east-1.Other supported regions are: eu-west-1, ap-northeast-1, ap-southeast-2, us-west-2. --logUri\nThe Amazon S3 destination path for Task Runner to back up log files to every hour. When Task Runner terminates, active logs in the local directory are pushed to the Amazon S3 destination folder. --proxyHost\nThe host of the proxy used by Task Runner clients to connect to AWS services. --proxyPort\nPort of the proxy host used by Task Runner clients to connect to AWS services. --proxyUsername\nThe user name for proxy. --proxyPassword\nThe password for proxy. --proxyDomain\nThe Windows domain name for NTLM Proxy. --proxyWorkstation\nThe Windows workstation name for NTLM Proxy.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html#dp-taskrunner-config-options",
"main_header": "Task Runner Configuration Options",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-config-options",
"text": "Task Runner Configuration Options"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html#datapipeline_region",
"title": "AWS Data Pipeline Regions and Endpoints"
}
],
"text": "Command Line Parameter\nDescription --help\nCommand line help. Example: Java -jar TaskRunner-1.0.jar --help --config\nThe path and file name of your credentials.json file. --accessId\nYour AWS access key ID for Task Runner to use when making requests. The --accessID and --secretKey options provide an alternative to using a credentials.json file. If a credentials.json file is also provided, the --accessID and --secretKey options take precedence. --secretKey\nYour AWS secret key for Task Runner to use when making requests. For more information, see --accessID. --endpoint\nAn endpoint is a URL that is the entry point for a web service. The AWS Data Pipeline service endpoint in the region where you are making requests. Optional. In general, it is sufficient to specify a region, and you do not need to set the endpoint. For a listing of AWS Data Pipeline regions and endpoints, see AWS Data Pipeline Regions and Endpoints in the AWS General Reference. --workerGroup\nThe name of the worker group for which Task Runner retrieves work. Required.When Task Runner polls the web service, it uses the credentials you supplied and the value of workerGroup to select which (if any) tasks to retrieve. You can use any name that is meaningful to you; the only requirement is that the string must match between the Task Runner and its corresponding pipeline activities. The worker group name is bound to a region. Even if there are identical worker group names in other regions, Task Runner always get tasks from the region specified in --region. --taskrunnerId\nThe ID of the task runner to use when reporting progress. Optional. --output\nThe Task Runner directory for log output files. Optional. Log files are stored in a local directory until they are pushed to Amazon S3. This option overrides the default directory. --region\nThe region to use. Optional, but it is recommended to always set the region. If you do not specify the region, Task Runner retrieves tasks from the default service region, us-east-1.Other supported regions are: eu-west-1, ap-northeast-1, ap-southeast-2, us-west-2. --logUri\nThe Amazon S3 destination path for Task Runner to back up log files to every hour. When Task Runner terminates, active logs in the local directory are pushed to the Amazon S3 destination folder. --proxyHost\nThe host of the proxy used by Task Runner clients to connect to AWS services. --proxyPort\nPort of the proxy host used by Task Runner clients to connect to AWS services. --proxyUsername\nThe user name for proxy. --proxyPassword\nThe password for proxy. --proxyDomain\nThe Windows domain name for NTLM Proxy. --proxyWorkstation\nThe Windows workstation name for NTLM Proxy.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html#dp-taskrunner-config-options",
"main_header": "Task Runner Configuration Options",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-proxy",
"text": "Using Task Runner with a Proxy"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-proxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-proxy.html#dp-taskrunner-proxy",
"main_header": "Using Task Runner with a Proxy",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-ami",
"text": "Task Runner and Custom AMIs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html#dp-custom-ami",
"main_header": "Task Runner and Custom AMIs",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-ami",
"text": "Task Runner and Custom AMIs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami.html",
"title": "Creating Your Own AMI"
},
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html",
"title": "Linux AMI Virtualization Types"
}
],
"text": "Create the AMI in the same region in which the instances will run. For more information, see Creating Your Own AMI in the Amazon EC2 User Guide for Linux Instances.\n\nEnsure that the virtualization type of the AMI is supported by the instance type you plan to use. For example, the I2 and G2 instance types require an HVM AMI and the T1, C1, M1, and M2 instance types require a PV AMI. For more information, see Linux AMI Virtualization Types in the Amazon EC2 User Guide for Linux Instances.\n\nInstall the following software:\n\nLinuxBashwgetunzipJava 1.6 or 1.8cloud-init\n\nCreate and configure a user account named ec2-user.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html#dp-custom-ami",
"main_header": "Task Runner and Custom AMIs",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-troubleshooting",
"text": "Troubleshooting"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html#dp-troubleshooting",
"main_header": "Troubleshooting",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-troubleshoot-locate-errors",
"text": "Locating Errors in Pipelines"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html#dp-troubleshoot-locate-errors",
"main_header": "Locating Errors in Pipelines",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-troubleshoot-locate-errors",
"text": "Locating Errors in Pipelines"
},
"links": [],
"text": "To locate errors about failed or incomplete runs with the console\nOn the List Pipelines page, if the Status column of any of your pipeline instances shows a status other than FINISHED, either your pipeline is waiting for some precondition to be met or it has failed and you need to troubleshoot the pipeline. \nOn the List Pipelines page, locate the instance pipeline and select the triangle to the left of it, to expand the details.\n\nAt the bottom of this panel, choose View execution details; the Instance summary panel opens to show the details of the selected instance.\n\nIn the Instance summary panel, select the triangle next to the instance to see additional details of the instance, and choose Details, More... If the status of your selected instance is FAILED, the details box has entries for the error message, the errorStackTrace and other information. You can save this information into a file. Choose OK.\n\nIn the Instance summary pane, choose Attempts, to see details for each attempt row.\n\nTo take an action on your incomplete or failed instance, select the check box next the instance. This activates the actions. Then, select an action (Rerun|Cancel|Mark Finished).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html#dp-troubleshoot-locate-errors",
"main_header": "Locating Errors in Pipelines",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-troubleshoot-locate-errors",
"text": "Locating Errors in Pipelines"
},
"links": [],
"text": "To locate errors about failed or incomplete runs with the console",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html#dp-troubleshoot-locate-errors",
"main_header": "Locating Errors in Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-troubleshoot-emr",
"text": "Identifying the Amazon EMR Cluster that Serves Your Pipeline"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html#dp-troubleshoot-emr",
"main_header": "Identifying the Amazon EMR Cluster that Serves Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-troubleshoot-emr",
"text": "Identifying the Amazon EMR Cluster that Serves Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/Debugging.html",
"title": "Troubleshooting Tips"
}
],
"text": "To see more detailed Amazon EMR error information\nIn the AWS Data Pipeline console, select the triangle next to the pipeline instance, to expand the instance details. Choose View execution details and select the triangle next to the component.\n\nIn the Details column, choose More.... The information screen opens listing the details of the component. Locate and copy the instanceParent value from the screen, such as: @EmrActivityId_xiFDD_2017-09-30T21:40:13 Navigate to the Amazon EMR console, search for a cluster with the matching instanceParent value in its name, and then choose Debug. \nNoteFor the Debug button to function, your pipeline definition must have set the EmrActivity enableDebugging option to true and the EmrLogUri option to a valid path.\n\nNow that you know which Amazon EMR cluster contains the error that causes your pipeline failure, follow the Troubleshooting Tips in the Amazon EMR Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html#dp-troubleshoot-emr",
"main_header": "Identifying the Amazon EMR Cluster that Serves Your Pipeline",
"images": [],
"container_type": "div",
"children_tags": [
"ol",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-troubleshoot-emr",
"text": "Identifying the Amazon EMR Cluster that Serves Your Pipeline"
},
"links": [],
"text": "To see more detailed Amazon EMR error information",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html#dp-troubleshoot-emr",
"main_header": "Identifying the Amazon EMR Cluster that Serves Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-pipeline-status",
"text": "Interpreting Pipeline Status Details"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html#dp-pipeline-status",
"main_header": "Interpreting Pipeline Status Details",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-pipeline-status",
"text": "Interpreting Pipeline Status Details"
},
"links": [],
"text": "Status Codes\n\nACTIVATING\n\nThe component or resource is being started, such as an EC2 instance.\n\nCANCELED\n\nThe component was canceled by a user or AWS Data Pipeline before it could run. This can happen automatically when a failure occurs in a different component or resource that this component depends on.\n\nCASCADE_FAILED\n\nThe component or resource was canceled as a result of a cascade failure from one of its dependencies, but the component was probably not the original source of the failure.\n\nDEACTIVATING\n\nThe pipeline is being deactivated.\n\nFAILED\n\nThe component or resource encountered an error and stopped working. When a component or resource fails, it can cause cancelations and failures to cascade to other components that depend on it.\n\nFINISHED\n\nThe component completed its assigned work.\n\nINACTIVE\n\nThe pipeline was deactivated.\n\nPAUSED\n\nThe component was paused and is not currently performing its work.\n\nPENDING\n\nThe pipeline is ready to be activated for the first time.\n\nRUNNING\n\nThe resource is running and ready to receive work.\n\nSCHEDULED\n\nThe resource is scheduled to run.\n\nSHUTTING_DOWN\n\nThe resource is shutting down after successfully completing its work.\n\nSKIPPED\n\nThe component skipped intervals of execution after the pipeline was activated using a time stamp that is later than the current schedule.\n\nTIMEDOUT\n\nThe resource exceeded the terminateAfter threshold and was stopped by AWS Data Pipeline. After the resource reaches this status, AWS Data Pipeline ignores the actionOnResourceFailure, retryDelay, and retryTimeout values for that resource. This status applies only to resources.\n\nVALIDATING\n\nThe pipeline definition is being validated by AWS Data Pipeline.\n\nWAITING_FOR_RUNNER\n\nThe component is waiting for its worker client to retrieve a work item. The component and worker client relationship is controlled by the runsOn or workerGroup fields defined by that component.\n\nWAITING_ON_DEPENDENCIES\n\nThe component is verifying that its default and user-configured preconditions are met before performing its work.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html#dp-pipeline-status",
"main_header": "Interpreting Pipeline Status Details",
"images": [],
"container_type": "div",
"children_tags": [
"dl",
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-pipeline-status",
"text": "Interpreting Pipeline Status Details"
},
"links": [],
"text": "Status Codes",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html#dp-pipeline-status",
"main_header": "Interpreting Pipeline Status Details",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-error-logs",
"main_header": "Locating Error Logs",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-verify-task-runner",
"title": "Verifying Task Runner Logging"
}
],
"text": "NoteTask Runner stores its logs in a different location by default, which may be unavailable when the pipeline finishes and the instance that runs Task Runner terminates. For more information, see Verifying Task Runner Logging.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-verify-task-runner",
"title": "Verifying Task Runner Logging"
}
],
"text": "Task Runner stores its logs in a different location by default, which may be unavailable when the pipeline finishes and the instance that runs Task Runner terminates. For more information, see Verifying Task Runner Logging.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-verify-task-runner",
"title": "Verifying Task Runner Logging"
}
],
"text": "Task Runner stores its logs in a different location by default, which may be unavailable when the pipeline finishes and the instance that runs Task Runner terminates. For more information, see Verifying Task Runner Logging.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-check-when-run-fails",
"main_header": "Resolving Common Problems",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-pipeline-doesnt-start",
"title": "Pipeline Stuck in Pending Status"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"title": "Pipeline Component Stuck in Waiting for Runner Status"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-runs-stay-pending",
"title": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-run-doesnt-start-scheduled",
"title": "Run Doesn't Start When Scheduled"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-out-of-order",
"title": "Pipeline Components Run in Wrong Order"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-securitytoken",
"title": "EMR Cluster Fails With Error: The security token included in the request is invalid"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-insufficient-permissions",
"title": "Insufficient Permissions to Access Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-error-code-400",
"title": "Status Code: 400 Error Code: PipelineNotFoundException"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-bad-token",
"title": "Creating a Pipeline Causes a Security Token Error"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-no-details-shown",
"title": "Cannot See Pipeline Details in the Console"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-error-code-s3-404",
"title": "Error in remote runner Status Code: 404, AWS Service: Amazon S3"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"title": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#AMIs-false-data-large-CSV",
"title": "Older Amazon EMR AMIs May Create False Data for Large CSV Files"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-increase-limits",
"title": "Increasing AWS Data Pipeline Limits"
}
],
"text": "ContentsPipeline Stuck in Pending StatusPipeline Component Stuck in Waiting for Runner StatusPipeline Component Stuck in WAITING_ON_DEPENDENCIES StatusRun Doesn't Start When ScheduledPipeline Components Run in Wrong OrderEMR Cluster Fails With Error: The security token included in the request is invalidInsufficient Permissions to Access ResourcesStatus Code: 400 Error Code: PipelineNotFoundExceptionCreating a Pipeline Causes a Security Token ErrorCannot See Pipeline Details in the ConsoleError in remote runner Status Code: 404, AWS Service: Amazon S3Access Denied - Not Authorized to Perform Function datapipeline:Older Amazon EMR AMIs May Create False Data for Large CSV FilesIncreasing AWS Data Pipeline Limits",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-check-when-run-fails",
"main_header": "Resolving Common Problems",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-check-when-run-fails",
"main_header": "Resolving Common Problems",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-waiting-for-runner",
"text": "Pipeline Component Stuck in Waiting for Runner Status"
},
"links": [],
"text": "NoteIf you provide a runsOn value and workerGroup exists, workerGroup is ignored.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"main_header": "Pipeline Component Stuck in Waiting for Runner Status",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-waiting-for-runner",
"text": "Pipeline Component Stuck in Waiting for Runner Status"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"main_header": "Pipeline Component Stuck in Waiting for Runner Status",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-waiting-for-runner",
"text": "Pipeline Component Stuck in Waiting for Runner Status"
},
"links": [],
"text": "If you provide a runsOn value and workerGroup exists, workerGroup is ignored.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"main_header": "Pipeline Component Stuck in Waiting for Runner Status",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-waiting-for-runner",
"text": "Pipeline Component Stuck in Waiting for Runner Status"
},
"links": [],
"text": "If you provide a runsOn value and workerGroup exists, workerGroup is ignored.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"main_header": "Pipeline Component Stuck in Waiting for Runner Status",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-error-code-s3-404",
"text": "Error in remote runner Status Code: 404, AWS Service: Amazon S3"
},
"links": [],
"text": "You have credentials correctly set The Amazon S3 bucket that you are trying to access exists You are authorized to access the Amazon S3 bucket",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-error-code-s3-404",
"main_header": "Error in remote runner Status Code: 404, AWS Service: Amazon S3",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [],
"text": "ERROR Status Code: 403 AWS Service: DataPipeline AWS Error Code: AccessDenied AWS Error Message: User: arn:aws:sts::XXXXXXXXXXXX:federated-user/i-XXXXXXXX is not authorized to perform: datapipeline:PollForTask.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [],
"text": "NoteIn this error message, PollForTask may be replaced with names of other AWS Data Pipeline permissions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [],
"text": "Note",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "div",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [],
"text": "In this error message, PollForTask may be replaced with names of other AWS Data Pipeline permissions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "div",
"children_tags": [
"p"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [],
"text": "In this error message, PollForTask may be replaced with names of other AWS Data Pipeline permissions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits",
"main_header": "AWS Data Pipeline Limits",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-account",
"title": "Account Limits"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"title": "Web Service Call Limits"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-scaling-considerations",
"title": "Scaling Considerations"
}
],
"text": "ContentsAccount LimitsWeb Service Call LimitsScaling Considerations",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits",
"main_header": "AWS Data Pipeline Limits",
"images": [],
"container_type": "div",
"children_tags": [
"p",
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"links": [],
"text": "Contents",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits",
"main_header": "AWS Data Pipeline Limits",
"images": [],
"container_type": "p",
"children_tags": [
"strong"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-account",
"text": "Account Limits"
},
"links": [],
"text": "Attribute\nLimit\nAdjustable Number of pipelines\n100\nYes Number of objects per pipeline\n100\nYes Number of active instances per object\n5\nYes Number of fields per object\n50\nNo Number of UTF8 bytes per field name or identifier\n256\nNo Number of UTF8 bytes per field\n10,240\nNo Number of UTF8 bytes per object\n15,360 (including field names)\nNo Rate of creation of an instance from an object\n1 per 5 minutes\nNo Retries of a pipeline activity\n5 per task\nNo Minimum delay between retry attempts\n2 minutes\nNo Minimum scheduling interval\n15 minutes\nNo Maximum number of roll-ups into a single object\n32\nNo Maximum number of EC2 instances per Ec2Resource object\n1\nNo",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-account",
"main_header": "Account Limits",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-account",
"text": "Account Limits"
},
"links": [],
"text": "Attribute\nLimit\nAdjustable Number of pipelines\n100\nYes Number of objects per pipeline\n100\nYes Number of active instances per object\n5\nYes Number of fields per object\n50\nNo Number of UTF8 bytes per field name or identifier\n256\nNo Number of UTF8 bytes per field\n10,240\nNo Number of UTF8 bytes per object\n15,360 (including field names)\nNo Rate of creation of an instance from an object\n1 per 5 minutes\nNo Retries of a pipeline activity\n5 per task\nNo Minimum delay between retry attempts\n2 minutes\nNo Minimum scheduling interval\n15 minutes\nNo Maximum number of roll-ups into a single object\n32\nNo Maximum number of EC2 instances per Ec2Resource object\n1\nNo",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-account",
"main_header": "Account Limits",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "API \nRegular rate limit\nBurst limit ActivatePipeline\n1 call per second\n100 calls CreatePipeline\n1 call per second\n100 calls DeletePipeline\n1 call per second\n100 calls DescribeObjects\n2 calls per second\n100 calls DescribePipelines\n1 call per second\n100 calls GetPipelineDefinition\n1 call per second\n100 calls PollForTask\n2 calls per second\n100 calls ListPipelines\n1 call per second\n100 calls PutPipelineDefinition\n1 call per second\n100 calls QueryObjects\n2 calls per second\n100 calls ReportTaskProgress\n10 calls per second\n100 calls SetTaskStatus\n10 calls per second\n100 calls SetStatus\n1 call per second\n100 calls ReportTaskRunnerHeartbeat\n1 call per second\n100 calls ValidatePipelineDefinition\n1 call per second\n100 calls",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "API \nRegular rate limit\nBurst limit ActivatePipeline\n1 call per second\n100 calls CreatePipeline\n1 call per second\n100 calls DeletePipeline\n1 call per second\n100 calls DescribeObjects\n2 calls per second\n100 calls DescribePipelines\n1 call per second\n100 calls GetPipelineDefinition\n1 call per second\n100 calls PollForTask\n2 calls per second\n100 calls ListPipelines\n1 call per second\n100 calls PutPipelineDefinition\n1 call per second\n100 calls QueryObjects\n2 calls per second\n100 calls ReportTaskProgress\n10 calls per second\n100 calls SetTaskStatus\n10 calls per second\n100 calls SetStatus\n1 call per second\n100 calls ReportTaskRunnerHeartbeat\n1 call per second\n100 calls ValidatePipelineDefinition\n1 call per second\n100 calls",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#RelatedResources",
"text": "AWS Data Pipeline Resources"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html#RelatedResources",
"main_header": "AWS Data Pipeline Resources",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#RelatedResources",
"text": "AWS Data Pipeline Resources"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline",
"title": "AWS Data Pipeline\n\t\t\t\t\t\t\tProduct Information"
},
{
"url": "http://aws.amazon.com/datapipeline/faqs/",
"title": "AWS Data Pipeline Technical FAQ"
},
{
"url": "http://aws.amazon.com/releasenotes/AWS-Data-Pipeline/",
"title": "Release Notes"
},
{
"url": "https://forums.aws.amazon.com/forum.jspa?forumID=151",
"title": "AWS Data\n\t\t\t\t\t\t\tPipeline Discussion Forums"
}
],
"text": "AWS Data Pipeline Product Information\u00e2\u0080\u0093The primary web page for information about AWS Data Pipeline.\n\nAWS Data Pipeline Technical FAQ  \u00e2\u0080\u0093 Covers the top 20 questions developers ask about this product.\n\nRelease Notes \u00e2\u0080\u0093 Provide a high-level overview of the current release. They specifically note any new features, corrections, and known issues.\n\nAWS Data Pipeline Discussion Forums  \u00e2\u0080\u0093 A community-based forum for developers to discuss technical questions related to Amazon Web Services.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html#RelatedResources",
"main_header": "AWS Data Pipeline Resources",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#RelatedResources",
"text": "AWS Data Pipeline Resources"
},
"links": [
{
"url": "https://aws.amazon.com/training/course-descriptions/",
"title": "Classes & Workshops"
},
{
"url": "https://aws.amazon.com/tools/",
"title": "AWS Developer Tools"
},
{
"url": "https://aws.amazon.com/whitepapers/",
"title": "AWS Whitepapers"
},
{
"url": "https://console.aws.amazon.com/support/home#/",
"title": "AWS Support Center"
},
{
"url": "https://aws.amazon.com/premiumsupport/",
"title": "AWS Support"
},
{
"url": "https://aws.amazon.com/contact-us/",
"title": "Contact Us"
},
{
"url": "https://aws.amazon.com/terms/",
"title": "AWS Site Terms"
}
],
"text": "Classes & Workshops \u00e2\u0080\u0093 Links to role-based and specialty courses, in addition to self-paced labs to help sharpen your AWS skills and gain practical experience. AWS Developer Tools \u00e2\u0080\u0093 Links to developer tools, SDKs, IDE toolkits, and command line tools for developing and managing AWS applications. AWS Whitepapers \u00e2\u0080\u0093 Links to a comprehensive list of technical AWS whitepapers, covering topics such as architecture, security, and economics and authored by AWS Solutions Architects or other technical experts. AWS Support Center \u00e2\u0080\u0093 The hub for creating and managing your AWS Support cases. Also includes links to other helpful resources, such as forums, technical FAQs, service health status, and AWS Trusted Advisor. AWS Support \u00e2\u0080\u0093 The primary webpage for information about AWS Support, a one-on-one, fast-response support channel to help you build and run applications in the cloud. Contact Us \u00e2\u0080\u0093 A central contact point for inquiries concerning AWS billing, account, events, abuse, and other issues. AWS Site Terms \u00e2\u0080\u0093 Detailed information about our copyright and trademark; your account, license, and site access; and other topics.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html#RelatedResources",
"main_header": "AWS Data Pipeline Resources",
"images": [],
"container_type": "div",
"children_tags": [
"ul"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#DocHistory",
"text": "Document History"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html#DocHistory",
"main_header": "Document History",
"images": [],
"container_type": "div",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#DocHistory",
"text": "Document History"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"title": "Syntax"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"title": "Attach EBS volumes to cluster nodes"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"title": "Configure an Amazon EMR cluster in a private subnet"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"title": "On-Demand"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"title": "RdsDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"title": "SqlActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"title": "SqlActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"title": "JdbcDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"title": "HadoopActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"title": "Deactivating Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"title": "Creating Pipelines Using Console Templates"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"title": "PigActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"title": "Cascading Failures and Reruns"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"title": "Editing Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"title": "Using a Pipeline with Resources in Multiple Regions"
}
],
"text": "Change\nDescription\nRelease Date Updated the lists of supported Amazon EC2 and Amazon EMR instances. \nUpdated the list of IDs of the HVM (Hardware Virtual Machine) AMIs used for the instances. Updated the lists of supported Amazon EC2 and Amazon EMR instances. For more information, see Supported Instance Types for Pipeline Work Activities.\nUpdated the list of IDs of the HVM (Hardware Virtual Machine) AMIs used for the instances. For more information, see Syntax and search for imageId.\n\n9 November 2018 Added configuration for attaching Amazon EBS volumes to cluster nodes, and for launching an Amazon EMR cluster into a private subnet.\n\nAdded configuration options to an EMRcluster object. You can use these options in pipelines that use Amazon EMR clusters. \nUse the coreEbsConfiguration, masterEbsConfiguration, and TaskEbsConfiguration fields to configure the attachment of Amazon EBS volumes to core, master, and task nodes in the Amazon EMR cluster. For more information, see Attach EBS volumes to cluster nodes.\nUse the emrManagedMasterSecurityGroupId, emrManagedSlaveSecurityGroupId, and ServiceAccessSecurityGroupId fields to configure an Amazon EMR cluster in a private subnet. For more information, see Configure an Amazon EMR cluster in a private subnet.\n For more information about EMRcluster syntax, see EmrCluster.\n\n19 April 2018 Added the list of supported Amazon EC2 and Amazon EMR instances.\n\nAdded the list of instances that AWS Data Pipeline creates by default, if you do not specify an instance type in the pipeline definition. Added a list of supported Amazon EC2 and Amazon EMR instances. For more information, see Supported Instance Types for Pipeline Work Activities.\n\n22 March 2018 Added support for On-demand pipelines. Added support for On-demand pipelines, which allows you to re-run a pipeline by activating it again. For more information, see On-Demand. 22 February 2016 Additional support for RDS databases Added rdsInstanceId, region, and jdbcDriverJarUri to RdsDatabase.Updated database in SqlActivity to also support RdsDatabase.\n\n17 August 2015 Additional JDBC support Updated database in SqlActivity to also support JdbcDatabase.Added jdbcDriverJarUri to JdbcDatabaseAdded initTimeout to Ec2Resource and EmrCluster.Added runAsUser to Ec2Resource.\n\n7 July 2015 HadoopActivity, Availability Zone, and Spot Support Added support for submitting parallel work to Hadoop clusters. For more information, see HadoopActivity.\nAdded the ability to request Spot Instances with Ec2Resource and EmrCluster.\n\nAdded the ability to launch EmrCluster resources in a specified Availability Zone. 1 June 2015 Deactivating pipelines\n\nAdded support for deactivating active pipelines. For more information, see Deactivating Your Pipeline.\n\n7 April 2015 Updated templates and console\n\nAdded new templates as reflected in the console. Updated the Getting Started chapter to use the Getting Started with ShellCommandActivity template. For more information, see Creating Pipelines Using Console Templates.\n\n25 November 2014 VPC support\n\nAdded support for launching resources into a virtual private cloud (VPC). For more information, see Launching Resources for Your Pipeline into a VPC.\n\n12 March 2014 Region support\n\nAdded support for multiple service regions. In addition to us-east-1, AWS Data Pipeline is supported in eu-west-1, ap-northeast-1, ap-southeast-2, and us-west-2.\n\n20 February 2014 Amazon Redshift support\n\nAdded support for Amazon Redshift in AWS Data Pipeline, including a new console template (Copy to Redshift) and a tutorial to demonstrate the template. For more information, see Copy Data to Amazon Redshift Using AWS Data Pipeline, RedshiftDataNode, RedshiftDatabase, and RedshiftCopyActivity.\n\n6 November 2013 PigActivity\n\nAdded PigActivity, which provides native support for Pig. For more information, see PigActivity.\n\n15 October 2013 New console template, activity, and data format\n\nAdded the new CrossRegion DynamoDB Copy console template, including the new HiveCopyActivity and DynamoDBExportDataFormat.\n\n21 August 2013 Cascading failures and reruns\n\nAdded information about AWS Data Pipeline cascading failure and rerun behavior. For more information, see Cascading Failures and Reruns. 8 August 2013 Troubleshooting video\n\nAdded the AWS Data Pipeline Basic Troubleshooting video. For more information, see Troubleshooting. 17 July 2013 Editing active pipelines\n\nAdded more information about editing active pipelines and rerunning pipeline components. For more information, see Editing Your Pipeline. 17 July 2013 Use resources in different regions\n\nAdded more information about using resources in different regions. For more information, see Using a Pipeline with Resources in Multiple Regions. 17 June 2013 WAITING_ON_DEPENDENCIES status\n\nCHECKING_PRECONDITIONS status changed to WAITING_ON_DEPENDENCIES and added the @waitingOn runtime field for pipeline objects.\n\n20 May 2013 DynamoDBDataFormat\n\nAdded DynamoDBDataFormat template.\n\n23 April 2013 Process Web Logs video and Spot Instances support\n\nIntroduced the video \"Process Web Logs with AWS Data Pipeline, Amazon EMR, and Hive,\" and Amazon EC2 Spot Instances support.\n\n21 February 2013 The initial release of the AWS Data Pipeline Developer Guide. 20 December 2012",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html#DocHistory",
"main_header": "Document History",
"images": [],
"container_type": "div",
"children_tags": [
"div"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#DocHistory",
"text": "Document History"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"title": "Syntax"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-ebs.html",
"title": "Attach EBS volumes to cluster nodes"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example-private-subnet.html",
"title": "Configure an Amazon EMR cluster in a private subnet"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"title": "On-Demand"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"title": "RdsDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"title": "SqlActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"title": "SqlActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"title": "JdbcDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"title": "HadoopActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"title": "Deactivating Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"title": "Creating Pipelines Using Console Templates"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"title": "PigActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"title": "Cascading Failures and Reruns"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"title": "Editing Your Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"title": "Using a Pipeline with Resources in Multiple Regions"
}
],
"text": "Change\nDescription\nRelease Date Updated the lists of supported Amazon EC2 and Amazon EMR instances. \nUpdated the list of IDs of the HVM (Hardware Virtual Machine) AMIs used for the instances. Updated the lists of supported Amazon EC2 and Amazon EMR instances. For more information, see Supported Instance Types for Pipeline Work Activities.\nUpdated the list of IDs of the HVM (Hardware Virtual Machine) AMIs used for the instances. For more information, see Syntax and search for imageId.\n\n9 November 2018 Added configuration for attaching Amazon EBS volumes to cluster nodes, and for launching an Amazon EMR cluster into a private subnet.\n\nAdded configuration options to an EMRcluster object. You can use these options in pipelines that use Amazon EMR clusters. \nUse the coreEbsConfiguration, masterEbsConfiguration, and TaskEbsConfiguration fields to configure the attachment of Amazon EBS volumes to core, master, and task nodes in the Amazon EMR cluster. For more information, see Attach EBS volumes to cluster nodes.\nUse the emrManagedMasterSecurityGroupId, emrManagedSlaveSecurityGroupId, and ServiceAccessSecurityGroupId fields to configure an Amazon EMR cluster in a private subnet. For more information, see Configure an Amazon EMR cluster in a private subnet.\n For more information about EMRcluster syntax, see EmrCluster.\n\n19 April 2018 Added the list of supported Amazon EC2 and Amazon EMR instances.\n\nAdded the list of instances that AWS Data Pipeline creates by default, if you do not specify an instance type in the pipeline definition. Added a list of supported Amazon EC2 and Amazon EMR instances. For more information, see Supported Instance Types for Pipeline Work Activities.\n\n22 March 2018 Added support for On-demand pipelines. Added support for On-demand pipelines, which allows you to re-run a pipeline by activating it again. For more information, see On-Demand. 22 February 2016 Additional support for RDS databases Added rdsInstanceId, region, and jdbcDriverJarUri to RdsDatabase.Updated database in SqlActivity to also support RdsDatabase.\n\n17 August 2015 Additional JDBC support Updated database in SqlActivity to also support JdbcDatabase.Added jdbcDriverJarUri to JdbcDatabaseAdded initTimeout to Ec2Resource and EmrCluster.Added runAsUser to Ec2Resource.\n\n7 July 2015 HadoopActivity, Availability Zone, and Spot Support Added support for submitting parallel work to Hadoop clusters. For more information, see HadoopActivity.\nAdded the ability to request Spot Instances with Ec2Resource and EmrCluster.\n\nAdded the ability to launch EmrCluster resources in a specified Availability Zone. 1 June 2015 Deactivating pipelines\n\nAdded support for deactivating active pipelines. For more information, see Deactivating Your Pipeline.\n\n7 April 2015 Updated templates and console\n\nAdded new templates as reflected in the console. Updated the Getting Started chapter to use the Getting Started with ShellCommandActivity template. For more information, see Creating Pipelines Using Console Templates.\n\n25 November 2014 VPC support\n\nAdded support for launching resources into a virtual private cloud (VPC). For more information, see Launching Resources for Your Pipeline into a VPC.\n\n12 March 2014 Region support\n\nAdded support for multiple service regions. In addition to us-east-1, AWS Data Pipeline is supported in eu-west-1, ap-northeast-1, ap-southeast-2, and us-west-2.\n\n20 February 2014 Amazon Redshift support\n\nAdded support for Amazon Redshift in AWS Data Pipeline, including a new console template (Copy to Redshift) and a tutorial to demonstrate the template. For more information, see Copy Data to Amazon Redshift Using AWS Data Pipeline, RedshiftDataNode, RedshiftDatabase, and RedshiftCopyActivity.\n\n6 November 2013 PigActivity\n\nAdded PigActivity, which provides native support for Pig. For more information, see PigActivity.\n\n15 October 2013 New console template, activity, and data format\n\nAdded the new CrossRegion DynamoDB Copy console template, including the new HiveCopyActivity and DynamoDBExportDataFormat.\n\n21 August 2013 Cascading failures and reruns\n\nAdded information about AWS Data Pipeline cascading failure and rerun behavior. For more information, see Cascading Failures and Reruns. 8 August 2013 Troubleshooting video\n\nAdded the AWS Data Pipeline Basic Troubleshooting video. For more information, see Troubleshooting. 17 July 2013 Editing active pipelines\n\nAdded more information about editing active pipelines and rerunning pipeline components. For more information, see Editing Your Pipeline. 17 July 2013 Use resources in different regions\n\nAdded more information about using resources in different regions. For more information, see Using a Pipeline with Resources in Multiple Regions. 17 June 2013 WAITING_ON_DEPENDENCIES status\n\nCHECKING_PRECONDITIONS status changed to WAITING_ON_DEPENDENCIES and added the @waitingOn runtime field for pipeline objects.\n\n20 May 2013 DynamoDBDataFormat\n\nAdded DynamoDBDataFormat template.\n\n23 April 2013 Process Web Logs video and Spot Instances support\n\nIntroduced the video \"Process Web Logs with AWS Data Pipeline, Amazon EMR, and Hive,\" and Amazon EC2 Spot Instances support.\n\n21 February 2013 The initial release of the AWS Data Pipeline Developer Guide. 20 December 2012",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html#DocHistory",
"main_header": "Document History",
"images": [],
"container_type": "div",
"children_tags": [
"table"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"links": [],
"text": "AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#what-is-datapipeline",
"main_header": "What is AWS Data Pipeline?",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"links": [],
"text": "The following components of AWS Data Pipeline work together to manage your data:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#what-is-datapipeline",
"main_header": "What is AWS Data Pipeline?",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"links": [],
"text": "For example, you can use AWS Data Pipeline to archive your web server's logs to Amazon Simple Storage Service (Amazon S3) each day and then run a weekly Amazon EMR (Amazon EMR) cluster over those logs to generate traffic reports. AWS Data Pipeline schedules the daily tasks to copy data and the weekly task to launch the Amazon EMR cluster. AWS Data Pipeline also ensures that Amazon EMR waits for the final day's data to be uploaded to Amazon S3 before it begins its analysis, even if there is an unforeseen delay in uploading the logs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#what-is-datapipeline",
"main_header": "What is AWS Data Pipeline?",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"h2": {
"urllink": "#accessing-datapipeline",
"text": "Accessing AWS Data Pipeline"
},
"links": [],
"text": "You can create, access, and manage your pipelines using any of the following interfaces:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#accessing-datapipeline",
"main_header": "Accessing AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"h2": {
"urllink": "#datapipeline-pricing",
"text": "Pricing"
},
"links": [
{
"url": "https://aws.amazon.com/datapipeline/pricing/",
"title": "AWS Data Pipeline Pricing"
}
],
"text": "With Amazon Web Services, you pay only for what you use. For AWS Data Pipeline, you pay for your pipeline based on how often your activities and preconditions are scheduled to run and where they run. For more information, see AWS Data Pipeline Pricing.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#datapipeline-pricing",
"main_header": "Pricing",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#what-is-datapipeline",
"text": "What is AWS Data Pipeline?"
},
"h2": {
"urllink": "#datapipeline-pricing",
"text": "Pricing"
},
"links": [
{
"url": "https://aws.amazon.com/free/",
"title": "AWS Free Tier"
}
],
"text": "If your AWS account is less than 12 months old, you are eligible to use the free tier. The free tier includes three low-frequency preconditions and five low-frequency activities per month at no charge. For more information, see AWS Free Tier.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html#datapipeline-pricing",
"main_header": "Pricing",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#datapipeline-related-services",
"text": "Related Services"
},
"links": [],
"text": "AWS Data Pipeline works with the following services to store data.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html#datapipeline-related-services",
"main_header": "Related Services",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#datapipeline-related-services",
"text": "Related Services"
},
"links": [],
"text": "AWS Data Pipeline works with the following compute services to transform data.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/datapipeline-related-services.html#datapipeline-related-services",
"main_header": "Related Services",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#dp-supported-instance-types",
"text": "Supported Instance Types for Pipeline Work Activities"
},
"links": [],
"text": "When AWS Data Pipeline runs a pipeline, it compiles the pipeline components to create a set of actionable Amazon EC2 instances. Each instance contains all the information for performing a specific task. The complete set of instances is the to-do list of the pipeline. AWS Data Pipeline hands the instances out to task runners to process.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html#dp-supported-instance-types",
"main_header": "Supported Instance Types for Pipeline Work Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#dp-supported-instance-types",
"text": "Supported Instance Types for Pipeline Work Activities"
},
"links": [
{
"url": "https://aws.amazon.com//ec2/pricing",
"title": "Amazon EC2 Pricing Page"
},
{
"url": "https://aws.amazon.com/ec2/instance-types/",
"title": "Amazon EC2 Instances"
},
{
"url": "https://aws.amazon.com/amazon-linux-ami/instance-type-matrix/",
"title": "Amazon Linux AMI Instance Type Matrix"
}
],
"text": "EC2 instances come in different configurations, which are known as instance types. Each instance type has a different CPU, input/output, and storage capacity. In addition to specifying the instance type for an activity, you can choose different purchasing options. Not all instance types are available in all AWS Regions. If an instance type is not available, your pipeline may fail to provision or may be stuck provisioning. For information about instance availability, see the Amazon EC2 Pricing Page. Open the link for your instance purchasing option and filter by Region to see if an instance type is available in the Region. For more information about these instance types, families, and virtualization types, see Amazon EC2 Instances and Amazon Linux AMI Instance Type Matrix.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html#dp-supported-instance-types",
"main_header": "Supported Instance Types for Pipeline Work Activities",
"images": [],
"container_type": "p",
"children_tags": [
"em",
"a",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#dp-supported-instance-types",
"text": "Supported Instance Types for Pipeline Work Activities"
},
"links": [
{
"url": "https://docs.aws.amazon.com/general/latest/gr/rande.html#datapipeline_region",
"title": "AWS Regions and Endpoints"
}
],
"text": "The following tables describe the instance types that AWS Data Pipeline supports. You can use AWS Data Pipeline to launch Amazon EC2 instances in any Region, including Regions where AWS Data Pipeline is not supported. For information about Regions where AWS Data Pipeline is supported, see AWS Regions and Endpoints.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html#dp-supported-instance-types",
"main_header": "Supported Instance Types for Pipeline Work Activities",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "If you do not specify an instance type in your pipeline definition, AWS Data Pipeline launches an instance by default.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "The following table lists the Amazon EC2 instances that AWS Data Pipeline uses by default in those Regions where AWS Data Pipeline is supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-default-instance-types",
"text": "Default Amazon EC2 Instances by AWS Region"
},
"links": [],
"text": "The following table lists the Amazon EC2 instances that AWS Data Pipeline launches by default in those Regions where AWS Data Pipeline is not supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html#dp-ec2-default-instance-types",
"main_header": "Default Amazon EC2 Instances by AWS Region",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-supported-instance-types",
"text": "Additional Supported Amazon EC2 Instances"
},
"links": [],
"text": "In addition to the default instances that are created if you don't specify an instance type in your pipeline definition, the following instances are supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html#dp-ec2-supported-instance-types",
"main_header": "Additional Supported Amazon EC2 Instances",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-ec2-supported-instance-types",
"text": "Additional Supported Amazon EC2 Instances"
},
"links": [],
"text": "The following table lists the Amazon EC2 instances that AWS Data Pipeline supports and can create, if specified.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html#dp-ec2-supported-instance-types",
"main_header": "Additional Supported Amazon EC2 Instances",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-emr-supported-instance-types",
"text": "Supported Amazon EC2 Instances for Amazon EMR Clusters"
},
"links": [
{
"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html",
"title": "Supported Instance Types"
}
],
"text": "This table lists the Amazon EC2 instances that AWS Data Pipeline supports and can create for Amazon EMR clusters, if specified. For more information, see Supported Instance Types in the Amazon EMR Management Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-supported-instance-types.html#dp-emr-supported-instance-types",
"main_header": "Supported Amazon EC2 Instances for Amazon EMR Clusters",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-supported-instance-types.html",
"title": "Supported Instance Types for Pipeline Work Activities"
}
]
},
{
"h1": {
"urllink": "#dp-concepts",
"text": "AWS Data Pipeline Concepts"
},
"links": [],
"text": "Before you begin, read about the key concepts and components for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html#dp-concepts",
"main_header": "AWS Data Pipeline Concepts",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "A pipeline definition is how you communicate your business logic to AWS Data Pipeline. It contains the following information:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "From your pipeline definition, AWS Data Pipeline determines the tasks, schedules them, and assigns them to task runners. If a task is not completed successfully, AWS Data Pipeline retries the task according to your instructions and, if necessary, reassigns it to another task runner. If the task fails repeatedly, you can configure the pipeline to notify you.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "For example, in your pipeline definition, you might specify that log files generated by your application are archived each month in 2013 to an Amazon S3 bucket. AWS Data Pipeline would then create 12 tasks, each copying over a month's worth of data, regardless of whether the month contained 30, 31, 28, or 29 days.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "You can create a pipeline definition in the following ways:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [],
"text": "A pipeline definition can contain the following types of components.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-pipeline-definition",
"text": "Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"title": "Pipeline Definition File Syntax"
}
],
"text": "For more information, see Pipeline Definition File Syntax.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-pipeline-definition.html#dp-how-pipeline-definition",
"main_header": "Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-tasks-scheduled",
"text": "Pipeline Components, Instances, and Attempts"
},
"links": [],
"text": "There are three types of items associated with a scheduled pipeline:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-tasks-scheduled.html#dp-how-tasks-scheduled",
"main_header": "Pipeline Components, Instances, and Attempts",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "A task runner is an application that polls AWS Data Pipeline for tasks and then performs those tasks.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "Task Runner is a default implementation of a task runner that is provided by AWS Data Pipeline. When Task Runner is installed and configured, it polls AWS Data Pipeline for tasks associated with pipelines that you have activated. When a task is assigned to Task Runner, it performs that task and reports its status back to AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "The following diagram illustrates how AWS Data Pipeline and a task runner interact to process a scheduled task. A task is a discrete unit of work that the AWS Data Pipeline service shares with a task runner. It differs from a pipeline, which is a general definition of activities and resources that usually yields several tasks.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [],
"text": "There are two ways you can use Task Runner to process your pipeline:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-how-remote-taskrunner-client",
"text": "Task Runners"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html",
"title": "Working with Task Runner"
}
],
"text": "For more information about working with Task Runner, see Working with Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-remote-taskrunner-client.html#dp-how-remote-taskrunner-client",
"main_header": "Task Runners",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-datanodes",
"text": "Data Nodes"
},
"links": [],
"text": "In AWS Data Pipeline, a data node defines the location and type of data that a pipeline activity uses as input or output. AWS Data Pipeline supports the following types of data nodes:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html#dp-concepts-datanodes",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-databases",
"text": "Databases"
},
"links": [],
"text": "AWS Data Pipeline supports the following types of databases:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-databases.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-databases.html#dp-concepts-databases",
"main_header": "Databases",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-activities",
"text": "Activities"
},
"links": [],
"text": "In AWS Data Pipeline, an activity is a pipeline component that defines the work to perform. AWS Data Pipeline provides several pre-packaged activities that accommodate common scenarios, such as moving data from one location to another, running Hive queries, and so on. Activities are extensible, so you can run your own custom scripts to support endless combinations.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html#dp-concepts-activities",
"main_header": "Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-activities",
"text": "Activities"
},
"links": [],
"text": "AWS Data Pipeline supports the following types of activities:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html#dp-concepts-activities",
"main_header": "Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-activities",
"text": "Activities"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"title": "Staging Data and Tables with Pipeline Activities"
}
],
"text": "Some activities have special support for staging data and database tables. For more information, see Staging Data and Tables with Pipeline Activities.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html#dp-concepts-activities",
"main_header": "Activities",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-preconditions",
"text": "Preconditions"
},
"links": [],
"text": "In AWS Data Pipeline, a precondition is a pipeline component containing conditional statements that must be true before an activity can run. For example, a precondition can check whether source data is present before a pipeline activity attempts to copy it. AWS Data Pipeline provides several pre-packaged preconditions that accommodate common scenarios, such as whether a database table exists, whether an Amazon S3 key is present, and so on. However, preconditions are extensible and allow you to run your own custom scripts to support endless combinations.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html#dp-concepts-preconditions",
"main_header": "Preconditions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-preconditions",
"text": "Preconditions"
},
"links": [],
"text": "There are two types of preconditions: system-managed preconditions and user-managed preconditions. System-managed preconditions are run by the AWS Data Pipeline web service on your behalf and do not require a computational resource. User-managed preconditions only run on the computational resource that you specify using the runsOn or workerGroup fields. The workerGroup resource is derived from the activity that uses the precondition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html#dp-concepts-preconditions",
"main_header": "Preconditions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"links": [],
"text": "In AWS Data Pipeline, a resource is the computational resource that performs the work that a pipeline activity specifies. AWS Data Pipeline supports the following types of resources:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-concepts-resources",
"main_header": "Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"title": "Using a Pipeline with Resources in Multiple Regions"
}
],
"text": "Resources can run in the same region with their working dataset, even a region different than AWS Data Pipeline. For more information, see Using a Pipeline with Resources in Multiple Regions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-concepts-resources",
"main_header": "Resources",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-limits",
"text": "Resource Limits"
},
"links": [
{
"url": "http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html",
"title": "AWS Service Limits"
}
],
"text": "AWS Data Pipeline scales to accommodate a huge number of concurrent tasks and you can configure it to automatically create the resources necessary to handle large workloads. These automatically created resources are under your control and count against your AWS account resource limits. For example, if you configure AWS Data Pipeline to create a 20-node Amazon EMR cluster automatically to process data and your AWS account has an EC2 instance limit set to 20, you may inadvertently exhaust your available backfill resources. As a result, consider these resource restrictions in your design or increase your account limits accordingly. For more information about service limits, see AWS Service Limits in the AWS General Reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-limits",
"main_header": "Resource Limits",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-supported-platforms",
"text": "Supported Platforms"
},
"links": [],
"text": "Pipelines can launch your resources into the following platforms:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-supported-platforms",
"main_header": "Supported Platforms",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-supported-platforms",
"text": "Supported Platforms"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-platforms.html",
"title": "Supported Platforms"
}
],
"text": "Your AWS account can launch resources either into both platforms or only into EC2-VPC, on a region by region basis. For more information, see Supported Platforms in the Amazon EC2 User Guide for Linux Instances.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-supported-platforms",
"main_header": "Supported Platforms",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-supported-platforms",
"text": "Supported Platforms"
},
"links": [],
"text": "If your AWS account supports only EC2-VPC, we create a default VPC for you in each AWS Region. By default, we launch your resources into a default subnet of your default VPC. Alternatively, you can create a nondefault VPC and specify one of its subnets when you configure your resources, and then we launch your resources into the specified subnet of the nondefault VPC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-supported-platforms",
"main_header": "Supported Platforms",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-supported-platforms",
"text": "Supported Platforms"
},
"links": [],
"text": "When you launch an instance into a VPC, you must specify a security group created specifically for that VPC. You can't specify a security group that you created for EC2-Classic when you launch an instance into a VPC. In addition, you must use the security group ID and not the security group name to identify a security group for a VPC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-supported-platforms",
"main_header": "Supported Platforms",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-resource-supported-platforms",
"text": "Supported Platforms"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"title": "Launching Resources for Your Pipeline into a VPC"
}
],
"text": "For more information about using a VPC with AWS Data Pipeline, see Launching Resources for Your Pipeline into a VPC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-resource-supported-platforms",
"main_header": "Supported Platforms",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-emrspotinstances",
"text": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/ec2/spot-instances/",
"title": "Amazon EC2 Spot Instances"
}
],
"text": "Pipelines can use Amazon EC2 Spot Instances for the task nodes in their Amazon EMR cluster resources. By default, pipelines use On-Demand Instances. Spot Instances let you use spare EC2 instances and run them. The Spot Instance pricing model complements the On-Demand and Reserved Instance pricing models, potentially providing the most cost-effective option for obtaining compute capacity, depending on your application. For more information, see the Amazon EC2 Spot Instances product page.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-emrspotinstances",
"main_header": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-emrspotinstances",
"text": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline"
},
"links": [],
"text": "When you use Spot Instances, AWS Data Pipeline submits your Spot Instance maximum price to Amazon EMR when your cluster is launched. It automatically allocates the cluster's work to the number of Spot Instance task nodes that you define using the taskInstanceCount field. AWS Data Pipeline limits Spot Instances for task nodes to ensure that on-demand core nodes are available to run your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-emrspotinstances",
"main_header": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-emrspotinstances",
"text": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline"
},
"links": [],
"text": "You can edit a failed or completed pipeline resource instance to add Spot Instances. When the pipeline re-launches the cluster, it uses Spot Instances for the task nodes.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-emrspotinstances",
"main_header": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-resources",
"text": "Resources"
},
"h2": {
"urllink": "#dp-emrspotinstances",
"text": "Amazon EC2 Spot Instances with Amazon EMR Clusters and AWS Data Pipeline"
},
"h3": {
"urllink": "#dp-emrspotinstances-considerations",
"text": "Spot Instances Considerations"
},
"links": [],
"text": "When you use Spot Instances with AWS Data Pipeline, the following considerations apply:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-resources.html#dp-emrspotinstances-considerations",
"main_header": "Spot Instances Considerations",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-actions",
"text": "Actions"
},
"links": [],
"text": "AWS Data Pipeline actions are steps that a pipeline component takes when certain events occur, such as success, failure, or late activities. The event field of an activity refers to an action, such as a reference to snsalarm in the onLateAction field of EmrActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html#dp-concepts-actions",
"main_header": "Actions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-actions",
"text": "Actions"
},
"links": [
{
"url": "https://aws.amazon.com/sns/",
"title": "Amazon SNS"
}
],
"text": "AWS Data Pipeline relies on Amazon SNS notifications as the primary way to indicate the status of pipelines and their components in an unattended manner. For more information, see Amazon SNS. In addition to SNS notifications, you can use the AWS Data Pipeline console and CLI to obtain pipeline status information.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html#dp-concepts-actions",
"main_header": "Actions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-actions",
"text": "Actions"
},
"links": [],
"text": "AWS Data Pipeline supports the following actions:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html#dp-concepts-actions",
"main_header": "Actions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-actions",
"text": "Actions"
},
"h2": {
"urllink": "#dp-monitor-pipelines",
"text": "Proactively Monitoring Pipelines"
},
"links": [],
"text": "The best way to detect problems is to monitor your pipelines proactively from the start. You can configure pipeline components to inform you of certain situations or events, such as when a pipeline component fails or doesn't begin by its scheduled start time. AWS Data Pipeline makes it easy to configure notifications by providing event fields on pipeline components that you can associate with Amazon SNS notifications, such as onSuccess, OnFail, and onLateAction.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-actions.html#dp-monitor-pipelines",
"main_header": "Proactively Monitoring Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts.html",
"title": "AWS Data Pipeline Concepts"
}
]
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"links": [],
"text": "Before you use AWS Data Pipeline for the first time, complete the following tasks.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-get-setup",
"main_header": "Setting up for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"title": "Getting Started with AWS Data Pipeline"
}
],
"text": "After you complete these tasks, you can start using AWS Data Pipeline. For a basic tutorial, see Getting Started with AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-get-setup",
"main_header": "Setting up for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-sign-up",
"text": "Sign up for AWS"
},
"links": [
{
"url": "http://aws.amazon.com/datapipeline/",
"title": "AWS Data Pipeline"
}
],
"text": "When you sign up for Amazon Web Services (AWS), your AWS account is automatically signed up for all services in AWS, including AWS Data Pipeline. You are charged only for the services that you use. For more information about AWS Data Pipeline usage rates, see AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-sign-up",
"main_header": "Sign up for AWS",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-sign-up",
"text": "Sign up for AWS"
},
"links": [],
"text": "If you have an AWS account already, skip to the next task. If you don't have an AWS account, use the following procedure to create one.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-sign-up",
"main_header": "Sign up for AWS",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Create IAM Roles for AWS Data Pipeline and Pipeline Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "AWS Data Pipeline requires IAM roles that determine the permissions to perform actions and access AWS resources. The pipeline role determines the permissions that AWS Data Pipeline has, and a resource role determines the permissions that applications running on pipeline resources, such as EC2 instances, have. You specify these roles when you create a pipeline. Even if you do not specify a custom role and use the default roles DataPipelineDefaultRole and DataPipelineDefaultResourceRole, you must first create the roles and attach permissions policies. For more information, see IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-roles-new",
"main_header": "Create IAM Roles for AWS Data Pipeline and Pipeline Resources",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"em",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/APIReference/API_Operations.html",
"title": "AWS Data Pipeline actions"
}
],
"text": "To work with a pipeline, an IAM principal (a user or group) in your account must be allowed to perform required AWS Data Pipeline actions and actions for other services as defined by your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [],
"text": "To simplify permissions, the AWSDataPipeline_FullAccess managed policy is available for you to attach to IAM principals. This managed policy allows the principal to perform all actions that a user requires and the iam:PassRole action on the default roles used with AWS Data Pipeline when a custom role is not specified.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"title": ""
}
],
"text": "We highly recommend that you carefully evaluate this managed policy and restrict permissions only to those that your users require. If necessary, use this policy as a starting point, and then remove permissions to create a more restrictive inline permissions policy that you can attach to IAM principals. For more information and example permissions policies, see",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [],
"text": "A policy statement similar to the following example must be included in a policy attached to any IAM principal that uses the pipeline. This statement allows the IAM principal to perform the PassRole action on the roles that a pipeline uses. If you do not use default roles, replace MyPipelineRole and MyResourceRole with the custom roles that you create.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-get-setup",
"text": "Setting up for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-create-user-groups",
"text": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions"
},
"links": [],
"text": "The following procedure demonstrates how to create an IAM group, attach the AWSDataPipeline_FullAccess managed policy to the group, and then add users to the group. You can use this procedure for any inline policy",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html#dp-iam-create-user-groups",
"main_header": "Allow IAM Principals (Users and Groups) to Perform Necessary Actions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "AWS Data Pipeline helps you sequence, schedule, run, and manage recurring data processing workloads reliably and cost-effectively. This service makes it easy for you to design extract-transform-load (ETL) activities using structured and unstructured data, both on-premises and in the cloud, based on your business logic.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html",
"title": "activities"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html",
"title": "data nodes"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"title": "schedule"
}
],
"text": "To use AWS Data Pipeline, you create a pipeline definition that specifies the business logic for your data processing. A typical pipeline definition consists of activities that define the work to perform, data nodes that define the location and type of input and output data, and a schedule that determines when the activities are performed.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"em",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "In this tutorial, you run a shell command script that counts the number of GET requests in Apache web server logs. This pipeline runs every 15 minutes for an hour, and writes output to Amazon S3 on each iteration.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-setup.html",
"title": "Setting up for AWS Data Pipeline"
}
],
"text": "Before you begin, complete the tasks in Setting up for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "Pipeline Objects",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"links": [],
"text": "The pipeline uses the following objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started",
"main_header": "Getting Started with AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-create",
"text": "Create the Pipeline"
},
"links": [],
"text": "The quickest way to get started with AWS Data Pipeline is to use a pipeline definition called a template.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-create",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-monitor",
"text": "Monitor the Running Pipeline"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-monitor",
"main_header": "Monitor the Running Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-output",
"text": "View the Output"
},
"links": [],
"text": "Open the Amazon S3 console and navigate to your bucket. If you ran your pipeline every 15 minutes for an hour, you'll see four time-stamped subfolders. Each subfolder contains output in a file named output.txt. Because we ran the script on the same input file each time, the output files are identical.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-output",
"main_header": "View the Output",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-delete",
"text": "Delete the Pipeline"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-delete",
"main_header": "Delete the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-getting-started",
"text": "Getting Started with AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-getting-started-delete",
"text": "Delete the Pipeline"
},
"links": [],
"text": "If you are finished with the output from this tutorial, delete the output folders from your Amazon S3 bucket.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-getting-started.html#dp-getting-started-delete",
"main_header": "Delete the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-managing-pipeline",
"text": "Working with Pipelines"
},
"links": [],
"text": "You can administer, create, and modify pipelines using the AWS Data Pipeline console, an AWS SDK, or the command line interface (CLI). The following sections introduce fundamental AWS Data Pipeline concepts and show you how to work with pipelines.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html#dp-managing-pipeline",
"main_header": "Working with Pipelines",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "In AWS Data Pipeline, a schedule defines the timing of a scheduled event, such as when an activity runs. AWS Data Pipeline exposes this functionality through the Schedule pipeline component.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-schedules",
"main_header": "Scheduling Pipelines",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "The AWS Data Pipeline console allows you to schedule and create pipelines. This is useful for testing and prototyping pipelines before establishing them for production workloads.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "The Create Pipeline section has the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "The Schedule section has the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-creating-schedule",
"text": "Creating a Schedule Using the Console"
},
"links": [],
"text": "The IAM Roles & Permissions section has the following options:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-creating-schedule",
"main_header": "Creating a Schedule Using the Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "AWS Data Pipeline offers an on-demand schedule type, which gives the option for a pipeline to be run on pipeline activation. The pipeline is run one time in response to an activation request.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "On-demand pipelines only require the schedule type to be set to ondemand on the default object. On-demand pipelines require that you not use a schedule object and they do not allow for multiple schedules. The maximum number of concurrent executions of an on-demand pipeline can be configured using the slot maxActiveInstances in the Default object. The default value for this slot is 1 for on-demand pipelines and can have a maximum value of 5.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-ondemand",
"text": "On-Demand"
},
"links": [],
"text": "The following Default object uses on-demand scheduling:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-ondemand",
"main_header": "On-Demand",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "AWS Data Pipeline offers two types of periodic pipeline component scheduling: time series style scheduling and cron style scheduling.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "The schedule type allows you to specify whether the pipeline component instances should start at the beginning of the interval (also known as the period) or at the end of the interval.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "Time series style scheduling means that instances are scheduled at the end of each interval and cron style scheduling means that instances are scheduled at the beginning of each interval. For example, using time series style scheduling, if the start time is 22:00 UTC and the interval/period is set to 30 minutes, then the pipeline component instance's first run starts at 22:30 UTC, not 22:00 UTC. If you want the instance to run at the beginning of the period/interval, such as 22:00 UTC, use cron style scheduling instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "The following is a Default object for cron style pipelines:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"links": [],
"text": "The following is a Default object for time series style pipelines:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"main_header": "Time Series Style vs. Cron Style",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-timeseries-cron",
"text": "Time Series Style vs. Cron Style"
},
"h3": {
"urllink": "#dp-concepts-resources-special-schedule",
"text": "Resources Ignore Schedule Type"
},
"links": [],
"text": "AWS Data Pipeline creates activity and data node instances at the beginning or end of the schedule interval depending on the pipeline's schedule type setting (time series style scheduling or cron style scheduling). However, AWS Data Pipeline creates Resource instances, such as EC2Resource and EmrCluster, at the beginning of the interval regardless of the pipeline schedule type and sets them to the WAITING_ON_DEPENDENCIES status. The actual underlying resources are not instantiated until an associated activity is scheduled.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-resources-special-schedule",
"main_header": "Resources Ignore Schedule Type",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-backfills",
"text": "Backfill Tasks"
},
"links": [],
"text": "When you define a pipeline with a scheduled start time for the past, AWS Data Pipeline backfills the tasks in the pipeline. In that situation, AWS Data Pipeline immediately runs many instances of the tasks in the pipeline to catch up to the number of times those tasks would have run between the scheduled start time and the current time. When this happens, you see pipeline component instances running back-to-back at a greater frequency than the period value that you specified when you created the pipeline. AWS Data Pipeline returns your pipeline to the defined period only when it catches up to the number of past runs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-backfills",
"main_header": "Backfill Tasks",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-backfills",
"text": "Backfill Tasks"
},
"links": [],
"text": "To minimize backfills in your development and testing phases, use a relatively short interval for startDateTime..endDateTime.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-backfills",
"main_header": "Backfill Tasks",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-backfills",
"text": "Backfill Tasks"
},
"links": [],
"text": "AWS Data Pipeline attempts to prevent accidental backfills by blocking pipeline activation if the pipeline component scheduledStartTime is earlier than 1 day ago.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-backfills",
"main_header": "Backfill Tasks",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-backfills",
"text": "Backfill Tasks"
},
"links": [],
"text": "To get your pipeline to launch immediately, set Start Date Time to a date one day in the past. AWS Data Pipeline starts launching the \"past due\" runs immediately in an attempt to address what it perceives as a backlog of work. This backfilling means that you don't have to wait an hour to see AWS Data Pipeline launch its first cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-backfills",
"main_header": "Backfill Tasks",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-schedule-resources",
"text": "Maximum Resource Efficiency Using Schedules"
},
"links": [],
"text": "AWS Data Pipeline allows you to maximize the efficiency of resources by supporting different schedule periods for a resource and an associated activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-schedule-resources",
"main_header": "Maximum Resource Efficiency Using Schedules",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-schedule-resources",
"text": "Maximum Resource Efficiency Using Schedules"
},
"links": [],
"text": "For example, consider an activity with a 20-minute schedule period. If the activity's resource were also configured for a 20-minute schedule period, AWS Data Pipeline would create three instances of the resource in an hour and consume triple the resources necessary for the task.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-schedule-resources",
"main_header": "Maximum Resource Efficiency Using Schedules",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-concepts-schedule-resources",
"text": "Maximum Resource Efficiency Using Schedules"
},
"links": [],
"text": "Instead, AWS Data Pipeline lets you configure the resource with a different schedule; for example, a one-hour schedule. When paired with an activity on a 20-minute schedule, AWS Data Pipeline creates only one resource to service all three instances of the activity in an hour, thus maximizing usage of the resource.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-schedule-resources",
"main_header": "Maximum Resource Efficiency Using Schedules",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-overwrite-data",
"text": "Protecting Against Overwriting Data"
},
"links": [],
"text": "Consider a recurring import job using AWS Data Pipeline that runs multiple times per day and routes the output to the same Amazon S3 location for each run.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-overwrite-data",
"main_header": "Protecting Against Overwriting Data",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-schedules",
"text": "Scheduling Pipelines"
},
"h2": {
"urllink": "#dp-overwrite-data",
"text": "Protecting Against Overwriting Data"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"title": "Schedule"
}
],
"text": "You could accidentally overwrite your output data, unless you use a date-based expression. A date-based expression such as s3://myBucket/#{@scheduledStartTime} for your S3Output.DirectoryPath can specify a separate directory path for each period. For more information, see Schedule.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-overwrite-data",
"main_header": "Protecting Against Overwriting Data",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-creating-pipelines",
"text": "Creating a Pipeline"
},
"links": [],
"text": "AWS Data Pipeline provides several ways for you to create pipelines:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html#dp-creating-pipelines",
"main_header": "Creating a Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"title": "Creating a Pipeline Using Parametrized Templates"
}
],
"text": "The AWS Data Pipeline console provides several pre-configured pipeline definitions, known as templates. You can use templates to get started with AWS Data Pipeline quickly. You can also create templates with parametrized values. This allows you to specify pipeline objects with parameters and pre-defined attributes. You can then use a tool to create values for a specific purpose within the pipeline. This allows you to reuse pipeline definitions with different values. For more information, see Creating a Pipeline Using Parametrized Templates.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-console-templates",
"main_header": "Creating Pipelines Using Console Templates",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"h2": {
"urllink": "#dp-create-pipeline",
"text": "Initialize, Create, and Schedule a Pipeline"
},
"links": [],
"text": "The AWS Data Pipeline console Create Pipeline page allows you to create and schedule a pipeline easily.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-create-pipeline",
"main_header": "Initialize, Create, and Schedule a Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-templates",
"text": "Creating Pipelines Using Console Templates"
},
"h2": {
"urllink": "#dp-choose-templates",
"text": "Choose a Template"
},
"links": [],
"text": "When you choose a template, the pipeline create page populates with the parameters specified in the pipeline definition, such as custom Amazon S3 directory paths, Amazon EC2 key pair names, database connection strings, and so on. You can provide this information at pipeline creation and activation. The following templates available in the console are also available for download from the Amazon S3 bucket: s3://datapipeline-us-east-1/templates/.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-templates.html#dp-choose-templates",
"main_header": "Choose a Template",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-gettingstartedshell",
"text": "Getting Started Using ShellCommandActivity"
},
"links": [],
"text": "The Getting Started using ShellCommandActivity template runs a shell command script to count the number of GET requests in a log file. The output is written in a time-stamped Amazon S3 location on every scheduled run of the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html#dp-template-gettingstartedshell",
"main_header": "Getting Started Using ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-gettingstartedshell",
"text": "Getting Started Using ShellCommandActivity"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-gettingstartedshell.html#dp-template-gettingstartedshell",
"main_header": "Getting Started Using ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-runawscli",
"text": "Run AWS CLI Command"
},
"links": [],
"text": "This template runs a user-specified AWS CLI command at scheduled intervals.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-runawscli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-runawscli.html#dp-template-runawscli",
"main_header": "Run AWS CLI Command",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exportddbtos3",
"text": "Export DynamoDB Table to S3"
},
"links": [],
"text": "The Export DynamoDB table to S3 template schedules an Amazon EMR cluster to export data from a DynamoDB table to an Amazon S3 bucket. This template uses an Amazon EMR cluster, which is sized proportionally to the value of the throughput available to the DynamoDB table. Although you can increase IOPs on a table, this may incur additional costs while importing and exporting. Previously, export used a HiveActivity but now uses native MapReduce.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html#dp-template-exportddbtos3",
"main_header": "Export DynamoDB Table to S3",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exportddbtos3",
"text": "Export DynamoDB Table to S3"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html#dp-template-exportddbtos3",
"main_header": "Export DynamoDB Table to S3",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exportddbtos3",
"text": "Export DynamoDB Table to S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
}
],
"text": "For a tutorial, see Import and Export DynamoDB Data Using AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html#dp-template-exportddbtos3",
"main_header": "Export DynamoDB Table to S3",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exports3toddb",
"text": "Import DynamoDB Backup Data from S3"
},
"links": [],
"text": "The Import DynamoDB backup data from S3 template schedules an Amazon EMR cluster to load a previously created DynamoDB backup in Amazon S3 to a DynamoDB table. Existing items in the DynamoDB table are updated with those from the backup data and new items are added to the table. This template uses an Amazon EMR cluster, which is sized proportionally to the value of the throughput available to the DynamoDB table. Although you can increase IOPs on a table, this may incur additional costs while importing and exporting. Previously, import used a HiveActivity but now uses native MapReduce.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html#dp-template-exports3toddb",
"main_header": "Import DynamoDB Backup Data from S3",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exports3toddb",
"text": "Import DynamoDB Backup Data from S3"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html#dp-template-exports3toddb",
"main_header": "Import DynamoDB Backup Data from S3",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-exports3toddb",
"text": "Import DynamoDB Backup Data from S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
}
],
"text": "For a tutorial, see Import and Export DynamoDB Data Using AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exports3toddb.html#dp-template-exports3toddb",
"main_header": "Import DynamoDB Backup Data from S3",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-emr",
"text": "Run Job on an Amazon EMR Cluster"
},
"links": [],
"text": "The Run Job on an Elastic MapReduce Cluster template launches an Amazon EMR cluster based on the parameters provided and starts running steps based on the specified schedule. Once the job completes, the EMR cluster is terminated. Optional bootstrap actions can be specified to install additional software or to change application configuration on the cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html#dp-template-emr",
"main_header": "Run Job on an Amazon EMR Cluster",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-emr",
"text": "Run Job on an Amazon EMR Cluster"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-emr.html#dp-template-emr",
"main_header": "Run Job on an Amazon EMR Cluster",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copyrdstos3",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [],
"text": "The Full Copy of RDS MySQL Table to S3 template copies an entire Amazon RDS MySQL table and stores the output in an Amazon S3 location. The output is stored as a CSV file in a timestamped subfolder under the specified Amazon S3 location.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html#dp-template-copyrdstos3",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copyrdstos3",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copyrdstos3.html#dp-template-copyrdstos3",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-incrementalcopyrdstos3",
"text": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [],
"text": "The Incremental Copy of RDS MySQL Table to S3 template does an incremental copy of the data from an Amazon RDS MySQL table and stores the output in an Amazon S3 location. The Amazon RDS MySQL table must have a Last Modified column.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html#dp-template-incrementalcopyrdstos3",
"main_header": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-incrementalcopyrdstos3",
"text": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html#dp-concepts-timeseries-cron",
"title": "time series"
}
],
"text": "This template copies changes that are made to the table between scheduled intervals starting from the scheduled start time. The schedule type is time series so if a copy was scheduled for a certain hour, AWS Data Pipeline copies the table rows that have a Last Modified time stamp that falls within the hour. Physical deletes to the table are not copied. The output is written in a timestamped subfolder under the Amazon S3 location on every scheduled run.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html#dp-template-incrementalcopyrdstos3",
"main_header": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-incrementalcopyrdstos3",
"text": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html#dp-template-incrementalcopyrdstos3",
"main_header": "Incremental Copy of Amazon RDS MySQL Table to Amazon S3",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copys3tords",
"text": "Load S3 Data into Amazon RDS MySQL Table"
},
"links": [],
"text": "The Load S3 Data into RDS MySQL Table template schedules an Amazon EC2 instance to copy the CSV file from the Amazon S3 file path specified below to an Amazon RDS MySQL table. The CSV file should not have a header row. The template updates existing entries in the Amazon RDS MySQL table with those in the Amazon S3 data and adds new entries from the Amazon S3 data to the Amazon RDS MySQL table. You can load the data into an existing table or provide an SQL query to create a new table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html#dp-template-copys3tords",
"main_header": "Load S3 Data into Amazon RDS MySQL Table",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-copys3tords",
"text": "Load S3 Data into Amazon RDS MySQL Table"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-copys3tords.html#dp-template-copys3tords",
"main_header": "Load S3 Data into Amazon RDS MySQL Table",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "The following two templates copy tables from Amazon RDS MySQL to Amazon Redshift using a translation script, which creates an Amazon Redshift table using the source table schema with the following caveats:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "If the Overwrite_Existing Amazon Redshift insert mode is being used:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "For more information about Amazon Redshift, see the following topics:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshift",
"text": "Amazon RDS to Amazon Redshift Templates"
},
"links": [],
"text": "The following table describes how the script translates the data types:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html#dp-template-redshift",
"main_header": "Amazon RDS to Amazon Redshift Templates",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsfull",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "The Full copy of Amazon RDS MySQL table to Amazon Redshift template copies the entire Amazon RDS MySQL table to an Amazon Redshift table by staging data in an Amazon S3 folder. The Amazon S3 staging folder must be in the same region as the Amazon Redshift cluster. An Amazon Redshift table is created with the same schema as the source Amazon RDS MySQL table if it does not already exist. Please provide any Amazon RDS MySQL to Amazon Redshift column data type overrides you would like to apply during Amazon Redshift table creation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html#dp-template-redshiftrdsfull",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsfull",
"text": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsfull.html#dp-template-redshiftrdsfull",
"main_header": "Full Copy of Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "The Incremental copy of Amazon RDS MySQL table to Amazon Redshift template copies data from an Amazon RDS MySQL table to an Amazon Redshift table by staging data in an Amazon S3 folder.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "The Amazon S3 staging folder must be in the same region as the Amazon Redshift cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "AWS Data Pipeline uses a translation script to create an Amazon Redshift table with the same schema as the source Amazon RDS MySQL table if it does not already exist. You must provide any Amazon RDS MySQL to Amazon Redshift column data type overrides you would like to apply during Amazon Redshift table creation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "This template copies changes that are made to the Amazon RDS MySQL table between scheduled intervals, starting from the scheduled start time. Physical deletes to the Amazon RDS MySQL table are not copied. You must provide the column name that stores the last modified time value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "When you use the default template to create pipelines for incremental Amazon RDS copy,  an activity with the default name RDSToS3CopyActivity is created. You can rename it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-redshiftrdsincremental",
"text": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html#dp-template-redshiftrdsincremental",
"main_header": "Incremental Copy of an Amazon RDS MySQL Table to Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-s3redshift",
"text": "Load Data from Amazon S3 into Amazon Redshift"
},
"links": [],
"text": "The Load data from S3 into Redshift template copies data from an Amazon S3 folder into an Amazon Redshift table. You can load the data into an existing table or provide a SQL query to create the table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html#dp-template-s3redshift",
"main_header": "Load Data from Amazon S3 into Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-s3redshift",
"text": "Load Data from Amazon S3 into Amazon Redshift"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html",
"title": "COPY"
}
],
"text": "The data is copied based on the Amazon Redshift COPY options. The Amazon Redshift table must have the same schema as the data in Amazon S3. For COPY options, see COPY in the Amazon Redshift Database Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html#dp-template-s3redshift",
"main_header": "Load Data from Amazon S3 into Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-template-s3redshift",
"text": "Load Data from Amazon S3 into Amazon Redshift"
},
"links": [],
"text": "The template uses the following pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html#dp-template-s3redshift",
"main_header": "Load Data from Amazon S3 into Amazon Redshift",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"links": [],
"text": "You can use a parametrized template to customize a pipeline definition. This enables you to create a common pipeline definition but provide different parameters when you add the pipeline definition to a new pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#dp-custom-templates",
"main_header": "Creating a Pipeline Using Parametrized Templates",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#add-pipeline-variables",
"text": "Add myVariables to the Pipeline Definition"
},
"links": [],
"text": "When you create the pipeline definition file, specify variables using the following syntax: #{myVariable}. It is required that the variable is prefixed by my. For example, the following pipeline definition file, pipeline-definition.json, includes the following variables: myShellCmd, myS3InputLoc, and myS3OutputLoc.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables",
"main_header": "Add myVariables to the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "You can create a separate file with parameter objects that defines the variables in your pipeline definition. For example, the following JSON file, parameters.json, contains parameter objects for the myShellCmd, myS3InputLoc, and myS3OutputLoc variables from the example pipeline definition above.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-objects",
"text": "Define Parameter Objects"
},
"links": [],
"text": "The following table describes the attributes for parameter objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-objects",
"main_header": "Define Parameter Objects",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#define-pipeline-parameter-values",
"text": "Define Parameter Values"
},
"links": [],
"text": "You can create a separate file to define your variables using parameter values. For example, the following JSON file, file://values.json, contains the value for myS3OutputLoc variable from the example pipeline definition above.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#define-pipeline-parameter-values",
"main_header": "Define Parameter Values",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-templates",
"text": "Creating a Pipeline Using Parametrized Templates"
},
"h2": {
"urllink": "#submit-pipeline-definition",
"text": "Submitting the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/put-pipeline-definition.html",
"title": "put-pipeline-definition"
}
],
"text": "When you submit your pipeline definition, you can specify parameters, parameter objects, and parameter values. For example, you can use the put-pipeline-definition AWS CLI command as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#submit-pipeline-definition",
"main_header": "Submitting the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"links": [],
"text": "You can create a pipeline using the AWS Data Pipeline architect rather than starting with a template. The example pipeline that you create in this section demonstrates using the architect to create a pipeline that copies files from one Amazon S3 bucket to another on a schedule that you specify.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual",
"main_header": "Creating Pipelines Using the Console Manually",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual",
"main_header": "Creating Pipelines Using the Console Manually",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html",
"title": "Create a Bucket"
}
],
"text": "You must have an Amazon S3 location where the file that you copy is located and a destination Amazon S3 location to copy the file to. For more information, see Create a Bucket in the Amazon Simple Storage Service User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual",
"main_header": "Creating Pipelines Using the Console Manually",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-create",
"text": "Create the Pipeline Definition"
},
"links": [],
"text": "Complete the initial pipeline creation screen to create the pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-create",
"main_header": "Create the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"links": [],
"text": "The AWS Data Pipeline Architect allows you to select predefined activities to add to a pipeline. The architect creates a graphical representation of the pipeline flow as you define activities and the resources associated with an activity, such as data nodes, schedules, resources, and so on. A data pipeline can comprise multiple activities.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activity",
"main_header": "Define an Activity Using the AWS Data Pipeline Architect",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"links": [],
"text": "In the following procedure, you add and configure a CopyActivity that copies data between two Amazon S3 locations. You specify one Amazon S3 location as the source DataNode from which to copy and another Amazon S3 location as the destination DataNode. You also configure the schedule for the activity to run and the AWS resource that the activity uses to run.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activity",
"main_header": "Define an Activity Using the AWS Data Pipeline Architect",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-schedule",
"text": "Configure the Schedule"
},
"links": [],
"text": "Configure the date and time for your pipeline to run. AWS Data Pipeline supports the date and time expressed in \"YYYY-MM-DDTHH:MM:SS\" format in UTC/GMT only.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-schedule",
"main_header": "Configure the Schedule",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-data",
"text": "Configure Data Nodes"
},
"links": [],
"text": "In this step, you configure the data nodes that you created and specified as Input and Output when you configured the copy activity. After you create the data nodes, other activities that you might add to the pipeline can also use them.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-data",
"main_header": "Configure Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activity",
"text": "Define an Activity Using the AWS Data Pipeline Architect"
},
"h3": {
"urllink": "#dp-console-manual-resources",
"text": "Configure Resources"
},
"links": [],
"text": "In this step, you configure the resource that AWS Data Pipeline uses to perform the copy activity, which you specified as the Runs On resource when you configured the activity. The copy activity uses an Amazon EC2 instance.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-resources",
"main_header": "Configure Resources",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-validate",
"text": "Validate and Save the Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-validate",
"main_header": "Validate and Save the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-console-manual",
"text": "Creating Pipelines Using the Console Manually"
},
"h2": {
"urllink": "#dp-console-manual-activate",
"text": "Activate the Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-console-manual.html#dp-console-manual-activate",
"main_header": "Activate the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-list-pipelines",
"text": "Viewing Your Pipelines"
},
"links": [],
"text": "You can view your pipelines using the console or the command line interface (CLI).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-list-pipelines.html#dp-list-pipelines",
"main_header": "Viewing Your Pipelines",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-interpret-status",
"text": "Interpreting Pipeline Status Codes"
},
"links": [],
"text": "The status levels displayed in the AWS Data Pipeline console and CLI indicate the condition of a pipeline and its components. The pipeline status is simply an overview of a pipeline; to see more information, view the status of individual pipeline components.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html#dp-interpret-status",
"main_header": "Interpreting Pipeline Status Codes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-status",
"text": "Interpreting Pipeline Status Codes"
},
"links": [],
"text": "A pipeline has a SCHEDULED status if it is ready (the pipeline definition passed validation), currently performing work, or finished performing work. A pipeline has a PENDING status if it is not activated or not able to perform work (for example, the pipeline definition failed validation.)",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html#dp-interpret-status",
"main_header": "Interpreting Pipeline Status Codes",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-status",
"text": "Interpreting Pipeline Status Codes"
},
"links": [
{
"url": "https://aws.amazon.com/datapipeline/pricing",
"title": "Pricing"
}
],
"text": "A pipeline is considered inactive if its status is PENDING, INACTIVE, or FINISHED. Inactive pipelines incur a charge (for more information, see Pricing).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-status.html#dp-interpret-status",
"main_header": "Interpreting Pipeline Status Codes",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-interpret-health-status",
"text": "Interpreting Pipeline and Component Health State"
},
"links": [],
"text": "Each pipeline and component within that pipeline returns a health status of HEALTHY, ERROR, \"-\", No Completed Executions, or No Health Information Available. A pipeline only has a health state after a pipeline component has completed its first execution or if component preconditions have failed. The health status for components aggregates into a pipeline health status in that error states are visible first when you view your pipeline execution details.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-interpret-health-status.html#dp-interpret-health-status",
"main_header": "Interpreting Pipeline and Component Health State",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"title": "Pipeline Definition File Syntax"
}
],
"text": "Use the AWS Data Pipeline console or the command line interface (CLI) to view your pipeline definition. The console shows a graphical representation, while the CLI prints a pipeline definition file, in JSON format. For information about the syntax and usage of pipeline definition files, see Pipeline Definition File Syntax.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "If you are using the CLI, it's a good idea to retrieve the pipeline definition before you submit modifications, because it's possible that another user or process changed the pipeline definition after you last worked with it. By downloading a copy of the current definition and using that as the basis for your modifications, you can be sure that you are working with the most recent pipeline definition. It's also a good idea to retrieve the pipeline definition again after you modify it, so that you can ensure that the update was successful.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "If you are using the CLI, you can get two different versions of your pipeline. The active version is the pipeline that is currently running. The latest version is a copy that's created when you edit a running pipeline. When you upload the edited pipeline, it becomes the active version and the previous active version is no longer available.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "To get a pipeline definition using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/get-pipeline-definition.html",
"title": "get-pipeline-definition"
}
],
"text": "To get the complete pipeline definition, use the get-pipeline-definition command. The pipeline definition is printed to standard output (stdout).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "The following example gets the pipeline definition for the specified pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-view-definition",
"text": "Viewing Your Pipeline Definitions"
},
"links": [],
"text": "To retrieve a specific version of a pipeline, use the --version option. The following example retrieves the active version of the specified pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-view-definition.html#dp-view-definition",
"main_header": "Viewing Your Pipeline Definitions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html",
"title": "Interpreting Pipeline Status Details"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"title": "Resolving Common Problems"
}
],
"text": "You can monitor the progress of your pipeline. For more information about instance status, see Interpreting Pipeline Status Details. For more information about troubleshooting failed or incomplete instance runs of your pipeline, see Resolving Common Problems.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "To monitor the progress of a pipeline using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/list-runs.html",
"title": "list-runs"
}
],
"text": "To retrieve pipeline instance details, such as a history of the times that a pipeline has run, use the list-runs command. This command enables you to filter the list of runs returned based on either their current status or the date-range in which they were launched. Filtering the results is useful because, depending on the pipeline's age and scheduling, the run history can be large.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "The following example retrieves information for all runs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "The following example retrieves information for all runs that have completed.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-details-console",
"text": "Viewing Pipeline Instance Details"
},
"links": [],
"text": "The following example retrieves information for all runs launched in the specified time frame.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-details-console.html#dp-manage-pipeline-details-console",
"main_header": "Viewing Pipeline Instance Details",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "Pipeline-level logging is supported at pipeline creation by specifying an Amazon S3 location in either the console or with a pipelineLogUri in the default object in SDK/CLI. The directory structure for each pipeline within that URI is like the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "For pipeline, df-00123456ABC7DEF8HIJK, the directory structure looks like:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "For ShellCommandActivity, logs for stderr and stdout associated with these activities are stored in the directory for each attempt.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "For resources like, EmrCluster, where an emrLogUri is set, that value takes precedence. Otherwise, resources (including TaskRunner logs for those resources) follow the above pipeline logging structure. You may view these logs for each component in the Execution Details page for your pipeline by viewing a component's details and clicking on the link for logs:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-viewing-logs",
"text": "Viewing Pipeline Logs"
},
"links": [],
"text": "You can also view logs for each attempt. For example, to view logs for a HadoopActivity, you can click the pipeline Attempts tab for your activity. Hadoop Logs gives the logs created by Hadoop jobs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-viewing-logs.html#dp-viewing-logs",
"main_header": "Viewing Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"links": [],
"text": "To change some aspect of one of your pipelines, you can update its pipeline definition. After you change a pipeline that is running, you must re-activate the pipeline for your changes to take effect. In addition, you can re-run one or more pipeline components.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-manage-pipeline-modify-console",
"main_header": "Editing Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-limits",
"text": "Limitations"
},
"links": [],
"text": "While the pipeline is in the PENDING state and is not activated, you can't make any changes to it. After you activate a pipeline, you can edit the pipeline with the following restrictions. The changes you make apply to new runs of the pipeline objects after you save them and then activate the pipeline again.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-limits",
"main_header": "Limitations",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-console",
"text": "Editing a Pipeline Using the Console"
},
"links": [],
"text": "You can edit a pipeline using the AWS Management Console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-console",
"main_header": "Editing a Pipeline Using the Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-aws-cli",
"text": "Editing a Pipeline Using the AWS CLI"
},
"links": [],
"text": "You can edit a pipeline using the command line tools.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"main_header": "Editing a Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-aws-cli",
"text": "Editing a Pipeline Using the AWS CLI"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/get-pipeline-definition.html",
"title": "get-pipeline-definition"
}
],
"text": "First, download a copy of the current pipeline definition using the get-pipeline-definition command. By doing this, you can be sure that you are modifying the most recent pipeline definition. The following example uses prints the pipeline definition to standard output (stdout).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"main_header": "Editing a Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-aws-cli",
"text": "Editing a Pipeline Using the AWS CLI"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/put-pipeline-definition.html",
"title": "put-pipeline-definition"
}
],
"text": "Save the pipeline definition to a file and edit it as needed. Update your pipeline definition using the put-pipeline-definition command. The following example uploads the updated pipeline definition file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"main_header": "Editing a Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-aws-cli",
"text": "Editing a Pipeline Using the AWS CLI"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/activate-pipeline.html",
"title": "activate-pipeline"
}
],
"text": "You can retrieve the pipeline definition again using the get-pipeline-definition command to ensure that the update was successful. To activate the pipeline, use the following activate-pipeline command:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"main_header": "Editing a Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-aws-cli",
"text": "Editing a Pipeline Using the AWS CLI"
},
"links": [],
"text": "If you prefer, you can activate the pipeline from a specific date and time, using the --start-timestamp option as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"main_header": "Editing a Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-modify-console",
"text": "Editing Your Pipeline"
},
"h2": {
"urllink": "#dp-edit-pipeline-aws-cli",
"text": "Editing a Pipeline Using the AWS CLI"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/set-status.html",
"title": "set-status"
}
],
"text": "To re-run one or more pipeline components, use the set-status command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-modify-console.html#dp-edit-pipeline-aws-cli",
"main_header": "Editing a Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-clone-console",
"text": "Cloning Your Pipeline"
},
"links": [],
"text": "Cloning makes a copy of a pipeline and allows you to specify a name for the new pipeline. You can clone a pipeline that is in any state, even if it has errors; however, the new pipeline remains in the PENDING state until you manually activate it. For the new pipeline, the clone operation uses the latest version of the original pipeline definition rather than the active version. In the clone operation, the full schedule from the original pipeline is not copied into the new pipeline, only the period setting.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-clone-console.html#dp-manage-pipeline-clone-console",
"main_header": "Cloning Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [],
"text": "Tags are case-sensitive key-value pairs that consist of a key and an optional value, both defined by the user. You can apply up to ten tags to each pipeline. Tag keys must be unique for each pipeline. If you add a tag with a key that is already associated with the pipeline, it updates the value of that tag.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [],
"text": "Applying a tag to a pipeline also propagates the tags to its underlying resources (for example, Amazon EMR clusters and Amazon EC2 instances). However, it does not apply these tags to resources in a FINISHED or otherwise terminated state. You can use the CLI to apply tags to these resources, if needed.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [],
"text": "When you are finished with a tag, you can remove it from your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [],
"text": "To tag your pipeline using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/create-pipeline.html",
"title": "create-pipeline"
}
],
"text": "To add tags to a new pipeline, add the --tags option to your create-pipeline command. For example, the following option creates a pipeline with two tags, an environment tag with a value of production, and an owner tag with a value of sales.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/add-tags.html",
"title": "add-tags"
}
],
"text": "To add tags to an existing pipeline, use the add-tags command as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-adding-tags",
"text": "Tagging Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/remove-tags.html",
"title": "remove-tags"
}
],
"text": "To remove tags from an existing pipeline, use the remove-tags command as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-adding-tags.html#dp-adding-tags",
"main_header": "Tagging Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"links": [],
"text": "Deactivating a running pipeline pauses the pipeline execution. To resume pipeline execution, you can activate the pipeline. This enables you to make changes. For example, if you are writing data to a database that is scheduled to undergo maintenance, you can deactivate the pipeline, wait for the maintenance to complete, and then activate the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline",
"main_header": "Deactivating Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"links": [],
"text": "When you deactivate a pipeline, you can specify what happens to running activities. By default, these activities are canceled immediately. Alternatively, you can have AWS Data Pipeline wait until the activities finish before deactivating the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline",
"main_header": "Deactivating Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"links": [],
"text": "When you activate a deactivated pipeline, you can specify when it resumes. For example, using the AWS Management Console, you can resume after the last completed run, from the current time, or from a specified date and time. Using the AWS CLI or the API, the pipeline resumes from the last completed execution by default, or you can specify the date and time to resume the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline",
"main_header": "Deactivating Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-console",
"text": "Deactivate Your Pipeline Using the Console"
},
"links": [],
"text": "Use the following procedure to deactivate a running pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"main_header": "Deactivate Your Pipeline Using the Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-console",
"text": "Deactivate Your Pipeline Using the Console"
},
"links": [],
"text": "When you are ready to resume the pipeline runs, use the following procedure to activate the deactivated pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-console",
"main_header": "Deactivate Your Pipeline Using the Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-cli",
"text": "Deactivate Your Pipeline Using the AWS CLI"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/deactivate-pipeline.html",
"title": "deactivate-pipeline"
}
],
"text": "Use the following deactivate-pipeline command to deactivate a pipeline:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-cli",
"main_header": "Deactivate Your Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-cli",
"text": "Deactivate Your Pipeline Using the AWS CLI"
},
"links": [],
"text": "To deactivate the pipeline only after all running activities finish, add the --no-cancel-active option, as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-cli",
"main_header": "Deactivate Your Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-cli",
"text": "Deactivate Your Pipeline Using the AWS CLI"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/activate-pipeline.html",
"title": "activate-pipeline"
}
],
"text": "When you are ready, you can resume the pipeline execution where it left off using the following activate-pipeline command:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-cli",
"main_header": "Deactivate Your Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-deactivate-pipeline",
"text": "Deactivating Your Pipeline"
},
"h2": {
"urllink": "#dp-deactivate-pipeline-cli",
"text": "Deactivate Your Pipeline Using the AWS CLI"
},
"links": [],
"text": "To start the pipeline from a specific date and time, add the --start-timestamp option, as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-deactivate-pipeline.html#dp-deactivate-pipeline-cli",
"main_header": "Deactivate Your Pipeline Using the AWS CLI",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "When you no longer require a pipeline, such as a pipeline created during application testing, you should delete it to remove it from active use. Deleting a pipeline puts it into a deleting state. When the pipeline is in the deleted state, its pipeline definition and run history are gone. Therefore, you can no longer perform operations on the pipeline, including describing it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [],
"text": "To delete a pipeline using the AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-pipeline-delete-console",
"text": "Deleting Your Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/delete-pipeline.html",
"title": "delete-pipeline"
}
],
"text": "To delete a pipeline, use the delete-pipeline command. The following command deletes the specified pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-pipeline-delete-console.html#dp-manage-pipeline-delete-console",
"main_header": "Deleting Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "AWS Data Pipeline can stage input and output data in your pipelines to make it easier to use certain activities, such as ShellCommandActivity and HiveActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "Data staging enables you to copy data from the input data node to the resource executing the activity, and, similarly, from the resource to the output data node.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "The staged data on the Amazon EMR or Amazon EC2 resource is available by using special variables in the activity's shell commands or Hive scripts.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "Table staging is similar to data staging, except the staged data takes the form of database tables, specifically.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "AWS Data Pipeline supports the following staging scenarios:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"links": [],
"text": "In addition, data nodes and activities can relate in four ways:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-staging",
"main_header": "Staging Data and Tables with Pipeline Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-datastaging",
"text": "Data Staging with ShellCommandActivity"
},
"links": [],
"text": "Consider a scenario using a ShellCommandActivity with S3DataNode objects as data input and output. AWS Data Pipeline automatically stages the data nodes to make them accessible to the shell command as if they were local file folders using the environment variables ${INPUT1_STAGING_DIR} and ${OUTPUT1_STAGING_DIR} as shown in the following example. The numeric portion of the variables named INPUT1_STAGING_DIR and OUTPUT1_STAGING_DIR increment depending on the number of data nodes your activity references.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-datastaging",
"main_header": "Data Staging with ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-tablestaging",
"text": "Table Staging with Hive and Staging-Supported Data Nodes"
},
"links": [],
"text": "Consider a scenario using a HiveActivity with S3DataNode objects as data input and output. AWS Data Pipeline automatically stages the data nodes to make them accessible to the Hive script as if they were Hive tables using the variables ${input1} and ${output1} as shown in the following example for HiveActivity. The numeric portion of the variables named input and output increment depending on the number of data nodes your activity references.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-tablestaging",
"main_header": "Table Staging with Hive and Staging-Supported Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-concepts-staging",
"text": "Staging Data and Tables with Pipeline Activities"
},
"h2": {
"urllink": "#dp-concepts-nostaging",
"text": "Table Staging with Hive and Staging-Unsupported Data Nodes"
},
"links": [],
"text": "Consider a scenario using a HiveActivity with DynamoDBDataNode as data input and an S3DataNode object as the output. No data staging is available for DynamoDBDataNode, therefore you must first manually create the table within your hive script, using the variable name #{input.tableName} to refer to the DynamoDB table. Similar nomenclature applies if the DynamoDB table is the output, except you use variable #{output.tableName}. Staging is available for the output S3DataNode object in this example, therefore you can refer to the output data node as ${output1}.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-staging.html#dp-concepts-nostaging",
"main_header": "Table Staging with Hive and Staging-Unsupported Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/",
"title": "Amazon VPC User Guide"
}
],
"text": "Pipelines launch Amazon EC2 instances and Amazon EMR clusters into an Amazon Virtual Private Cloud(Amazon VPC). AWS accounts created after 2013-12-04 each have default VPCs created for each region. The default configuration of the default VPC supports AWS Data Pipeline resources. You can use this VPC or create custom VPCs and use those. For production environments, we recommend that you create custom VPCs because it allows you greater control over network configurations. For more information, see Amazon VPC User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [],
"text": "The steps to configure a VPC that AWS Data Pipeline can use are listed below:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/",
"title": "Amazon VPC User Guide"
}
],
"text": "For more information about VPCs, see the Amazon VPC User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-resources-vpc",
"main_header": "Launching Resources for Your Pipeline into a VPC",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-create-vpc",
"text": "Create and Configure a VPC"
},
"links": [],
"text": "A VPC that you create must have a subnet, an internet gateway, and a route table for the subnet with a route to the internet gateway so that instances in the VPC can access Amazon S3. If you have a default VPC, it is already configured this way. The easiest way to create and configure your VPC is to use the VPC wizard, as shown in the following procedure.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-create-vpc",
"main_header": "Create and Configure a VPC",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-create-vpc",
"text": "Create and Configure a VPC"
},
"links": [
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html#Create-VPC",
"title": "Creating a VPC"
},
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html#Create-VPC",
"title": "Adding an Internet Gateway to Your VPC"
}
],
"text": "As an alternative to using the Amazon VPC Wizard, you can create a VPC, subnet, internet gateway, and route table manually, see Creating a VPC and Adding an Internet Gateway to Your VPC in the Amazon VPC User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-create-vpc",
"main_header": "Create and Configure a VPC",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [],
"text": "Security groups act as a virtual firewall for your instances to control inbound and outbound traffic. You must grant Task Runner access to your data sources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
"title": "Security Groups for Your VPC"
}
],
"text": "For more information about security groups, see Security Groups for Your VPC in the Amazon VPC User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [],
"text": "First, identify the security group or IP address used by the resource running Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-vpc-security-groups",
"text": "Set up Connectivity Between Resources"
},
"links": [],
"text": "Next, create rules in the resource security groups that allow inbound traffic for the data sources Task Runner must access. For example, if Task Runner must access an Amazon Redshift cluster, the security group for the Amazon Redshift cluster must allow inbound traffic from the resource.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-vpc-security-groups",
"main_header": "Set up Connectivity Between Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-configure-resource",
"text": "Configure the Resource"
},
"links": [],
"text": "To launch a resource into a subnet of a nondefault VPC or a nondefault subnet of a default VPC, you must specify the subnet using the subnetId field when you configure the resource. If you have a default VPC and you don't specify subnetId, we launch the resource into the default subnet of the default VPC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-configure-resource",
"main_header": "Configure the Resource",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-configure-resource",
"text": "Configure the Resource"
},
"h3": {
"urllink": "#dp-emrcluster",
"text": "Example EmrCluster"
},
"links": [],
"text": "The following example object launches an Amazon EMR cluster into a nondefault VPC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-emrcluster",
"main_header": "Example EmrCluster",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-configure-resource",
"text": "Configure the Resource"
},
"h3": {
"urllink": "#dp-emrcluster",
"text": "Example EmrCluster"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
],
"text": "For more information, see EmrCluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-emrcluster",
"main_header": "Example EmrCluster",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-configure-resource",
"text": "Configure the Resource"
},
"h3": {
"urllink": "#dp-ec2resource",
"text": "Example Ec2Resource"
},
"links": [],
"text": "The following example object launches an EC2 instance into a nondefault VPC. Notice that you must specify security groups for an instance in a nondefault VPC using their IDs, not their names.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2resource",
"main_header": "Example Ec2Resource",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-configure-resource",
"text": "Configure the Resource"
},
"h3": {
"urllink": "#dp-ec2resource",
"text": "Example Ec2Resource"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
}
],
"text": "For more information, see Ec2Resource.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2resource",
"main_header": "Example Ec2Resource",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-ec2classic-to-vpc",
"text": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
},
"links": [],
"text": "If you have pipeline resources in EC2-Classic, we recommend that you migrate them to use Amazon VPC. Use the following steps as guidance to migrate resources from EC2-Classic to VPC in a pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"main_header": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-resources-vpc",
"text": "Launching Resources for Your Pipeline into a VPC"
},
"h2": {
"urllink": "#dp-ec2classic-to-vpc",
"text": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs"
},
"links": [],
"text": "To confirm that a pipeline's resources migrated to VPC successfully, you can verify that the EC2 instances launched during pipeline execution launched in the VPC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html#dp-ec2classic-to-vpc",
"main_header": "Migrating EC2Resource Objects in a Pipeline from EC2-Classic to VPCs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-spot-instances",
"text": "Using Amazon EC2 Spot Instances in a Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/ec2/spot-instances/",
"title": "Amazon EC2 Spot Instances"
}
],
"text": "Pipelines can use Amazon EC2 Spot Instances for the task nodes in their Amazon EMR cluster resources. By default, pipelines use on-demand Amazon EC2 instances. In addition, you can use Spot Instances. Spot Instances let you use spare Amazon EC2 instances and run them. The Spot Instance pricing model complements the on-demand and Reserved Instance pricing models, potentially providing the most cost-effective option for obtaining compute capacity, depending on your application. For more information, see Amazon EC2 Spot Instances on the Amazon EC2 Product Page.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html#dp-spot-instances",
"main_header": "Using Amazon EC2 Spot Instances in a Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-spot-instances",
"text": "Using Amazon EC2 Spot Instances in a Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
],
"text": "For more information, see EmrCluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html#dp-spot-instances",
"main_header": "Using Amazon EC2 Spot Instances in a Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "By default, the Ec2Resource and EmrCluster resources run in the same region as AWS Data Pipeline, however AWS Data Pipeline supports the ability to orchestrate data flows across multiple regions, such as running resources in one region that consolidate input data from another region. By allowing resources to run a specified region, you also have the flexibility to co-locate your resources with their dependent datasets and maximize performance by reducing latencies and avoiding cross-region data transfer charges. You can configure resources to run in a different region than AWS Data Pipeline by using the region field on Ec2Resource and EmrCluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "The following example pipeline JSON file shows how to run an EmrCluster resource in the Europe (Ireland) region, assuming that a large amount of data for the cluster to work on exists in the same region. In this example, the only difference from a typical pipeline is that the EmrCluster has a region field value set to eu-west-1.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-region",
"text": "Using a Pipeline with Resources in Multiple Regions"
},
"links": [],
"text": "The following table lists the regions that you can choose and the associated region codes to use in the region field.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html#dp-manage-region",
"main_header": "Using a Pipeline with Resources in Multiple Regions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"links": [],
"text": "AWS Data Pipeline allows you to configure the way pipeline objects behave when a dependency fails or is canceled by a user. You can ensure that failures cascade to other pipeline objects (consumers), to prevent indefinite waiting. All activities, data nodes, and preconditions have a field named failureAndRerunMode with a default value of none. To enable cascading failures, set the failureAndRerunMode field to cascade.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-failandrerun",
"main_header": "Cascading Failures and Reruns",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"links": [],
"text": "When this field is enabled, cascade failures occur if a pipeline object is blocked in the WAITING_ON_DEPENDENCIES state and any dependencies have failed with no pending command. During a cascade failure, the following events occur:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-failandrerun",
"main_header": "Cascading Failures and Reruns",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"links": [],
"text": "Cascade failure does not operate on a failed object's dependencies (upstream), except for preconditions associated with the original failed object. Pipeline objects affected by a cascade failure may trigger any retries or post-actions, such as onFail.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-failandrerun",
"main_header": "Cascading Failures and Reruns",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"links": [],
"text": "The detailed effects of a cascading failure depend on the object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-failandrerun",
"main_header": "Cascading Failures and Reruns",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-activity",
"text": "Activities"
},
"links": [],
"text": "An activity changes to CASCADE_FAILED if any of its dependencies fail, and it subsequently triggers a cascade failure in the activity's consumers. If a resource fails that the activity depends on, the activity is CANCELED and all its consumers change to CASCADE_FAILED.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-activity",
"main_header": "Activities",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-datanode",
"text": "Data Nodes and Preconditions"
},
"links": [],
"text": "If a data node is configured as the output of an activity that fails, the data node changes to the CASCADE_FAILED state. The failure of a data node propagates to any associated preconditions, which change to the CANCELED state.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-datanode",
"main_header": "Data Nodes and Preconditions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-resources",
"text": "Resources"
},
"links": [],
"text": "If the objects that depend on a resource are in the FAILED state and the resource itself is in the WAITING_ON_DEPENDENCIES state, then the resource changes to the FINISHED state.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-resources",
"main_header": "Resources",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-rerun",
"text": "Rerunning Cascade-Failed Objects"
},
"links": [],
"text": "By default, rerunning any activity or data node only reruns the associated resource. However, setting the failureAndRerunMode field to cascade on a pipeline object allows a rerun command on a target object to propagate to all consumers, under the following conditions:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-rerun",
"main_header": "Rerunning Cascade-Failed Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-rerun",
"text": "Rerunning Cascade-Failed Objects"
},
"links": [],
"text": "If you attempt to rerun a CASCADE_FAILED object and any of its dependencies are FAILED, CASCADE_FAILED, or CANCELED, the rerun will fail and return the object to the CASCADE_FAILED state. To successfully rerun the failed object, you must trace the failure up the dependency chain to locate the original source of failure and rerun that object instead. When you issue a rerun command on a resource, you also attempt to rerun any objects that depend on it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-rerun",
"main_header": "Rerunning Cascade-Failed Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-manage-cascade-failandrerun",
"text": "Cascading Failures and Reruns"
},
"h2": {
"urllink": "#dp-manage-cascade-backfills",
"text": "Cascade-Failure and Backfills"
},
"links": [],
"text": "If you enable cascade failure and have a pipeline that creates many backfills, pipeline runtime errors can cause resources to be created and deleted in rapid succession without performing useful work. AWS Data Pipeline attempts to alert you about this situation with the following warning message when you save a pipeline: \nPipeline_object_name has 'failureAndRerunMode' field set to 'cascade' and you are about to create a backfill with scheduleStartTime start_time. This can result in rapid creation of pipeline objects in case of failures.  This happens because cascade failure can quickly set downstream activities as CASCADE_FAILED and shut down EMR clusters and EC2 resources that are no longer needed. We recommended that you test pipelines with short time ranges to limit the effects of this situation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-cascade-failandrerun.html#dp-manage-cascade-backfills",
"main_header": "Cascade-Failure and Backfills",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"links": [],
"text": "The instructions in this section are for working manually with pipeline definition files using the AWS Data Pipeline command line interface (CLI). This is an alternative to designing a pipeline interactively using the AWS Data Pipeline console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-writing-pipeline-definition",
"main_header": "Pipeline Definition File Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"links": [],
"text": "You can manually create pipeline definition files using any text editor that supports saving files using the UTF-8 file format, and submit the files using the AWS Data Pipeline command line interface.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-writing-pipeline-definition",
"main_header": "Pipeline Definition File Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"title": "Pipeline Expressions and Functions"
}
],
"text": "AWS Data Pipeline also supports a variety of complex expressions and functions within pipeline definitions. For more information, see Pipeline Expressions and Functions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-writing-pipeline-definition",
"main_header": "Pipeline Definition File Syntax",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-file-structure",
"text": "File Structure"
},
"links": [],
"text": "The first step in pipeline creation is to compose pipeline definition objects in a pipeline definition file. The following example illustrates the general structure of a pipeline definition file. This file defines two objects, which are delimited by '{' and '}', and separated by a comma.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-file-structure",
"main_header": "File Structure",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-file-structure",
"text": "File Structure"
},
"links": [],
"text": "In the following example, the first object defines two name-value pairs, known as fields. The second object defines three fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-file-structure",
"main_header": "File Structure",
"images": [],
"container_type": "p",
"children_tags": [
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-file-structure",
"text": "File Structure"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
],
"text": "When creating a pipeline definition file, you must select the types of pipeline objects that you need, add them to the pipeline definition file, and then add the appropriate fields. For more information about pipeline objects, see Pipeline Object Reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-file-structure",
"main_header": "File Structure",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-file-structure",
"text": "File Structure"
},
"links": [],
"text": "For example, you could create a pipeline definition object for an input data node and another for the output data node. Then create another pipeline definition object for an activity, such as processing the input data using Amazon EMR.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-file-structure",
"main_header": "File Structure",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [],
"text": "After you know which object types to include in your pipeline definition file, you add fields to the definition of each pipeline object. Field names are enclosed in quotes, and are separated from field values by a space, a colon, and a space, as shown in the following example.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-pipeline-datatypes",
"title": "Simple Data Types"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"title": "Expression Evaluation"
}
],
"text": "The field value can be a text string, a reference to another object, a function call, an expression, or an ordered list of any of the preceding types. For more information about the types of data that can be used for field values, see Simple Data Types . For more information about functions that you can use to evaluate field values, see Expression Evaluation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [],
"text": "Fields are limited to 2048 characters. Objects can be 20 KB in size, which means that you can't add many large fields to an object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
],
"text": "Each pipeline object must contain the following fields: id and type, as shown in the following example. Other fields may also be required based on the object type. Select a value for id that's meaningful to you, and is unique within the pipeline definition. The value for type specifies the type of the object. Specify one of the supported pipeline definition object types, which are listed in the topic Pipeline Object Reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [],
"text": "For more information about the required and optional fields for each object, see the documentation for the object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [],
"text": "To include fields from one object in another object, use the parent field with a reference to the object. For example, object \"B\" includes its fields, \"B1\" and \"B2\", plus the fields from object \"A\", \"A1\" and \"A2\".",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-add-fields",
"text": "Pipeline Fields"
},
"links": [],
"text": "You can define common fields in an object with the ID \"Default\". These fields are automatically included in every object in the pipeline definition file that doesn't explicitly set its parent field to reference a different object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-add-fields",
"main_header": "Pipeline Fields",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-userdefined-fields",
"text": "User-Defined Fields"
},
"links": [],
"text": "You can create user-defined or custom fields on your pipeline components and refer to them with expressions. The following example shows a custom field named myCustomField and my_customFieldReference added to an S3DataNode object:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"main_header": "User-Defined Fields",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-writing-pipeline-definition",
"text": "Pipeline Definition File Syntax"
},
"h2": {
"urllink": "#dp-userdefined-fields",
"text": "User-Defined Fields"
},
"links": [],
"text": "A user-defined field must have a name prefixed with the word \"my\" in all lower-case letters, followed by a capital letter or underscore character. Additionally, a user-defined field can be a string value such as the preceding myCustomField example, or a reference to another pipeline component such as the preceding my_customFieldReference example.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"main_header": "User-Defined Fields",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-set-up-aws-sdk",
"title": "Install the AWS SDK"
}
],
"text": "The easiest way to write applications that interact with AWS Data Pipeline or to implement a custom Task Runner is to use one of the AWS SDKs. The AWS SDKs provide functionality that simplifies calling the web service APIs from your preferred programming environment. For more information, see Install the AWS SDK.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-program-pipeline",
"main_header": "Working with the API",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"h2": {
"urllink": "#dp-set-up-aws-sdk",
"text": "Install the AWS SDK"
},
"links": [
{
"url": "http://aws.amazon.com/code",
"title": "Sample Code & Libraries"
}
],
"text": "The AWS SDKs provide functions that wrap the API and take care of many of the connection details, such as calculating signatures, handling request retries, and error handling. The SDKs also contain sample code, tutorials, and other resources to help you get started writing applications that call AWS. Calling the wrapper functions in an SDK can greatly simplify the process of writing an AWS application. For more information about how to download and use the AWS SDKs, go to Sample Code & Libraries.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-set-up-aws-sdk",
"main_header": "Install the AWS SDK",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-program-pipeline",
"text": "Working with the API"
},
"h2": {
"urllink": "#dp-set-up-aws-sdk",
"text": "Install the AWS SDK"
},
"links": [],
"text": "AWS Data Pipeline support is available in SDKs for the following platforms:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-program-pipeline.html#dp-set-up-aws-sdk",
"main_header": "Install the AWS SDK",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-managing-pipeline.html",
"title": "Working with Pipelines"
}
]
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/APIReference/Welcome.html",
"title": "AWS Data Pipeline API Reference"
}
],
"text": "For a complete description of the programmatic objects in AWS Data Pipeline, see the AWS Data Pipeline API Reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-make-http-request",
"main_header": "Making an HTTP Request to AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"links": [],
"text": "If you don't use one of the AWS SDKs, you can perform AWS Data Pipeline operations over HTTP using the POST request method. The POST method requires you to specify the operation in the header of the request and provide the data for the operation in JSON format in the body of the request.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-make-http-request",
"main_header": "Making an HTTP Request to AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-header",
"text": "HTTP Header Contents"
},
"links": [],
"text": "AWS Data Pipeline requires the following information in the header of an HTTP request:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-http-header",
"main_header": "HTTP Header Contents",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-header",
"text": "HTTP Header Contents"
},
"links": [],
"text": "The following is an example header for an HTTP request to activate a pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-http-header",
"main_header": "HTTP Header Contents",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-body-content",
"text": "HTTP Body Content"
},
"links": [],
"text": "The body of an HTTP request contains the data for the operation specified in the header of the HTTP request. The data must be formatted according to the JSON data schema for each AWS Data Pipeline API. The AWS Data Pipeline JSON data schema defines the types of data and parameters (such as comparison operators and enumeration constants) available for each operation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-http-body-content",
"main_header": "HTTP Body Content",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-body-content",
"text": "HTTP Body Content"
},
"h3": {
"urllink": "#dp-format-http-body",
"text": "Format the Body of an HTTP Request"
},
"links": [],
"text": "Use the JSON data format to convey data values and data structure, simultaneously. Elements can be nested within other elements by using bracket notation. The following example shows a request for putting a pipeline definition consisting of three objects and their corresponding slots.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-format-http-body",
"main_header": "Format the Body of an HTTP Request",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-body-content",
"text": "HTTP Body Content"
},
"h3": {
"urllink": "#dp-handle-http-responses",
"text": "Handle the HTTP Response"
},
"links": [],
"text": "Here are some important headers in the HTTP response, and how you should handle them in your application:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-handle-http-responses",
"main_header": "Handle the HTTP Response",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-body-content",
"text": "HTTP Body Content"
},
"h3": {
"urllink": "#dp-handle-http-responses",
"text": "Handle the HTTP Response"
},
"links": [],
"text": "AWS SDK users do not need to manually perform this verification, because the SDKs compute the checksum of each reply from Amazon DynamoDB and automatically retry if a mismatch is detected.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-handle-http-responses",
"main_header": "Handle the HTTP Response",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-make-http-request",
"text": "Making an HTTP Request to AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-http-body-content",
"text": "HTTP Body Content"
},
"h3": {
"urllink": "#dp-json-sample-request-response",
"text": "Sample AWS Data Pipeline JSON Request and Response"
},
"links": [],
"text": "The following examples show a request for creating a new pipeline. Then it shows the AWS Data Pipeline response, including the pipeline identifier of the newly created pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-make-http-request.html#dp-json-sample-request-response",
"main_header": "Sample AWS Data Pipeline JSON Request and Response",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#security",
"text": "Security in AWS Data Pipeline"
},
"links": [],
"text": "Cloud security at AWS is the highest priority. As an AWS customer, you benefit from data centers and network architectures that are built to meet the requirements of the most security-sensitive organizations.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html#security",
"main_header": "Security in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#security",
"text": "Security in AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/compliance/shared-responsibility-model/",
"title": "shared responsibility model"
}
],
"text": "Security is a shared responsibility between AWS and you. The shared responsibility model describes this as security of the cloud and security in the cloud:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html#security",
"main_header": "Security in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#security",
"text": "Security in AWS Data Pipeline"
},
"links": [],
"text": "This documentation helps you understand how to apply the shared responsibility model when using AWS Data Pipeline. The following topics show you how to configure AWS Data Pipeline to meet your security and compliance objectives. You also learn how to use other AWS services that help you to monitor and secure your AWS Data Pipeline resources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html#security",
"main_header": "Security in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#data-protection",
"text": "Data Protection in AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/compliance/shared-responsibility-model/",
"title": "shared responsibility model"
},
{
"url": "http://aws.amazon.com/compliance/data-privacy-faq",
"title": "Data Privacy FAQ"
},
{
"url": "http://aws.amazon.com/blogs/security/the-aws-shared-responsibility-model-and-gdpr/",
"title": "AWS Shared Responsibility Model and GDPR"
}
],
"text": "The AWS shared responsibility model applies to data protection in AWS Data Pipeline. As described in this model, AWS is responsible for protecting the global infrastructure that runs all of the AWS Cloud. You are responsible for maintaining control over your content that is hosted on this infrastructure. This content includes the security configuration and management tasks for the AWS services that you use. For more information about data privacy, see the Data Privacy FAQ. For information about data protection in Europe, see the AWS Shared Responsibility Model and GDPR blog post on the AWS Security Blog.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html#data-protection",
"main_header": "Data Protection in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#data-protection",
"text": "Data Protection in AWS Data Pipeline"
},
"links": [],
"text": "For data protection purposes, we recommend that you protect AWS account credentials and set up individual user accounts with AWS Identity and Access Management (IAM). That way each user is given only the permissions necessary to fulfill their job duties. We also recommend that you secure your data in the following ways:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html#data-protection",
"main_header": "Data Protection in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#data-protection",
"text": "Data Protection in AWS Data Pipeline"
},
"links": [],
"text": "We strongly recommend that you never put confidential or sensitive information, such as your customers' email addresses, into tags or free-form fields such as a Name field. This includes when you work with AWS Data Pipeline or other AWS services using the console, API, AWS CLI, or AWS SDKs. Any data that you enter into tags or free-form fields used for names may be used for billing or diagnostic logs. If you provide a URL to an external server, we strongly recommend that you do not include credentials information in the URL to validate your request to that server.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/data-protection.html#data-protection",
"main_header": "Data Protection in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "Your security credentials identify you to services in AWS and grant you permissions to use AWS resources, such as your pipelines. You can use features of AWS Data Pipeline and AWS Identity and Access Management (IAM) to allow AWS Data Pipeline and other users to access your AWS Data Pipeline resources without sharing your security credentials.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "Organizations can share access to pipelines so that the individuals in that organization can develop and maintain them collaboratively. However, for example, it might be necessary to do the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "AWS Data Pipeline is integrated with AWS Identity and Access Management (IAM), which offers a wide range of features:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-control-access",
"text": "Identity and Access Management for AWS Data Pipeline"
},
"links": [],
"text": "By using IAM with AWS Data Pipeline, you can control whether users in your organization can perform a task using specific API actions and whether they can use specific AWS resources. You can use IAM policies based on pipeline tags and worker groups to share your pipelines with other users and control the level of access they have.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html#dp-control-access",
"main_header": "Identity and Access Management for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"links": [],
"text": "By default, IAM users don't have permission to create or modify AWS resources. To allow IAM users to create or modify resources and perform tasks, you must create IAM policies that grant IAM users permission to use the specific resources and API actions they'll need, and then attach those policies to the IAM users or groups that require those permissions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-iam-resourcebased-access",
"main_header": "IAM Policies for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/PermissionsAndPolicies.html",
"title": "Permissions and Policies"
},
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/ManagingPolicies.html",
"title": "Managing IAM Policies"
}
],
"text": "When you attach a policy to a user or group of users, it allows or denies the users permission to perform the specified tasks on the specified resources. For general information about IAM policies, see Permissions and Policies in the IAM User Guide guide. For more information about managing and creating custom IAM policies, see Managing IAM Policies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-iam-resourcebased-access",
"main_header": "IAM Policies for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-policy-syntax",
"text": "Policy Syntax"
},
"links": [],
"text": "An IAM policy is a JSON document that consists of one or more statements. Each statement is structured as follows:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-policy-syntax",
"main_header": "Policy Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-policy-syntax",
"text": "Policy Syntax"
},
"links": [],
"text": "The following elements make up a policy statement:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-policy-syntax",
"main_header": "Policy Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-tags",
"text": "Controlling Access to Pipelines Using Tags"
},
"links": [],
"text": "You can create IAM policies that reference the tags for your pipeline. This enables you to use pipeline tagging to do the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"main_header": "Controlling Access to Pipelines Using Tags",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-tags",
"text": "Controlling Access to Pipelines Using Tags"
},
"links": [],
"text": "For example, suppose that a manager has two pipeline environments, production and development, and an IAM group for each environment. For pipelines in the production environment, the manager grants read/write access to users in the production IAM group, but grants read-only access to users in the developer IAM group. For pipelines in the development environment, the manager grants read/write access to both the production and developer IAM groups.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"main_header": "Controlling Access to Pipelines Using Tags",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-tags",
"text": "Controlling Access to Pipelines Using Tags"
},
"links": [],
"text": "To achieve this scenario, the manager tags the production pipelines with the \"environment=production\" tag and attaches the following policy to the developer IAM group. The first statement grants read-only access to all pipelines. The second statement grants read/write access to pipelines that do not have an \"environment=production\" tag.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"main_header": "Controlling Access to Pipelines Using Tags",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-tags",
"text": "Controlling Access to Pipelines Using Tags"
},
"links": [],
"text": "In addition, the manager attaches the following policy to the production IAM group. This statement grants full access to all pipelines.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"main_header": "Controlling Access to Pipelines Using Tags",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-tags",
"text": "Controlling Access to Pipelines Using Tags"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex1",
"title": "Grant users read-only access based on a tag"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex2",
"title": "Grant users full access based on a tag"
}
],
"text": "For more examples, see Grant users read-only access based on a tag and Grant users full access based on a tag.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-tags",
"main_header": "Controlling Access to Pipelines Using Tags",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-workergroup",
"text": "Controlling Access to Pipelines Using Worker Groups"
},
"links": [],
"text": "You can create IAM policies that make reference worker group names.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-workergroup",
"main_header": "Controlling Access to Pipelines Using Worker Groups",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-workergroup",
"text": "Controlling Access to Pipelines Using Worker Groups"
},
"links": [],
"text": "For example, suppose that a manager has two pipeline environments, production and development, and an IAM group for each environment. The manager has three database servers with task runners configured for production, pre-production, and developer environments, respectively. The manager wants to ensure that users in the production IAM group can create pipelines that push tasks to production resources, and that users in the development IAM group can create pipelines that push tasks to both pre-production and developer resources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-workergroup",
"main_header": "Controlling Access to Pipelines Using Worker Groups",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-workergroup",
"text": "Controlling Access to Pipelines Using Worker Groups"
},
"links": [],
"text": "To achieve this scenario, the manager installs task runner on the production resources with production credentials, and sets workerGroup to \"prodresource\". In addition, the manager installs task runner on the development resources with development credentials, and sets workerGroup to \"pre-production\" and \"development\". The manager attaches the following policy to the developer IAM group to block access to \"prodresource\" resources. The first statement grants read-only access to all pipelines. The second statement grants read/write access to pipelines when the name of the worker group has a prefix of \"dev\" or \"pre-prod\".",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-workergroup",
"main_header": "Controlling Access to Pipelines Using Worker Groups",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-resourcebased-access",
"text": "IAM Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-control-access-workergroup",
"text": "Controlling Access to Pipelines Using Worker Groups"
},
"links": [],
"text": "In addition, the manager attaches the following policy to the production IAM group to grant access to \"prodresource\" resources. The first statement grants read-only access to all pipelines. The second statement grants read/write access when the name of the worker group has a prefix of \"prod\".",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-resourcebased-access.html#dp-control-access-workergroup",
"main_header": "Controlling Access to Pipelines Using Worker Groups",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"links": [],
"text": "The following examples demonstrate how to grant users full or restricted access to pipelines.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#dp-example-tag-policies",
"main_header": "Example Policies for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#ex1",
"text": "Example 1: Grant users read-only access based on a tag"
},
"links": [],
"text": "The following policy allows users to use the read-only AWS Data Pipeline API actions, but only with pipelines that have the tag \"environment=production\".",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex1",
"main_header": "Example 1: Grant users read-only access based on a tag",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#ex1",
"text": "Example 1: Grant users read-only access based on a tag"
},
"links": [],
"text": "The ListPipelines API action does not support tag-based authorization.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex1",
"main_header": "Example 1: Grant users read-only access based on a tag",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#ex2",
"text": "Example 2: Grant users full access based on a tag"
},
"links": [],
"text": "The following policy allows users to use all AWS Data Pipeline API actions, with the exception of ListPipelines, but only with pipelines that have the tag \"environment=test\".",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex2",
"main_header": "Example 2: Grant users full access based on a tag",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#ex3",
"text": "Example 3: Grant the pipeline owner full access"
},
"links": [],
"text": "The following policy allows users to use all the AWS Data Pipeline API actions, but only with their own pipelines.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#ex3",
"main_header": "Example 3: Grant the pipeline owner full access",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#example4-grant-users-access-to-console",
"text": "Example 4: Grant users access to the AWS Data Pipeline console"
},
"links": [],
"text": "The following policy allows users to create and manage a pipeline using the AWS Data Pipeline console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#example4-grant-users-access-to-console",
"main_header": "Example 4: Grant users access to the AWS Data Pipeline console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-example-tag-policies",
"text": "Example Policies for AWS Data Pipeline"
},
"h2": {
"urllink": "#example4-grant-users-access-to-console",
"text": "Example 4: Grant users access to the AWS Data Pipeline console"
},
"links": [
{
"url": "https://aws.amazon.com/blogs/security/granting-permission-to-launch-ec2-instances-with-iam-roles-passrole-permission/",
"title": "Granting Permission to Launch EC2 Instances with IAM Roles (PassRole Permission)"
}
],
"text": "This policy includes the action for PassRole permissions for specific resources tied to the roleARN that AWS Data Pipeline needs. For more information about the identity-based (IAM) PassRole permission, see the blog post Granting Permission to Launch EC2 Instances with IAM Roles (PassRole Permission).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-example-tag-policies.html#example4-grant-users-access-to-console",
"main_header": "Example 4: Grant users access to the AWS Data Pipeline console",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
"title": "IAM roles"
}
],
"text": "AWS Data Pipeline uses AWS Identity and Access Management roles. The permissions policies attached to IAM roles determine what actions AWS Data Pipeline and your applications can perform, and what AWS resources they can access. For more information, see IAM roles in the IAM User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"links": [],
"text": "AWS Data Pipeline requires two IAM roles:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles",
"main_header": "IAM Roles for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-role-permissions-policy-examples",
"text": "Example Permissions Policies for AWS Data Pipeline Roles"
},
"links": [],
"text": "Each role has one or more permissions policies attached to it that determine the AWS resources that the role can access and the actions that the role can perform. This topic provides an example permissions policy for the pipeline role. It also provides the contents of the AmazonEC2RoleforDataPipelineRole, which is the managed policy for the default EC2 instance role, DataPipelineDefaultResourceRole.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-role-permissions-policy-examples",
"main_header": "Example Permissions Policies for AWS Data Pipeline Roles",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-role-permissions-policy-examples",
"text": "Example Permissions Policies for AWS Data Pipeline Roles"
},
"h3": {
"urllink": "#dp-role-example-policy",
"text": "Example Pipeline Role Permissions Policy"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"title": "DynamoDBDataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"title": "SnsAlarm"
}
],
"text": "The example policy that follows is scoped to allow essential functions that AWS Data Pipeline requires to run a pipeline with Amazon EC2 and Amazon EMR resources. It also provides permissions to access other AWS resources, such as Amazon Simple Storage Service and Amazon Simple Notification Service, that many pipelines require. If the objects defined in a pipeline do not require the resources of an AWS service, we strongly recommend that you remove permissions to access that service. For example, if your pipeline does not define a DynamoDBDataNode or use the SnsAlarm action, we recommend that you remove the allow statements for those actions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-role-example-policy",
"main_header": "Example Pipeline Role Permissions Policy",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-role-permissions-policy-examples",
"text": "Example Permissions Policies for AWS Data Pipeline Roles"
},
"h3": {
"urllink": "#dp-resource-role-example-policy",
"text": "Default Managed Policy for the EC2 Instance Role"
},
"links": [],
"text": "The contents of the AmazonEC2RoleforDataPipelineRole is shown below. This is the managed policy attached to the default resource role for AWS Data Pipeline, DataPipelineDefaultResourceRole. When you define a resource role for your pipeline, we recommend that you begin with this permissions policy and then remove permissions for AWS service actions that are not required.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-resource-role-example-policy",
"main_header": "Default Managed Policy for the EC2 Instance Role",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-role-permissions-policy-examples",
"text": "Example Permissions Policies for AWS Data Pipeline Roles"
},
"h3": {
"urllink": "#dp-resource-role-example-policy",
"text": "Default Managed Policy for the EC2 Instance Role"
},
"links": [],
"text": "Version 3 of the policy is shown, which is the most recent version at the time of this writing. View the most recent version of the policy using the IAM console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-resource-role-example-policy",
"main_header": "Default Managed Policy for the EC2 Instance Role",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-roles-new",
"text": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions"
},
"links": [],
"text": "Use the following procedures to create roles for AWS Data Pipeline using the IAM console. The process consists of two steps. First, you create a permissions policy to attach to the role. Next, you create the role and attach the policy. After you create a role, you can change the role's permissions by attaching and detaching permissions policies.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-roles-new",
"main_header": "Creating IAM Roles for AWS Data Pipeline and Editing Role Permissions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-iam-roles",
"text": "IAM Roles for AWS Data Pipeline"
},
"h2": {
"urllink": "#dp-iam-change-console",
"text": "Changing Roles for an Existing Pipelines"
},
"links": [],
"text": "If you want to assign a different pipeline role or resource role to a pipeline, you can use the architect editor in the AWS Data Pipeline console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html#dp-iam-change-console",
"main_header": "Changing Roles for an Existing Pipelines",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-control-access.html",
"title": "Identity and Access Management for AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"links": [],
"text": "AWS Data Pipeline is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in AWS Data Pipeline. CloudTrail captures all API calls for AWS Data Pipeline as events. The calls captured include calls from the AWS Data Pipeline console and code calls to the AWS Data Pipeline  API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for AWS Data Pipeline. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history. Using the information collected by CloudTrail, you can determine the request that was made to AWS Data Pipeline, the IP address from which the request was made, who made the request, when it was made, and additional details.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#dp-cloudtrail-logging",
"main_header": "Logging and Monitoring in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/",
"title": "AWS CloudTrail User Guide"
}
],
"text": "To learn more about CloudTrail, see the AWS CloudTrail User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#dp-cloudtrail-logging",
"main_header": "Logging and Monitoring in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html",
"title": "Viewing Events with CloudTrail Event History"
}
],
"text": "CloudTrail is enabled on your AWS account when you create the account. When activity occurs in AWS Data Pipeline, that activity is recorded in a CloudTrail event along with other AWS service events in Event history. You can view, search, and download recent events in your AWS account. For more information, see Viewing Events with CloudTrail Event History.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [],
"text": "For an ongoing record of events in your AWS account, including events for AWS Data Pipeline, create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs. For more information, see the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "p",
"children_tags": [
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/APIReference/API_Operations.html",
"title": "AWS Data Pipeline API Reference Actions chapter"
}
],
"text": "All of the AWS Data Pipeline actions are logged by CloudTrail and are documented in the AWS Data Pipeline API Reference Actions chapter. For example, calls to the CreatePipeline action generate entries in the CloudTrail log files.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [],
"text": "Every event or log entry contains information about who generated the request. The identity information helps you determine the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#service-name-info-in-cloudtrail",
"text": "AWS Data Pipeline Information in CloudTrail"
},
"links": [
{
"url": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html",
"title": "CloudTrail userIdentity Element"
}
],
"text": "For more information, see the CloudTrail userIdentity Element.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#service-name-info-in-cloudtrail",
"main_header": "AWS Data Pipeline Information in CloudTrail",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#understanding-service-name-entries",
"text": "Understanding AWS Data Pipeline Log File Entries"
},
"links": [],
"text": "A trail is a configuration that enables delivery of events as log files to an Amazon S3 bucket that you specify. CloudTrail log files contain one or more log entries. An event represents a single request from any source and includes information about the requested action, the date and time of the action, request parameters, and so on. CloudTrail log files aren't an ordered stack trace of the public API calls, so they don't appear in any specific order.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#understanding-service-name-entries",
"main_header": "Understanding AWS Data Pipeline Log File Entries",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-cloudtrail-logging",
"text": "Logging and Monitoring in AWS Data Pipeline"
},
"h2": {
"urllink": "#understanding-service-name-entries",
"text": "Understanding AWS Data Pipeline Log File Entries"
},
"links": [],
"text": "The following example shows a CloudTrail log entry that demonstrates the CreatePipeline operation:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-cloudtrail-logging.html#understanding-service-name-entries",
"main_header": "Understanding AWS Data Pipeline Log File Entries",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#incident-response",
"text": "Incident Response in AWS Data Pipeline"
},
"links": [],
"text": "Incident response for AWS Data Pipeline is an AWS responsibility. AWS has a formal, documented policy and program that governs incident response.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/incident-response.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/incident-response.html#incident-response",
"main_header": "Incident Response in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#incident-response",
"text": "Incident Response in AWS Data Pipeline"
},
"links": [],
"text": "AWS operational issues with broad impact are posted on the AWS Service Health Dashboard. Operational issues are also posted to individual accounts via the Personal Health Dashboard.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/incident-response.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/incident-response.html#incident-response",
"main_header": "Incident Response in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#compliance-validation.title",
"text": "Compliance Validation for AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/compliance/services-in-scope/",
"title": "AWS Services in Scope by Compliance Program"
},
{
"url": "http://aws.amazon.com/compliance/programs/",
"title": "AWS Compliance Programs"
}
],
"text": "AWS Data Pipeline is not in scope of any AWS compliance programs. For a list of AWS services in scope of specific compliance programs, see AWS Services in Scope by Compliance Program. For general information, see AWS Compliance Programs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/compliance-validation.title.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/compliance-validation.title.html#compliance-validation.title",
"main_header": "Compliance Validation for AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#disaster-recovery-resiliency",
"text": "Resilience in AWS Data Pipeline"
},
"links": [],
"text": "The AWS global infrastructure is built around AWS Regions and Availability Zones. AWS Regions provide multiple physically separated and isolated Availability Zones, which are connected with low-latency, high-throughput, and highly redundant networking. With Availability Zones, you can design and operate applications and databases that automatically fail over between zones without interruption. Availability Zones are more highly available, fault tolerant, and scalable than traditional single or multiple data center infrastructures.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/disaster-recovery-resiliency.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/disaster-recovery-resiliency.html#disaster-recovery-resiliency",
"main_header": "Resilience in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#disaster-recovery-resiliency",
"text": "Resilience in AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/about-aws/global-infrastructure/",
"title": "AWS Global Infrastructure"
}
],
"text": "For more information about AWS Regions and Availability Zones, see AWS Global Infrastructure.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/disaster-recovery-resiliency.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/disaster-recovery-resiliency.html#disaster-recovery-resiliency",
"main_header": "Resilience in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#infrastructure-security",
"text": "Infrastructure Security in AWS Data Pipeline"
},
"links": [
{
"url": "https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Whitepaper.pdf",
"title": "Amazon Web Services: Overview of Security Processes"
}
],
"text": "As a managed service, AWS Data Pipeline is protected by the AWS global network security procedures that are described in the Amazon Web Services: Overview of Security Processes whitepaper.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html#infrastructure-security",
"main_header": "Infrastructure Security in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#infrastructure-security",
"text": "Infrastructure Security in AWS Data Pipeline"
},
"links": [],
"text": "You use AWS published API calls to access AWS Data Pipeline through the network. Clients must support Transport Layer Security (TLS) 1.0 or later. We recommend TLS 1.2 or later. Clients must also support cipher suites with perfect forward secrecy (PFS) such as Ephemeral Diffie-Hellman (DHE) or Elliptic Curve Ephemeral Diffie-Hellman (ECDHE). Most modern systems such as Java 7 and later support these modes.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html#infrastructure-security",
"main_header": "Infrastructure Security in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#infrastructure-security",
"text": "Infrastructure Security in AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html",
"title": "AWS Security Token Service"
}
],
"text": "Additionally, requests must be signed by using an access key ID and a secret access key that is associated with an IAM principal. Or you can use the AWS Security Token Service (AWS STS) to generate temporary security credentials to sign requests.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/infrastructure-security.html#infrastructure-security",
"main_header": "Infrastructure Security in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#configuration-and-vulnerability-analysis",
"text": "Configuration and Vulnerability Analysis in AWS Data Pipeline"
},
"links": [
{
"url": "http://aws.amazon.com/compliance/shared-responsibility-model/",
"title": "shared responsibility model"
}
],
"text": "Configuration and IT controls are a shared responsibility between AWS and you, our customer. For more information, see the AWS shared responsibility model.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/configuration-and-vulnerability-analysis.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/configuration-and-vulnerability-analysis.html#configuration-and-vulnerability-analysis",
"main_header": "Configuration and Vulnerability Analysis in AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/security.html",
"title": "Security in AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#welcome",
"text": "Tutorials"
},
"links": [],
"text": "The following tutorials walk you step-by-step through the process of creating and using pipelines with AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html#welcome",
"main_header": "Tutorials",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow",
"text": "Process Data Using Amazon EMR with Hadoop Streaming"
},
"links": [],
"text": "You can use AWS Data Pipeline to manage your Amazon EMR clusters. With AWS Data Pipeline you can specify preconditions that must be met before the cluster is launched (for example, ensuring that today's data been uploaded to Amazon S3), a schedule for repeatedly running the cluster, and the cluster configuration to use. The following tutorial walks you through launching a simple cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html#dp-launch-emr-jobflow",
"main_header": "Process Data Using Amazon EMR with Hadoop Streaming",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow",
"text": "Process Data Using Amazon EMR with Hadoop Streaming"
},
"links": [],
"text": "In this tutorial, you create a pipeline for a simple Amazon EMR cluster to run a pre-existing Hadoop Streaming job provided by Amazon EMR and send an Amazon SNS notification after the task completes successfully. You use the Amazon EMR cluster resource provided by AWS Data Pipeline for this task. The sample application is called WordCount, and can also be run manually from the Amazon EMR console. Note that clusters spawned by AWS Data Pipeline on your behalf are displayed in the Amazon EMR console and are billed to your AWS account.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html#dp-launch-emr-jobflow",
"main_header": "Process Data Using Amazon EMR with Hadoop Streaming",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow",
"text": "Process Data Using Amazon EMR with Hadoop Streaming"
},
"links": [],
"text": "Pipeline Objects",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html#dp-launch-emr-jobflow",
"main_header": "Process Data Using Amazon EMR with Hadoop Streaming",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow",
"text": "Process Data Using Amazon EMR with Hadoop Streaming"
},
"links": [],
"text": "The pipeline uses the following objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html#dp-launch-emr-jobflow",
"main_header": "Process Data Using Amazon EMR with Hadoop Streaming",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "Be sure you've completed the following steps.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-prereq.html#dp-emr-jobflow-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"links": [],
"text": "You can create a pipeline to launch a cluster to analyze web logs or perform analysis of scientific data.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-console",
"main_header": "Launch a Cluster Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "First, create the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, add an activity to your pipeline definition. This also defines the other objects that AWS Data Pipeline must use to perform this activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, configure the resource that AWS Data Pipeline uses to perform the Amazon EMR job.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, configure the Amazon SNS notification action that AWS Data Pipeline performs after the Amazon EMR job finishes successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-emr-jobflow-console",
"text": "Launch a Cluster Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-emr-jobflow-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-emr-jobflow-console.html#dp-emr-jobflow-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"links": [],
"text": "If you regularly run an Amazon EMR cluster to analyze web logs or perform analysis of scientific data, you can use AWS Data Pipeline to manage your Amazon EMR clusters. With AWS Data Pipeline, you can specify preconditions that must be met before the cluster is launched (for example, ensuring that today's data been uploaded to Amazon S3.) This tutorial walks you through launching a cluster that can be a model for a simple Amazon EMR-based pipeline, or as part of a more involved pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#dp-launch-emr-jobflow-cli",
"main_header": "Launch a Cluster Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-json",
"text": "Creating the Pipeline Definition File"
},
"links": [],
"text": "The following code is the pipeline definition file for a simple Amazon EMR cluster that runs an existing Hadoop streaming job provided by Amazon EMR. This sample application is called WordCount, and you can also run it using the Amazon EMR console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-json",
"main_header": "Creating the Pipeline Definition File",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-json",
"text": "Creating the Pipeline Definition File"
},
"links": [],
"text": "Copy this code into a text file and save it as MyEmrPipelineDefinition.json. You should replace the Amazon S3 bucket location with the name of an Amazon S3 bucket that you own. You should also replace the start and end dates. To launch clusters immediately, set startDateTime to a date one day in the past and endDateTime to one day in the future. AWS Data Pipeline then starts launching the \"past due\" clusters immediately in an attempt to address what it perceives as a backlog of work. This backfilling means you don't have to wait an hour to see AWS Data Pipeline launch its first cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-json",
"main_header": "Creating the Pipeline Definition File",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-json",
"text": "Creating the Pipeline Definition File"
},
"links": [],
"text": "This pipeline has three objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-json",
"main_header": "Creating the Pipeline Definition File",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [],
"text": "You must upload your pipeline definition and activate your pipeline.\nIn the following example commands, replace pipeline_name with a label for your pipeline and \npipeline_file with the fully-qualified path for the pipeline definition .json file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [],
"text": "AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/create-pipeline.html",
"title": "create-pipeline"
}
],
"text": "To create your pipeline definition and activate your pipeline, use the following create-pipeline command. Note the ID of your pipeline, because you'll use this value with most CLI commands.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/put-pipeline-definition.html",
"title": "put-pipeline-definition"
}
],
"text": "To upload your pipeline definition, use the following put-pipeline-definition command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [],
"text": "If you pipeline validates successfully, the validationErrors field is empty. You should review any warnings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/activate-pipeline.html",
"title": "activate-pipeline"
}
],
"text": "To activate your pipeline, use the following activate-pipeline command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-activate",
"text": "Uploading and Activating the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/list-pipelines.html",
"title": "list-pipelines"
}
],
"text": "You can verify that your pipeline appears in the pipeline list using the following list-pipelines command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-activate",
"main_header": "Uploading and Activating the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-launch-emr-jobflow-cli",
"text": "Launch a Cluster Using the Command Line"
},
"h2": {
"urllink": "#streaming-cluster-monitor",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "You can view clusters launched by AWS Data Pipeline using the Amazon EMR console and you can view the output folder using the Amazon S3 console.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow-cli.html#streaming-cluster-monitor",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-launch-emr-jobflow.html",
"title": "Process Data Using Amazon EMR with Hadoop Streaming"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb",
"text": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
"links": [],
"text": "These tutorials demonstrate how to move schema-less data in and out of Amazon DynamoDB using AWS Data Pipeline. Complete part one before you move on to part two.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html#dp-importexport-ddb",
"main_header": "Import and Export DynamoDB Data Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-part1",
"text": "Part One: Import Data into DynamoDB"
},
"links": [],
"text": "The first part of this tutorial explains how to define an AWS Data Pipeline to retrieve data from a tab-delimited file in Amazon S3 to populate a DynamoDB table, define the transformation steps, and create an Amazon EMR cluster to perform the work.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html#dp-importexport-ddb-part1",
"main_header": "Part One: Import Data into DynamoDB",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "Be sure to complete the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "Be aware of the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"h2": {
"urllink": "#dp-importexport-ddb-table-cli",
"text": "Create a DynamoDB Table"
},
"links": [],
"text": "You can create the DynamoDB table that is required for this tutorial. If you already have a DynamoDB table, you can skip this procedure to create one.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-table-cli",
"main_header": "Create a DynamoDB Table",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq",
"text": "Before You Begin"
},
"h2": {
"urllink": "#dp-importexport-ddb-table-cli",
"text": "Create a DynamoDB Table"
},
"links": [
{
"url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithDDTables.html",
"title": "Working with Tables in DynamoDB"
}
],
"text": "For more information, see Working with Tables in DynamoDB in the Amazon DynamoDB Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq.html#dp-importexport-ddb-table-cli",
"main_header": "Create a DynamoDB Table",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "First, create the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "Next, configure the Amazon SNS notification actions that AWS Data Pipeline performs depending on the outcome of the activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html#dp-importexport-ddb-console-start",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-save-pipeline-console.html#dp-import-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-activate-pipeline-console.html#dp-import-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-execution-pipeline-console.html#dp-import-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-pipelinejson-verifydata",
"text": "Step 5: Verify the Data Import"
},
"links": [],
"text": "Next, verify that the data import occurred successfully using the DynamoDB console to inspect the data in the table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-pipelinejson-verifydata.html#dp-import-ddb-pipelinejson-verifydata",
"main_header": "Step 5: Verify the Data Import",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-import-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-import-ddb-delete-pipeline-console.html#dp-import-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-part2",
"text": "Part Two: Export Data from DynamoDB"
},
"links": [],
"text": "This is the second of a two-part tutorial that demonstrates how to bring together multiple AWS features to solve real-world problems in a scalable way through a common scenario: moving schema-less data in and out of DynamoDB using AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html#dp-importexport-ddb-part2",
"main_header": "Part Two: Export Data from DynamoDB",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq2",
"text": "Before You Begin"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
"title": "Part One: Import Data into DynamoDB"
}
],
"text": "You must complete part one of this tutorial to ensure that your DynamoDB table contains the necessary data to perform the steps in this section. For more information, see Part One: Import Data into DynamoDB.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html#dp-importexport-ddb-prereq2",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq2",
"text": "Before You Begin"
},
"links": [],
"text": "Additionally, be sure you've completed the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html#dp-importexport-ddb-prereq2",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-prereq2",
"text": "Before You Begin"
},
"links": [],
"text": "Be aware of the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-prereq2.html#dp-importexport-ddb-prereq2",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "First, create the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-console-start2",
"text": "Step 1: Create the Pipeline"
},
"links": [],
"text": "Next, configure the Amazon SNS notification actions that AWS Data Pipeline performs depending on the outcome of the activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start2.html#dp-importexport-ddb-console-start2",
"main_header": "Step 1: Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-save-pipeline-console",
"text": "Step 2: Save and Validate Your Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-save-pipeline-console.html#dp-export-ddb-save-pipeline-console",
"main_header": "Step 2: Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-activate-pipeline-console",
"text": "Step 3: Activate Your Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-activate-pipeline-console.html#dp-export-ddb-activate-pipeline-console",
"main_header": "Step 3: Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-execution-pipeline-console",
"text": "Step 4: Monitor the Pipeline Runs"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-execution-pipeline-console.html#dp-export-ddb-execution-pipeline-console",
"main_header": "Step 4: Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-importexport-ddb-pipelinejson-verifydata2",
"text": "Step 5: Verify the Data Export File"
},
"links": [],
"text": "Next, verify that the data export occurred successfully using viewing the output file contents.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-pipelinejson-verifydata2.html#dp-importexport-ddb-pipelinejson-verifydata2",
"main_header": "Step 5: Verify the Data Export File",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-export-ddb-delete-pipeline-console",
"text": "Step 6: Delete Your Pipeline (Optional)"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-export-ddb-delete-pipeline-console.html#dp-export-ddb-delete-pipeline-console",
"main_header": "Step 6: Delete Your Pipeline (Optional)",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html",
"title": "Import and Export DynamoDB Data Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html",
"title": "Part Two: Export Data from DynamoDB"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3",
"text": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
"title": "What is AWS Data Pipeline?"
}
],
"text": "After you read What is AWS Data Pipeline? and decide you want to use AWS Data Pipeline to automate the movement and transformation of your data, it is time to get started with creating data pipelines. To help you make sense of how AWS Data Pipeline works, let\u00e2\u0080\u0099s walk through a simple task.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html#dp-copydata-s3",
"main_header": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3",
"text": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
},
"links": [],
"text": "This tutorial walks you through the process of creating a data pipeline to copy data from one Amazon S3 bucket to another and then send an Amazon SNS notification after the copy activity completes successfully. You use an EC2 instance managed by AWS Data Pipeline for this copy activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html#dp-copydata-s3",
"main_header": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3",
"text": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
},
"links": [],
"text": "Pipeline Objects",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html#dp-copydata-s3",
"main_header": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3",
"text": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
},
"links": [],
"text": "The pipeline uses the following objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html#dp-copydata-s3",
"main_header": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "Be sure you've completed the following steps.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-prereq.html#dp-copydata-s3-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"links": [],
"text": "You can create and use pipelines to copy data from one Amazon S3 bucket to another.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-console",
"main_header": "Copy CSV Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "First, create the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, define the Activity object in your pipeline definition. When you define the Activity object, you also define the objects that AWS Data Pipeline must use to perform this activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, configure the input and the output data nodes for your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, configure the resource AWS Data Pipeline must use to perform the copy activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, configure the Amazon SNS notification action that AWS Data Pipeline performs after the copy activity finishes successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-s3-console",
"text": "Copy CSV Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-s3-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3-console.html#dp-copydata-s3-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"links": [],
"text": "You can create and use pipelines to copy data from one Amazon S3 bucket to another.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-get-started-copy-data-cli",
"main_header": "Copy CSV Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "This example scenario shows how to use JSON pipeline definitions and the AWS Data Pipeline CLI to schedule copying data between two Amazon S3 buckets at a specific time interval. This is the full pipeline definition JSON file followed by an explanation for each of its sections.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "In this example, for clarity, we skip the optional fields and show only required fields. The complete pipeline JSON file for this example is:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-schedule-cli",
"text": "Schedule"
},
"links": [],
"text": "The pipeline defines a schedule with a begin and end date, along with a period to determine how frequently the activity in this pipeline runs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-schedule-cli",
"main_header": "Schedule",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-s3-node-cli",
"text": "Amazon S3 Data Nodes"
},
"links": [],
"text": "Next, the input S3DataNode pipeline component defines a location for the input files; in this case, an Amazon S3 bucket location. The input S3DataNode component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-s3-node-cli",
"main_header": "Amazon S3 Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-s3-node-cli",
"text": "Amazon S3 Data Nodes"
},
"links": [],
"text": "Next, the output S3DataNode component defines the output destination location for the data. It follows the same format as the input S3DataNode component, except the name of the component and a different path to indicate the target file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-s3-node-cli",
"main_header": "Amazon S3 Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-resource-cli",
"text": "Resource"
},
"links": [],
"text": "This is a definition of the computational resource that performs the copy operation. In this example, AWS Data Pipeline should automatically create an EC2 instance to perform the copy task and terminate the resource after the task completes. The fields defined here control the creation and function of the EC2 instance that does the work. The EC2Resource is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copy-json-activity-cli",
"text": "Activity"
},
"links": [],
"text": "The last section in the JSON file is the definition of the activity that represents the work to perform. This example uses CopyActivity to copy data from a CSV file in an http://aws.amazon.com/ec2/instance-types/ bucket to another. The CopyActivity component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "You must upload your pipeline definition and activate your pipeline.\nIn the following example commands, replace pipeline_name with a label for your pipeline and \npipeline_file with the fully-qualified path for the pipeline definition .json file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/create-pipeline.html",
"title": "create-pipeline"
}
],
"text": "To create your pipeline definition and activate your pipeline, use the following create-pipeline command. Note the ID of your pipeline, because you'll use this value with most CLI commands.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/put-pipeline-definition.html",
"title": "put-pipeline-definition"
}
],
"text": "To upload your pipeline definition, use the following put-pipeline-definition command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "If you pipeline validates successfully, the validationErrors field is empty. You should review any warnings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/activate-pipeline.html",
"title": "activate-pipeline"
}
],
"text": "To activate your pipeline, use the following activate-pipeline command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-get-started-copy-data-cli",
"text": "Copy CSV Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copy-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/list-pipelines.html",
"title": "list-pipelines"
}
],
"text": "You can verify that your pipeline appears in the pipeline list using the following list-pipelines command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-get-started-copy-data-cli.html#dp-copy-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-s3.html",
"title": "Copy CSV Data Between Amazon S3 Buckets Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql",
"text": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
},
"links": [],
"text": "This tutorial walks you through the process of creating a data pipeline to copy data (rows) from a table in MySQL database to a CSV (comma-separated values) file in an Amazon S3 bucket and then sending an Amazon SNS notification after the copy activity completes successfully. You will use an EC2 instance provided by AWS Data Pipeline for this copy activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html#dp-copydata-mysql",
"main_header": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql",
"text": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
},
"links": [],
"text": "Pipeline Objects",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html#dp-copydata-mysql",
"main_header": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql",
"text": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
},
"links": [],
"text": "The pipeline uses the following objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html#dp-copydata-mysql",
"main_header": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-prereq",
"text": "Before You Begin"
},
"links": [],
"text": "Be sure you've completed the following steps.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-prereq.html#dp-copydata-mysql-prereq",
"main_header": "Before You Begin",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"links": [],
"text": "You can create a pipeline to copy data from a MySQL table to a file in an Amazon S3 bucket.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-console",
"main_header": "Copy MySQL Data Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "First, create the pipeline. The pipeline must be created in the same region as your target RDS instance.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "Next, configure the database name setting, which currently is not present on the available template.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "You can configure the Amazon SNS notification action AWS Data Pipeline performs after the copy activity finishes successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-verify-pipeline-console",
"text": "Verify Your Pipeline Definition"
},
"links": [],
"text": "It is important that you verify that your pipeline was correctly initialized from your definitions before you activate it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-verify-pipeline-console",
"main_header": "Verify Your Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-mysql-console",
"text": "Copy MySQL Data Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-mysql-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html#dp-copydata-mysql-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"links": [],
"text": "You can create a pipeline to copy data from a MySQL table to a file in an Amazon S3 bucket.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-cli",
"main_header": "Copy MySQL Data Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "This example scenario shows how to use JSON pipeline definitions and the AWS Data Pipeline CLI to copy data (rows) from a table in a MySQL database to a CSV (comma-separated values) file in an Amazon S3 bucket at a specified time interval.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "This is the full pipeline definition JSON file followed by an explanation for each of its sections.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-rds-node-cli",
"text": "MySQL Data Node"
},
"links": [],
"text": "The input MySqlDataNode pipeline component defines a location for the input data; in this case, an Amazon RDS instance. The input MySqlDataNode component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-rds-node-cli",
"main_header": "MySQL Data Node",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-json-s3-node-cli",
"text": "Amazon S3 Data Node"
},
"links": [],
"text": "Next, the S3Output pipeline component defines a location for the output file; in this case a CSV file in an Amazon S3 bucket location. The output S3DataNode component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-s3-node-cli",
"main_header": "Amazon S3 Data Node",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-json-resource-cli",
"text": "Resource"
},
"links": [],
"text": "This is a definition of the computational resource that performs the copy operation. In this example, AWS Data Pipeline should automatically create an EC2 instance to perform the copy task and terminate the resource after the task completes. The fields defined here control the creation and function of the EC2 instance that does the work. The EC2Resource is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"h3": {
"urllink": "#dp-copymysql-json-activity-cli",
"text": "Activity"
},
"links": [],
"text": "The last section in the JSON file is the definition of the activity that represents the work to perform. In this case we use a CopyActivity component to copy data from a file in an Amazon S3 bucket to another file. The CopyActivity component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "You must upload your pipeline definition and activate your pipeline.\nIn the following example commands, replace pipeline_name with a label for your pipeline and \npipeline_file with the fully-qualified path for the pipeline definition .json file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/create-pipeline.html",
"title": "create-pipeline"
}
],
"text": "To create your pipeline definition and activate your pipeline, use the following create-pipeline command. Note the ID of your pipeline, because you'll use this value with most CLI commands.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/put-pipeline-definition.html",
"title": "put-pipeline-definition"
}
],
"text": "To upload your pipeline definition, use the following put-pipeline-definition command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "If you pipeline validates successfully, the validationErrors field is empty. You should review any warnings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/activate-pipeline.html",
"title": "activate-pipeline"
}
],
"text": "To activate your pipeline, use the following activate-pipeline command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copymysql-cli",
"text": "Copy MySQL Data Using the Command Line"
},
"h2": {
"urllink": "#dp-copymysql-json-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/list-pipelines.html",
"title": "list-pipelines"
}
],
"text": "You can verify that your pipeline appears in the pipeline list using the following list-pipelines command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copymysql-cli.html#dp-copymysql-json-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html",
"title": "Export MySQL Data to Amazon S3 Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift",
"text": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
"links": [],
"text": "This tutorial walks you through the process of creating a pipeline that periodically moves data from Amazon S3 to Amazon Redshift using either the Copy to Redshift template in the AWS Data Pipeline console, or a pipeline definition file with the AWS Data Pipeline CLI.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html#dp-copydata-redshift",
"main_header": "Copy Data to Amazon Redshift Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift",
"text": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/",
"title": "Amazon Simple Storage Service User Guide"
}
],
"text": "Amazon S3 is a web service that enables you to store data in the cloud. For more information, see the Amazon Simple Storage Service User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html#dp-copydata-redshift",
"main_header": "Copy Data to Amazon Redshift Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift",
"text": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/",
"title": "Amazon Redshift Cluster Management Guide"
}
],
"text": "Amazon Redshift is a data warehouse service in the cloud. For more information, see the Amazon Redshift Cluster Management Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html#dp-copydata-redshift",
"main_header": "Copy Data to Amazon Redshift Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift",
"text": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
"links": [],
"text": "This tutorial has several prerequisites. After completing the following steps, you can continue the tutorial using either the console or the CLI.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html#dp-copydata-redshift",
"main_header": "Copy Data to Amazon Redshift Using AWS Data Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [],
"text": "Before copying data to Amazon Redshift within AWS Data Pipeline, ensure that you:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [],
"text": "Once you have these options working and successfully complete a data load, transfer these options to  AWS Data Pipeline, for performing the copying within it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html",
"title": "COPY"
}
],
"text": "For COPY options, see COPY in the Amazon Redshift Database Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/t_Loading-data-from-S3.html",
"title": "Loading data from Amazon S3"
}
],
"text": "For steps to load data from Amazon S3, see Loading data from Amazon S3 in the Amazon Redshift Database Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [],
"text": "For example, the following SQL command in Amazon Redshift creates a new table named LISTING and copies sample data from a publicly available bucket in Amazon S3.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [],
"text": "Replace the <iam-role-arn> and region with your own.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-learn-copy-redshift",
"text": "Before You Begin: Configure COPY Options and Load Data"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-sample-db.html",
"title": "Load Sample Data from Amazon S3"
}
],
"text": "For details about this example, see Load Sample Data from Amazon S3 in the Amazon Redshift Getting Started Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html#dp-learn-copy-redshift",
"main_header": "Before You Begin: Configure COPY Options and Load Data",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [],
"text": "You can create a pipeline to copy data from Amazon S3 to Amazon Redshift. You'll create a new table in Amazon Redshift, and then use AWS Data Pipeline to transfer data to this table from a public Amazon S3 bucket, which contains sample input data in CSV format. The logs are saved to an Amazon S3 bucket that you own.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/",
"title": "Amazon Simple Storage Service User Guide"
}
],
"text": "Amazon S3 is a web service that enables you to store data in the cloud. For more information, see the Amazon Simple Storage Service User Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/",
"title": "Amazon Redshift Cluster Management Guide"
}
],
"text": "Amazon Redshift is a data warehouse service in the cloud. For more information, see the Amazon Redshift Cluster Management Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [],
"text": "Prerequisites",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-learn-copy-redshift.html",
"title": "Before You Begin: Configure COPY Options and Load Data"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-prereq.html",
"title": "Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster"
}
],
"text": "Before you start this tutorial, complete the prerequisites described in Before You Begin: Configure COPY Options and Load Data and  Set up Pipeline, Create a Security Group, and Create an Amazon Redshift Cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-create",
"main_header": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-define-objects-console",
"text": "Create the Pipeline"
},
"links": [],
"text": "First, create the pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"title": "Syntax"
}
],
"text": "This pipeline relies on the options of the Syntax.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-define-objects-console",
"text": "Create the Pipeline"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html",
"title": "Load Data from Amazon S3 into Amazon Redshift"
}
],
"text": "It uses the Copy to Redshift template in the AWS Data Pipeline console. For information about this template, see Load Data from Amazon S3 into Amazon Redshift.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-define-objects-console",
"main_header": "Create the Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-save-pipeline-console",
"text": "Save and Validate Your Pipeline"
},
"links": [],
"text": "You can save your pipeline definition at any point during the creation process. As soon as you save your pipeline definition, AWS Data Pipeline looks for syntax errors and missing values in your pipeline definition. If your pipeline is incomplete or incorrect, AWS Data Pipeline generates validation errors and warnings. Warning messages are informational only, but you must fix any error messages before you can activate your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-save-pipeline-console",
"main_header": "Save and Validate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-activate-pipeline-console",
"text": "Activate Your Pipeline"
},
"links": [],
"text": "Activate your pipeline to start creating and processing runs.\nThe pipeline starts based on the schedule and period in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-activate-pipeline-console",
"main_header": "Activate Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-execution-pipeline-console",
"text": "Monitor the Pipeline Runs"
},
"links": [],
"text": "After you activate your pipeline, you are taken to the Execution details page where you can monitor the progress of your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-execution-pipeline-console",
"main_header": "Monitor the Pipeline Runs",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-create",
"text": "Copy Data to Amazon Redshift Using the AWS Data Pipeline Console"
},
"h2": {
"urllink": "#dp-copydata-redshift-delete-pipeline-console",
"text": "(Optional) Delete Your Pipeline"
},
"links": [],
"text": "To stop incurring charges, delete your pipeline. Deleting your pipeline deletes the pipeline definition and all associated objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html#dp-copydata-redshift-delete-pipeline-console",
"main_header": "(Optional) Delete Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-cli",
"text": "Copy Data to Amazon Redshift Using the Command Line"
},
"links": [],
"text": "This tutorial demonstrates how to copy data from Amazon S3 to Amazon Redshift. You'll create a new table in Amazon Redshift, and then use AWS Data Pipeline to transfer data to this table from a public Amazon S3 bucket, which contains sample input data in CSV format. The logs are saved to an Amazon S3 bucket that you own.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html#dp-copydata-redshift-cli",
"main_header": "Copy Data to Amazon Redshift Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-cli",
"text": "Copy Data to Amazon Redshift Using the Command Line"
},
"links": [
{
"url": "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/",
"title": "Amazon Simple Storage Service User Guide"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/",
"title": "Amazon Redshift Cluster Management Guide"
}
],
"text": "Amazon S3 is a web service that enables you to store data in the cloud. For more information, see the Amazon Simple Storage Service User Guide. Amazon Redshift is a data warehouse service in the cloud. For more information, see the Amazon Redshift Cluster Management Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html#dp-copydata-redshift-cli",
"main_header": "Copy Data to Amazon Redshift Using the Command Line",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "This example scenario shows how to copy data from an Amazon S3 bucket to Amazon Redshift.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html#dp-copydata-redshift-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "This is the full pipeline definition JSON file followed by an explanation for each of its sections. We recommend that you use a text editor that can help you verify the syntax of JSON-formatted files, and name the file using the .json file extension.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html#dp-copydata-redshift-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-define-pipeline-cli",
"text": "Define a Pipeline in JSON Format"
},
"links": [],
"text": "For more information about these objects, see the following documentation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html#dp-copydata-redshift-define-pipeline-cli",
"main_header": "Define a Pipeline in JSON Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "This example uses an input data node, an output data node, and a database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "Input Data Node",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "The input S3DataNode pipeline component defines the location of the input data in Amazon S3 and the data format of the input data. For more information, see S3DataNode.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "This input component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "Output Data Node",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"title": "RedshiftDataNode"
}
],
"text": "The output RedshiftDataNode pipeline component defines a location for the output data; in this case, a table in an Amazon Redshift database. For more information, see RedshiftDataNode. This output component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [],
"text": "Database",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-node-cli",
"text": "Data Nodes"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"title": "RedshiftDatabase"
}
],
"text": "The RedshiftDatabase component is defined by the following fields. For more information, see RedshiftDatabase.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-node-cli.html#dp-copydata-redshift-node-cli",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-resource-cli",
"text": "Resource"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
}
],
"text": "This is a definition of the computational resource that performs the copy operation. In this example, AWS Data Pipeline should automatically create an EC2 instance to perform the copy task and terminate the instance after the task completes. The fields defined here control the creation and function of the instance that does the work. For more information, see Ec2Resource.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html#dp-copydata-redshift-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-resource-cli",
"text": "Resource"
},
"links": [],
"text": "The Ec2Resource is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-resource-cli.html#dp-copydata-redshift-resource-cli",
"main_header": "Resource",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-activity-cli",
"text": "Activity"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"title": "RedshiftCopyActivity"
}
],
"text": "The last section in the JSON file is the definition of the activity that represents the work to perform. In this case, we use a RedshiftCopyActivity component to copy data from Amazon S3 to Amazon Redshift. For more information, see RedshiftCopyActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html#dp-copydata-redshift-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-activity-cli",
"text": "Activity"
},
"links": [],
"text": "The RedshiftCopyActivity component is defined by the following fields:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-activity-cli.html#dp-copydata-redshift-activity-cli",
"main_header": "Activity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-define-pipeline-cli.html",
"title": "Define a Pipeline in JSON Format"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "You must upload your pipeline definition and activate your pipeline.\nIn the following example commands, replace pipeline_name with a label for your pipeline and \npipeline_file with the fully-qualified path for the pipeline definition .json file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "AWS CLI",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/create-pipeline.html",
"title": "create-pipeline"
}
],
"text": "To create your pipeline definition and activate your pipeline, use the following create-pipeline command. Note the ID of your pipeline, because you'll use this value with most CLI commands.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/put-pipeline-definition.html",
"title": "put-pipeline-definition"
}
],
"text": "To upload your pipeline definition, use the following put-pipeline-definition command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [],
"text": "If you pipeline validates successfully, the validationErrors field is empty. You should review any warnings.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/activate-pipeline.html",
"title": "activate-pipeline"
}
],
"text": "To activate your pipeline, use the following activate-pipeline command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-copydata-redshift-upload-cli",
"text": "Upload and Activate the Pipeline Definition"
},
"links": [
{
"url": "https://docs.aws.amazon.com/cli/latest/reference/datapipeline/list-pipelines.html",
"title": "list-pipelines"
}
],
"text": "You can verify that your pipeline appears in the pipeline list using the following list-pipelines command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-upload-cli.html#dp-copydata-redshift-upload-cli",
"main_header": "Upload and Activate the Pipeline Definition",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html",
"title": "Tutorials"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-cli.html",
"title": "Copy Data to Amazon Redshift Using the Command Line"
}
]
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"links": [],
"text": "This section explains the syntax for using expressions and functions in pipelines, including the associated data types.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-expressions-functions",
"main_header": "Pipeline Expressions and Functions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"links": [],
"text": "The following types of data can be set as field values.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-pipeline-datatypes",
"main_header": "Simple Data Types",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-datetime",
"text": "DateTime"
},
"links": [],
"text": "AWS Data Pipeline supports the date and time expressed in \"YYYY-MM-DDTHH:MM:SS\" format in UTC/GMT only. The following example sets the startDateTime field of a Schedule object to 1/15/2012, 11:59 p.m., in the UTC/GMT timezone.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-datetime",
"main_header": "DateTime",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-numeric",
"text": "Numeric"
},
"links": [],
"text": "AWS Data Pipeline supports both integers and floating-point values.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-numeric",
"main_header": "Numeric",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-object-reference",
"text": "Object References"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"title": "Referencing Fields and Objects"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
],
"text": "An object in the pipeline definition. This can either be the current object, the name of an object defined elsewhere in the pipeline, or an object that lists the current object in a field, referenced by the node keyword. For more information about node, see Referencing Fields and Objects. For more information about the pipeline object types, see Pipeline Object Reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-object-reference",
"main_header": "Object References",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-period",
"text": "Period"
},
"links": [],
"text": "Indicates how often a scheduled event should run. It's expressed in the format \"N [years|months|weeks|days|hours|minutes]\", where N is a positive integer value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-period",
"main_header": "Period",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-period",
"text": "Period"
},
"links": [],
"text": "The minimum period is 15 minutes and the maximum period is 3 years.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-period",
"main_header": "Period",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-period",
"text": "Period"
},
"links": [],
"text": "The following example sets the period field of the Schedule object to 3 hours. This creates a schedule that runs every three hours.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-period",
"main_header": "Period",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-section",
"text": "String"
},
"links": [],
"text": "Standard string values. Strings must be surrounded by double quotes (\"). You can use the backslash character (\\) to escape characters in a string. Multiline strings are not supported.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-section",
"main_header": "String",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-section",
"text": "String"
},
"links": [],
"text": "The following examples show examples of valid string values for the id field.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-section",
"main_header": "String",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-section",
"text": "String"
},
"links": [],
"text": "Strings can also contain expressions that evaluate to string values. These are inserted into the string, and are delimited with: \"#{\" and \"}\". The following example uses an expression to insert the name of the current object into a path.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-section",
"main_header": "String",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-expressions-functions",
"text": "Pipeline Expressions and Functions"
},
"h2": {
"urllink": "#dp-pipeline-datatypes",
"text": "Simple Data Types"
},
"h3": {
"urllink": "#dp-datatype-section",
"text": "String"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"title": "Referencing Fields and Objects"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"title": "Expression Evaluation"
}
],
"text": "For more information about using expressions, see Referencing Fields and Objects and Expression Evaluation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-expressions-functions.html#dp-datatype-section",
"main_header": "String",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"links": [],
"text": "Expressions enable you to share a value across related objects. Expressions are processed by the AWS Data Pipeline web service at runtime, ensuring that all expressions are substituted with the value of the expression.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions",
"main_header": "Expressions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"links": [],
"text": "Expressions are delimited by: \"#{\" and \"}\". You can use an expression in any pipeline definition object where a string is legal. If a slot is a reference or one of type ID, NAME, TYPE, SPHERE, its value is not evaluated and used verbatim.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions",
"main_header": "Expressions",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"title": "Expression Evaluation"
}
],
"text": "The following expression calls one of the AWS Data Pipeline functions. For more information, see Expression Evaluation.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions",
"main_header": "Expressions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "Expressions can use fields of the current object where the expression exists, or fields of another object that is linked by a reference.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "A slot format consists of a creation time followed by the object creation time, such as @S3BackupLocation_2018-01-31T11:05:33.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "You can also reference the exact slot ID specified in the pipeline definition, such as the slot ID of the Amazon S3 backup location. To reference the slot ID, use #{parent.@id}.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "In the following example, the filePath field references the id field in the same object to form a file name. The value of filePath evaluates to \"s3://mybucket/ExampleDataNode.csv\".",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "To use a field that exists on another object linked by a reference, use the node keyword. This keyword is only available with alarm and precondition objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "Continuing with the previous example, an expression in an SnsAlarm can refer to the date and time range in a Schedule, because the S3DataNode references both.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-pipeline-expressions-reference",
"text": "Referencing Fields and Objects"
},
"links": [],
"text": "Specifically, FailureNotify's message field can use the @scheduledStartTime and @scheduledEndTime runtime fields from ExampleSchedule, because ExampleDataNode's onFail field references FailureNotify and its schedule field references ExampleSchedule.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-pipeline-expressions-reference",
"main_header": "Referencing Fields and Objects",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-nested",
"text": "Nested Expressions"
},
"links": [],
"text": "AWS Data Pipeline allows you to nest values to create more complex expressions. For example, to perform a time calculation (subtract 30 minutes from the scheduledStartTime) and format the result to use in a pipeline definition, you could use the following expression in an activity:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-nested",
"main_header": "Nested Expressions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-nested",
"text": "Nested Expressions"
},
"links": [],
"text": "and using the node prefix if the expression is part of an SnsAlarm or Precondition:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-nested",
"main_header": "Nested Expressions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-list-function",
"text": "Lists"
},
"links": [],
"text": "Expressions can be evaluated on lists and functions on lists. For example, assume that a list is defined like the following: \"myList\":[\"one\",\"two\"]. If this list is used in the expression #{'this is ' + myList}, it will evaluate to [\"this is one\", \"this is two\"]. If you have two lists, Data Pipeline will ultimately flatten them in their evaluation. For example, if myList1 is defined as [1,2] and myList2 is defined as [3,4] then the expression [#{myList1}, #{myList2}] will evaluate to [1,2,3,4].",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-list-function",
"main_header": "Lists",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-node",
"text": "Node Expression"
},
"links": [],
"text": "AWS Data Pipeline uses the #{node.*} expression in either SnsAlarm or PreCondition for a back-reference to a pipeline component's parent object. Since SnsAlarm and PreCondition are referenced from an activity or resource with no reference back from them, node provides the way to refer to the referrer. For example, the following pipeline definition demonstrates how a failure notification can use node to make a reference to its parent, in this case ShellCommandActivity, and include the parent's scheduled start and end times in the SnsAlarm message. The scheduledStartTime reference on ShellCommandActivity does not require the node prefix because scheduledStartTime refers to itself.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-node",
"main_header": "Node Expression",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-node",
"text": "Node Expression"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html#dp-userdefined-fields",
"title": "User-Defined Fields"
}
],
"text": "AWS Data Pipeline supports transitive references for user-defined fields, but not runtime fields. A transitive reference is a reference between two pipeline components that depends on another pipeline component as the intermediary. The following example shows a reference to a transitive user-defined field and a reference to a non-transitive runtime field, both of which are valid. For more information, see User-Defined Fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-node",
"main_header": "Node Expression",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-expressions",
"text": "Expressions"
},
"h2": {
"urllink": "#dp-datatype-functions",
"text": "Expression Evaluation"
},
"links": [],
"text": "AWS Data Pipeline provides a set of functions that you can use to calculate the value of a field. The following example uses the makeDate function to set the startDateTime field of a Schedule object to \"2011-05-24T0:00:00\" GMT/UTC.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-expressions.html#dp-datatype-functions",
"main_header": "Expression Evaluation",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-math",
"text": "Mathematical Functions"
},
"links": [],
"text": "The following functions are available for working with numerical values.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-math.html#dp-pipeline-reference-functions-math",
"main_header": "Mathematical Functions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-string",
"text": "String Functions"
},
"links": [],
"text": "The following functions are available for working with string values.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-string.html#dp-pipeline-reference-functions-string",
"main_header": "String Functions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-reference-functions-datetime",
"text": "Date and Time Functions"
},
"links": [],
"text": "The following functions are available for working with DateTime values. For the examples, the value of myDateTime is May 24, 2011 @ 5:10 pm GMT.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-reference-functions-datetime.html#dp-pipeline-reference-functions-datetime",
"main_header": "Date and Time Functions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-characters",
"text": "Special Characters"
},
"links": [],
"text": "AWS Data Pipeline uses certain characters that have a special meaning in pipeline definitions, as shown in the following table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-characters.html#dp-pipeline-characters",
"main_header": "Special Characters",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [],
"text": "You can use the following pipeline objects and components in your pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-pipeline-objects",
"text": "Pipeline Object Reference"
},
"links": [],
"text": "The following is the object hierarchy for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html#dp-pipeline-objects",
"main_header": "Pipeline Object Reference",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-object-datanodes",
"text": "Data Nodes"
},
"links": [],
"text": "The following are the AWS Data Pipeline data node objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html#dp-object-datanodes",
"main_header": "Data Nodes",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"links": [],
"text": "Defines a data node using DynamoDB, which is specified as an input to a HiveActivity or EMRActivity object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dp-object-dynamodbdatanode",
"main_header": "DynamoDBDataNode",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. This object references two other objects that you'd define in the same pipeline definition file. CopyPeriod is a Schedule object and Ready is a precondition object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdatanode",
"text": "DynamoDBDataNode"
},
"h2": {
"urllink": "#dynamodbdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdatanode.html#dynamodbdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"links": [],
"text": "Defines a data node using MySQL.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#dp-object-mysqldatanode",
"main_header": "MySqlDataNode",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. This object references two other objects that you'd define in the same pipeline definition file. CopyPeriod is a Schedule object and Ready is a precondition object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-mysqldatanode",
"text": "MySqlDataNode"
},
"h2": {
"urllink": "#mysqldatanode-example",
"text": "Example"
},
"h3": {
"urllink": "#mysqldatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-mysqldatanode.html#mysqldatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"links": [],
"text": "Defines a data node using Amazon Redshift. RedshiftDataNode represents the properties of the data inside a database, such as a data table, used by your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#dp-object-redshiftdatanode",
"main_header": "RedshiftDataNode",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatanode",
"text": "RedshiftDataNode"
},
"h2": {
"urllink": "#redshiftdatanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatanode.html#redshiftdatanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"links": [],
"text": "Defines a data node using Amazon S3. By default, the S3DataNode uses server-side encryption. If you would like to disable this, set s3EncryptionType to NONE.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#dp-object-s3datanode",
"main_header": "S3DataNode",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. This object references another object that you'd define in the same pipeline definition file. CopyPeriod is a Schedule object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3datanode",
"text": "S3DataNode"
},
"h2": {
"urllink": "#s3datanode-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html#s3datanode-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"links": [],
"text": "Defines a data node using SQL.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#dp-object-sqldatanode",
"main_header": "SqlDataNode",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#example-sql-data-node",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. This object references two other objects that you'd define in the same pipeline definition file. CopyPeriod is a Schedule object and Ready is a precondition object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#example-sql-data-node",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqldatanode",
"text": "SqlDataNode"
},
"h2": {
"urllink": "#sql-data-node-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html#sql-data-node-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-datanodes.html",
"title": "Data Nodes"
}
]
},
{
"h1": {
"urllink": "#dp-object-activities",
"text": "Activities"
},
"links": [],
"text": "The following are the AWS Data Pipeline activity objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html#dp-object-activities",
"main_header": "Activities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqldatanode.html",
"title": "SqlDataNode"
}
],
"text": "Copies data from one location to another. CopyActivity supports S3DataNode and SqlDataNode as input and output and the copy operation is normally performed record-by-record. However, CopyActivity provides a high-performance Amazon S3 to Amazon S3 copy when all the following conditions are met:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "If you provide compressed data files as input and do not indicate this using the compression field on the S3 data nodes, CopyActivity might fail. In this case, CopyActivity does not properly detect the end of record character and the operation fails. Further, CopyActivity supports copying from a directory to another directory and copying a file to a directory, but record-by-record copy occurs when copying a directory to a file. Finally, CopyActivity does not support copying multipart Amazon S3 files.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "CopyActivity has specific limitations to its CSV support. When you use an S3DataNode as input for CopyActivity, you can only use a Unix/Linux variant of the CSV data file format for the Amazon S3 input and output fields. The Unix/Linux variant requires the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "Windows-based systems typically use a different end-of-record character sequence: a carriage return and line feed together (ASCII value 13 and ASCII value 10). You must accommodate this difference using an additional mechanism, such as a pre-copy script to modify the input data, to ensure that CopyActivity can properly detect the end of a record; otherwise, the CopyActivity fails repeatedly.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"links": [],
"text": "When using CopyActivity to export from a PostgreSQL RDS object to a TSV data format, the default NULL character is \\n.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#dp-object-copyactivity",
"main_header": "CopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. This object references three other objects that you would define in the same pipeline definition file. CopyPeriod is a Schedule object and InputData and OutputData are data node objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-copyactivity",
"text": "CopyActivity"
},
"h2": {
"urllink": "#copyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html#copyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"links": [],
"text": "Runs an EMR cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#dp-object-emractivity",
"main_header": "EmrActivity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"links": [],
"text": "AWS Data Pipeline uses a different format for steps than Amazon EMR; for example, AWS Data Pipeline uses comma-separated arguments after the JAR name in the EmrActivity step field. The following example shows a step formatted for Amazon EMR, followed by its AWS Data Pipeline equivalent:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#dp-object-emractivity",
"main_header": "EmrActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "The following is an example of this object type. This example uses older versions of Amazon EMR. Verify this example for correctness with the version of Amazon EMR cluster that you are using.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "This object references three other objects that you would define in the same pipeline definition file. MyEmrCluster is an EmrCluster object and MyS3Input and MyS3Output are S3DataNode objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-example",
"text": "Examples"
},
"links": [],
"text": "Hadoop 2.x (AMI 3.x)",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emractivity",
"text": "EmrActivity"
},
"h2": {
"urllink": "#emractivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html#emractivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "Runs a MapReduce job on a cluster. The cluster can be an EMR cluster managed by AWS Data Pipeline or another resource if you use TaskRunner. Use HadoopActivity when you want to run work in parallel. This allows you to use the scheduling resources of the YARN framework or the MapReduce resource negotiator in Hadoop 1. If you would like to run work sequentially using the Amazon EMR Step action, you can still use EmrActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#dp-object-hadoopactivity",
"main_header": "HadoopActivity",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "HadoopActivity using an EMR cluster managed by AWS Data Pipeline",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "The following HadoopActivity object uses an EmrCluster resource to run a program:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "Here is the corresponding MyEmrCluster, which configures the FairScheduler and queues in YARN for Hadoop 2-based AMIs:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "This is the EmrCluster you use to configure FairScheduler in Hadoop 1:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "The following EmrCluster configures CapacityScheduler for Hadoop 2-based AMIs:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "HadoopActivity using an existing EMR cluster",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-example",
"text": "Examples"
},
"links": [],
"text": "In this example, you use workergroups and a TaskRunner to run a program on an existing EMR cluster. The following pipeline definition uses HadoopActivity to:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hadoopactivity",
"text": "HadoopActivity"
},
"h2": {
"urllink": "#hadoopactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html#hadoopactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [],
"text": "Runs a Hive query on an EMR cluster. HiveActivity makes it easier to set up an Amazon EMR activity and automatically creates Hive tables based on input data coming in from either Amazon S3 or Amazon RDS. All you need to specify is the HiveQL to run on the source data. AWS Data Pipeline automatically creates Hive tables with ${input1}, ${input2}, and so on, based on the input fields in the HiveActivity object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [],
"text": "For Amazon S3 inputs, the dataFormat field is used to create the Hive column names.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"links": [],
"text": "For MySQL (Amazon RDS) inputs, the column names for the SQL query are used to create the Hive column names.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#dp-object-hiveactivity",
"main_header": "HiveActivity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. This object references three other objects that you define in the same pipeline definition file. MySchedule is a Schedule object and MyS3Input and MyS3Output are data node objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hiveactivity",
"text": "HiveActivity"
},
"h2": {
"urllink": "#hiveactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html#hiveactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"links": [],
"text": "Runs a Hive query on an EMR cluster. HiveCopyActivity makes it easier to copy data between DynamoDB tables. HiveCopyActivity accepts a HiveQL statement to filter input data from DynamoDB at the column and row level.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#dp-object-hivecopyactivity",
"main_header": "HiveCopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-example",
"text": "Example"
},
"links": [],
"text": "The following example shows how to use HiveCopyActivity and DynamoDBExportDataFormat to copy data from one DynamoDBDataNode to another, while filtering data, based on a time stamp.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-hivecopyactivity",
"text": "HiveCopyActivity"
},
"h2": {
"urllink": "#hivecopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html#hivecopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"links": [],
"text": "PigActivity provides native support for Pig scripts in AWS Data Pipeline without the requirement to use ShellCommandActivity or EmrActivity. In addition, PigActivity supports data staging. When the stage field is set to true, AWS Data Pipeline stages the input data as a schema in Pig without additional code from the user.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#dp-object-pigactivity",
"main_header": "PigActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-example",
"text": "Example"
},
"links": [],
"text": "The following example pipeline shows how to use PigActivity. The example pipeline performs the following steps:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-example",
"text": "Example"
},
"links": [],
"text": "The content of pigTestScript.q is as follows.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-pigactivity",
"text": "PigActivity"
},
"h2": {
"urllink": "#pigactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html#pigactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"links": [],
"text": "Copies data from DynamoDB or Amazon S3 to Amazon Redshift. You can load data into a new table, or easily merge data into an existing table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#dp-object-redshiftcopyactivity",
"main_header": "RedshiftCopyActivity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"links": [],
"text": "Here is an overview of a use case in which to use RedshiftCopyActivity:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#dp-object-redshiftcopyactivity",
"main_header": "RedshiftCopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3datanode.html",
"title": "S3DataNode"
}
],
"text": "In addition, RedshiftCopyActivity let's you work with an S3DataNode, since it supports a manifest file. For more information, see S3DataNode.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#dp-object-redshiftcopyactivity",
"main_header": "RedshiftCopyActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-emptyasnull",
"title": "EMPTYASNULL"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-ignoreblanklines",
"title": "IGNOREBLANKLINES"
},
{
"url": "https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html",
"title": "Data Conversion Parameters"
}
],
"text": "To ensure formats conversion, this example uses EMPTYASNULL and IGNOREBLANKLINES special conversion parameters in commandOptions. For information, see Data Conversion Parameters in the Amazon Redshift Database Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "The following example pipeline definition shows an activity that uses the APPEND insert mode:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "APPEND operation adds items to a table regardless of the primary or sort keys. For example, if you have the following table, you can append a record with the same ID and user value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [],
"text": "You can append a record with the same ID and user value:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-example",
"text": "Example"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html",
"title": "Copy Data to Amazon Redshift Using AWS Data Pipeline"
}
],
"text": "For a tutorial, see Copy Data to Amazon Redshift Using AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftcopyactivity",
"text": "RedshiftCopyActivity"
},
"h2": {
"urllink": "#redshiftcopyactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html#redshiftcopyactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "Runs a command or script. You can use ShellCommandActivity to run time-series or cron-like scheduled tasks.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "When the stage field is set to true and used with an S3DataNode, ShellCommandActivity supports the concept of staging data, which means that you can move data from Amazon S3 to a stage location, such as Amazon EC2 or your local environment, perform work on the data using scripts and the ShellCommandActivity, and move it back to Amazon S3.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "In this case, when your shell command is connected to an input S3DataNode, your shell scripts operate directly on the data using ${INPUT1_STAGING_DIR}, ${INPUT2_STAGING_DIR}, and other fields, referring to the ShellCommandActivity input fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "Similarly, output from the shell-command can be staged in an output directory to be automatically pushed to Amazon S3, referred to by ${OUTPUT1_STAGING_DIR}, ${OUTPUT2_STAGING_DIR}, and so on.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "These expressions can pass as command-line arguments to the shell-command for you to use in data transformation logic.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"links": [],
"text": "ShellCommandActivity returns Linux-style error codes and strings. If a ShellCommandActivity results in error, the error returned is a non-zero value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#dp-object-shellcommandactivity",
"main_header": "ShellCommandActivity",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandactivity",
"text": "ShellCommandActivity"
},
"h2": {
"urllink": "#shellcommandactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandactivity.html#shellcommandactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"links": [],
"text": "Runs an SQL query (script) on a database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#dp-object-sqlactivity",
"main_header": "SqlActivity",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-sqlactivity",
"text": "SqlActivity"
},
"h2": {
"urllink": "#sqlactivity-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html#sqlactivity-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-activities.html",
"title": "Activities"
}
]
},
{
"h1": {
"urllink": "#dp-object-resources",
"text": "Resources"
},
"links": [],
"text": "The following are the AWS Data Pipeline resource objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html#dp-object-resources",
"main_header": "Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"links": [],
"text": "An Amazon EC2 instance that performs the work defined by a pipeline activity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#dp-object-ec2resource",
"main_header": "Ec2Resource",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-default-instance-types.html",
"title": "Default Amazon EC2 Instances by AWS Region"
}
],
"text": "For information about default Amazon EC2 instances that AWS Data Pipeline creates if you do not specify an instance, see Default Amazon EC2 Instances by AWS Region.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#dp-object-ec2resource",
"main_header": "Ec2Resource",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [],
"text": "EC2-Classic",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [],
"text": "The following example object launches an EC2 instance into EC2-Classic, with some optional fields set.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [],
"text": "EC2-VPC",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-example",
"text": "Examples"
},
"links": [],
"text": "The following example object launches an EC2 instance into a nondefault VPC, with some optional fields set.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-ec2resource",
"text": "Ec2Resource"
},
"h2": {
"urllink": "#ec2resource-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html#ec2resource-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"title": "HadoopActivity"
}
],
"text": "Represents the configuration of an Amazon EMR cluster. This object is used by EmrActivity and HadoopActivity to launch a cluster.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-object-emrcluster",
"main_header": "EmrCluster",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-schedulers",
"text": "Schedulers"
},
"links": [],
"text": "Schedulers provide a way to specify resource allocation and job prioritization within a Hadoop cluster. Administrators or users can choose a scheduler for various classes of users and applications. A scheduler could use queues to allocate resources to users and applications. You set up those queues when you create the cluster. You can then set up priority for certain types of work and user over others. This provides for efficient use of cluster resources, while allowing more than one user to submit work to the cluster. There are three types of scheduler available:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-schedulers",
"main_header": "Schedulers",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"links": [],
"text": "An Amazon EMR release is a set of open-source applications from the big data ecosystem. Each release comprises different big data applications, components, and features that you select to have Amazon EMR install and configure when you create a cluster. You specify the release version using the release label. Release labels are in the form emr-x.x.x. For example, emr-5.30.0. Amazon EMR clusters based on release label emr-4.0.0 and later use the releaseLabel property to specify the release label of an EmrCluster object. Earlier versions use the amiVersion property.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emrcluster-release-versions",
"main_header": "Amazon EMR Release Versions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-task-runner-latest",
"text": "Use the latest version of Task Runner"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html",
"title": "Working with Task Runner"
},
{
"url": "http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/emr-configure-apps.html",
"title": "Configuring Applications"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"title": "EmrConfiguration"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"title": "Property"
}
],
"text": "If you are using a self-managed EmrCluster object with a release label, use the latest Task Runner. For more information about Task Runner, see Working with Task Runner. You can configure property values for all Amazon EMR configuration classifications. For more information, see Configuring Applications in the Amazon EMR Release Guide, the EmrConfiguration, and Property object references.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-task-runner-latest",
"main_header": "Use the latest version of Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"links": [],
"text": "AWS Data Pipeline only supports release version 6.1.0 (emr-6.1.0).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-not-supported",
"main_header": "Amazon EMR 6x Series Release Versions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "The Amazon EMR 6.x release series uses Hadoop version 3.x, which introduced breaking changes in how Hadoop's classpath is evaluated as compared to Hadoop version 2.x. Common libraries like Joda-Time were removed from the classpath.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"title": "HadoopActivity"
}
],
"text": "If EmrActivity or HadoopActivity runs a Jar file that has dependencies on a library that was removed in Hadoop 3.x, the step fails with the error java.lang.NoClassDefFoundError or java.lang.ClassNotFoundException. This can happen for Jar files that ran with no issues using Amazon EMR 5.x release versions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "To fix the issue, you must copy Jar file dependencies to the Hadoop classpath on an EmrCluster object before starting the EmrActivity or the HadoopActivity. We provide a bash script to do this. The bash script is available in the following location, where MyRegion is the AWS Region where your EmrCluster object runs, for example us-west-2.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "The way to run the script depends on whether EmrActivity or HadoopActivity runs on a resource managed by AWS Data Pipeline or runs on a self-managed resource.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "If you use a resource managed by AWS Data Pipeline, add a bootstrapAction to the EmrCluster object. The bootstrapAction specifies the script and the Jar files to copy as arguments. You can add up to 255 bootstrapAction fields per EmrCluster object, and you can add a bootstrapAction field to an EmrCluster object that already has bootstrap actions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "To specify this script as a bootstrap action, use the following syntax, where JarFileRegion is the Region where the Jar file is saved, and each MyJarFilen is the absolute path in Amazon S3 of a Jar file to be copied to the Hadoop classpath. Do not specify Jar files that are in the Hadoop classpath by default.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "The following example specifies a bootstrap action that copies two Jar files in Amazon S3: my-jar-file.jar and the emr-dynamodb-tool-4.14.0-jar-with-dependencies.jar. The Region used in the example is us-west-2.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "You must save and activate the pipeline for the change to the new bootstrapAction to take effect.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "If you use a self-managed resource, you can download the script to the cluster instance and run it from the command line using SSH. The script creates a directory named /etc/hadoop/conf/shellprofile.d and a file named datapipeline-jars.sh in that directory. The jar files provided as command-line arguments are copied to a directory that the script creates named /home/hadoop/datapipeline_jars. If your cluster is set up differently, modify the script appropriately after downloading it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#dp-emrcluster-release-versions",
"text": "Amazon EMR Release Versions"
},
"h3": {
"urllink": "#dp-emrcluster-considerations",
"text": "Considerations and Limitations"
},
"h4": {
"urllink": "#dp-emr-6-not-supported",
"text": "Amazon EMR 6x Series Release Versions"
},
"h5": {
"urllink": "#dp-emr-6-classpath",
"text": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies"
},
"links": [],
"text": "The syntax for running the script on the command line is slightly different from using the bootstrapAction shown in the previous example. Use spaces instead of commas between arguments, as shown in the following example.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#dp-emr-6-classpath",
"main_header": "Amazon EMR 6.1.0 Release and Hadoop 3.x Jar Dependencies",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#w24aac32c17b9c11",
"text": "Amazon EMR permissions"
},
"links": [],
"text": "When you create a custom IAM role, carefully consider the minimum permissions necessary for your cluster to perform its work. Be sure to grant access to required resources, such as files in Amazon S3 or data in Amazon RDS, Amazon Redshift, or DynamoDB. If you wish to set visibleToAllUsers to False, your role must have the proper permissions to do so. Note that DataPipelineDefaultRole does not have these permissions. You must either provide a union of the DefaultDataPipelineResourceRole and DataPipelineDefaultRole roles as the EmrCluster object role, or create your own role for this purpose.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#w24aac32c17b9c11",
"main_header": "Amazon EMR permissions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrcluster",
"text": "EmrCluster"
},
"h2": {
"urllink": "#emrcluster-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html#emrcluster-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#emrcluster-example",
"text": "Examples"
},
"links": [],
"text": "The following are examples of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/emrcluster-example.html#emrcluster-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"links": [],
"text": "HttpProxy allows you to configure your own proxy and make Task Runner access the AWS Data Pipeline service through it. You do not need to configure a running Task Runner with this information.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#dp-object-httpproxy",
"main_header": "HttpProxy",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#example9",
"text": "Example of an HttpProxy in TaskRunner"
},
"links": [],
"text": "The following pipeline definition shows an HttpProxy object:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#example9",
"main_header": "Example of an HttpProxy in TaskRunner",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-httpproxy",
"text": "HttpProxy"
},
"h2": {
"urllink": "#httpproxy-slots",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-httpproxy.html#httpproxy-slots",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-resources.html",
"title": "Resources"
}
]
},
{
"h1": {
"urllink": "#dp-object-preconditions",
"text": "Preconditions"
},
"links": [],
"text": "The following are the AWS Data Pipeline precondition objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html#dp-object-preconditions",
"main_header": "Preconditions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"links": [],
"text": "A precondition to check that data exists in a DynamoDB table.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists",
"main_header": "DynamoDBDataExists",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbdataexists",
"text": "DynamoDBDataExists"
},
"h2": {
"urllink": "#dp-dynamodbdataexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbdataexists.html#dp-dynamodbdataexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"links": [],
"text": "A precondition to check that the DynamoDB table exists.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists",
"main_header": "DynamoDBTableExists",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-dynamodbtableexists",
"text": "DynamoDBTableExists"
},
"h2": {
"urllink": "#dp-dynamodbtableexists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-dynamodbtableexists.html#dp-dynamodbtableexists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"links": [],
"text": "Checks whether a data node object exists.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#dp-object-exists",
"main_header": "Exists",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. The InputData object references this object, Ready, plus another object that you'd define in the same pipeline definition file. CopyPeriod is a Schedule object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-exists",
"text": "Exists"
},
"h2": {
"urllink": "#exists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-exists.html#exists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"links": [],
"text": "Checks whether a key exists in an Amazon S3 data node.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#dp-object-S3KeyExists",
"main_header": "S3KeyExists",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#dp-object-S3KeyExists-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.  The precondition will trigger when the key, s3://mybucket/mykey, referenced by the s3Key parameter, exists.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#dp-object-S3KeyExists-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#dp-object-S3KeyExists-example",
"text": "Example"
},
"links": [],
"text": "You can also use S3KeyExists as a precondition on the second pipeline that waits for the first pipeline to finish. To do so:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#dp-object-S3KeyExists-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-S3KeyExists",
"text": "S3KeyExists"
},
"h2": {
"urllink": "#S3KeyExists-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-S3KeyExists.html#S3KeyExists-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"links": [],
"text": "A precondition to check that the Amazon S3 objects with the given prefix (represented as a URI) are present.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#dp-object-s3prefixnotempty",
"main_header": "S3PrefixNotEmpty",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type using required, optional, and expression fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-s3prefixnotempty",
"text": "S3PrefixNotEmpty"
},
"h2": {
"urllink": "#s3prefixnotempty-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-s3prefixnotempty.html#s3prefixnotempty-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"links": [],
"text": "A Unix/Linux shell command that can be run as a precondition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#dp-object-shellcommandprecondition",
"main_header": "ShellCommandPrecondition",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellcommandprecondition",
"text": "ShellCommandPrecondition"
},
"h2": {
"urllink": "#shellcommandprecondition-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellcommandprecondition.html#shellcommandprecondition-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-preconditions.html",
"title": "Preconditions"
}
]
},
{
"h1": {
"urllink": "#dp-object-databases",
"text": "Databases"
},
"links": [],
"text": "The following are the AWS Data Pipeline database objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html#dp-object-databases",
"main_header": "Databases",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"links": [],
"text": "Defines a JDBC database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#dp-object-jdbcdatabase",
"main_header": "JdbcDatabase",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-jdbcdatabase",
"text": "JdbcDatabase"
},
"h2": {
"urllink": "#jdbcdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html#jdbcdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"links": [],
"text": "Defines an Amazon RDS database.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#dp-object-rdsdatabase",
"main_header": "RdsDatabase",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-example",
"text": "Example"
},
"links": [],
"text": "For the Oracle engine, the jdbcDriverJarUri field is required and you can specify the following driver: http://www.oracle.com/technetwork/database/features/jdbc/jdbc-drivers-12c-download-1958347.html. For the SQL Server engine, the jdbcDriverJarUri field is required and you can specify the following driver: https://www.microsoft.com/en-us/download/details.aspx?displaylang=en&id=11774. For the MySQL and PostgreSQL engines, the jdbcDriverJarUri field is optional.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-rdsdatabase",
"text": "RdsDatabase"
},
"h2": {
"urllink": "#rdsdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-rdsdatabase.html#rdsdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"links": [],
"text": "Defines an Amazon Redshift database. RedshiftDatabase represents the properties of the database used by your pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#dp-object-redshiftdatabase",
"main_header": "RedshiftDatabase",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-example",
"text": "Example"
},
"links": [],
"text": "By default, the object uses the Postgres driver, which requires the clusterId field. To use the Amazon Redshift driver, specify the Amazon Redshift database connection string from the Amazon Redshift console (starts with \"jdbc:redshift:\") in the connectionString field instead.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-redshiftdatabase",
"text": "RedshiftDatabase"
},
"h2": {
"urllink": "#redshiftdatabase-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftdatabase.html#redshiftdatabase-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-databases.html",
"title": "Databases"
}
]
},
{
"h1": {
"urllink": "#dp-object-dataformats",
"text": "Data Formats"
},
"links": [],
"text": "The following are the AWS Data Pipeline data format objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html#dp-object-dataformats",
"main_header": "Data Formats",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"links": [],
"text": "A comma-delimited data format where the column separator is a comma and the record separator is a newline character.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#dp-object-csv",
"main_header": "CSV Data Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-csv",
"text": "CSV Data Format"
},
"h2": {
"urllink": "#csv-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-csv.html#csv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"links": [],
"text": "A custom data format defined by a combination of a certain column separator, record separator, and escape character.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#dp-object-custom",
"main_header": "Custom Data Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-custom",
"text": "Custom Data Format"
},
"h2": {
"urllink": "#custom-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-custom.html#custom-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"title": "DynamoDBExportDataFormat"
}
],
"text": "Applies a schema to a DynamoDB table to make it accessible by a Hive query. DynamoDBDataFormat is used with a HiveActivity object and a DynamoDBDataNode input and output. DynamoDBDataFormat requires that you specify all columns in your Hive query. For more flexibility to specify certain columns in a Hive query or Amazon S3 support, see DynamoDBExportDataFormat.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dp-object-dynamodbdataformat",
"main_header": "DynamoDBDataFormat",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-example",
"text": "Example"
},
"links": [],
"text": "The following example shows how to use DynamoDBDataFormat to assign a schema to a DynamoDBDataNode input, which allows a HiveActivity object to access the data by named columns and copy the data to a DynamoDBDataNode output.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbdataformat",
"text": "DynamoDBDataFormat"
},
"h2": {
"urllink": "#dynamodbdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbdataformat.html#dynamodbdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"links": [],
"text": "Applies a schema to an DynamoDB table to make it accessible by a Hive query. Use DynamoDBExportDataFormat with a HiveCopyActivity object and DynamoDBDataNode or S3DataNode input and output. DynamoDBExportDataFormat has the following benefits:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dp-object-dynamodbexportdataformat",
"main_header": "DynamoDBExportDataFormat",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-example",
"text": "Example"
},
"links": [],
"text": "The following example shows how to use HiveCopyActivity and DynamoDBExportDataFormat to copy data from one DynamoDBDataNode to another, while filtering based on a time stamp.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-dynamodbexportdataformat",
"text": "DynamoDBExportDataFormat"
},
"h2": {
"urllink": "#dynamodbexportdataformat-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dynamodbexportdataformat.html#dynamodbexportdataformat-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"links": [],
"text": "A custom data format defined by a regular expression.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#dp-object-regex",
"main_header": "RegEx Data Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-regex",
"text": "RegEx Data Format"
},
"h2": {
"urllink": "#regex-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-regex.html#regex-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"links": [],
"text": "A comma-delimited data format where the column separator is a tab character and the record separator is a newline character.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#dp-object-tsv",
"main_header": "TSV Data Format",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-tsv",
"text": "TSV Data Format"
},
"h2": {
"urllink": "#tsv-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-tsv.html#tsv-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-dataformats.html",
"title": "Data Formats"
}
]
},
{
"h1": {
"urllink": "#dp-object-actions",
"text": "Actions"
},
"links": [],
"text": "The following are the AWS Data Pipeline action objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html#dp-object-actions",
"main_header": "Actions",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"links": [],
"text": "Sends an Amazon SNS notification message when an activity fails or finishes successfully.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#dp-object-snsalarm",
"main_header": "SnsAlarm",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. The values for node.input and node.output come from the data node or activity that references this object in its onSuccess field.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-snsalarm",
"text": "SnsAlarm"
},
"h2": {
"urllink": "#snsalarm-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-snsalarm.html#snsalarm-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"links": [],
"text": "An action to trigger the cancellation of a pending or unfinished activity, resource, or data node. AWS Data Pipeline attempts to put the activity, resource, or data node into the CANCELLED state if it does not start by the lateAfterTimeout value.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#dp-object-terminate",
"main_header": "Terminate",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"links": [],
"text": "You cannot terminate actions that include onSuccess, OnFail, or  onLateAction resources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#dp-object-terminate",
"main_header": "Terminate",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-example",
"text": "Example"
},
"links": [],
"text": "The following is an example of this object type. In this example, the onLateAction field of MyActivity contains a reference to the action DefaultAction1. When you provide an action for onLateAction, you must also provide a lateAfterTimeout value to indicate the period of time since the scheduled start of the pipeline after which the activity is considered late.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-terminate",
"text": "Terminate"
},
"h2": {
"urllink": "#terminate-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-terminate.html#terminate-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-actions.html",
"title": "Actions"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"links": [],
"text": "Defines the timing of a scheduled event, such as when an activity runs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#dp-object-schedule",
"main_header": "Schedule",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "The following is an example of this object type. It defines a schedule of every hour starting at 00:00:00 hours on 2012-09-01 and ending at 00:00:00 hours on 2012-10-01. The first period ends at 01:00:00 on 2012-09-01.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "The following pipeline will start at the FIRST_ACTIVATION_DATE_TIME and run every hour until 22:00:00 hours on 2014-04-25.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "The following pipeline will start at the FIRST_ACTIVATION_DATE_TIME and run every hour and complete after three occurrences.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "The following pipeline will start at 22:00:00 on 2014-04-25, run hourly, and end after three occurrences.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "On-demand using the Default object",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "On-demand with explicit Schedule object",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "The following examples demonstrate how a Schedule can be inherited from the default object, be explicitly set for that object, or be given by a parent reference:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "Schedule inherited from Default object",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "Explicit schedule on the object",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-example",
"text": "Examples"
},
"links": [],
"text": "Schedule from Parent reference",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-example",
"main_header": "Examples",
"images": [],
"container_type": "p",
"children_tags": [
"span"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-schedule",
"text": "Schedule"
},
"h2": {
"urllink": "#schedule-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html#schedule-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-utilities",
"text": "Utilities"
},
"links": [],
"text": "The following utility objects configure other pipeline objects:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html#dp-object-utilities",
"main_header": "Utilities",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"title": "HadoopActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"title": "HiveActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hivecopyactivity.html",
"title": "HiveCopyActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"title": "PigActivity"
}
],
"text": "Use with an Activity to run a shell script for preActivityTaskConfig and postActivityTaskConfig. This object is available for HadoopActivity, HiveActivity, HiveCopyActivity, and PigActivity. You specify an S3 URI and a list of arguments for the script.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#dp-object-shellscriptconfig",
"main_header": "ShellScriptConfig",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-example",
"text": "Example"
},
"links": [],
"text": "A ShellScriptConfig with arguments:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "This object includes the following fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-shellscriptconfig",
"text": "ShellScriptConfig"
},
"h2": {
"urllink": "#shellscriptconfig-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-shellscriptconfig.html#shellscriptconfig-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrcluster.html",
"title": "EmrCluster"
},
{
"url": "http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/",
"title": "http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/"
}
],
"text": "The EmrConfiguration object is the configuration used for EMR clusters with releases 4.0.0 or greater. Configurations (as a list) is a parameter to the RunJobFlow API call. The configuration API for Amazon EMR takes a classification and properties. AWS Data Pipeline uses EmrConfiguration with corresponding Property objects to configure an EmrCluster application such as Hadoop, Hive, Spark, or Pig on EMR clusters launched in a pipeline execution. Because configuration can only be changed for new clusters, you cannot provide a EmrConfiguration object for existing resources. For more information, see http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#dp-object-emrconfiguration",
"main_header": "EmrConfiguration",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-example",
"text": "Example"
},
"links": [],
"text": "The following configuration object sets the io.file.buffer.size and fs.s3.block.size properties in core-site.xml:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-example",
"text": "Example"
},
"links": [],
"text": "The corresponding pipeline object definition uses a EmrConfiguration object and a list of Property objects in the property field:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-example",
"text": "Example"
},
"links": [],
"text": "The following example is a nested configuration used to set the Hadoop environment with the hadoop-env classification:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-example",
"text": "Example"
},
"links": [],
"text": "The corresponding pipeline definition object that uses this configuration is below:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-example",
"text": "Example"
},
"links": [],
"text": "The following example modifies a Hive-specific property for an EMR cluster:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "This object includes the following fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-emrconfiguration",
"text": "EmrConfiguration"
},
"h2": {
"urllink": "#emrconfiguration-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emrconfiguration.html#emrconfiguration-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"links": [],
"text": "A single key-value property for use with an EmrConfiguration object.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#dp-object-property",
"main_header": "Property",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-example",
"text": "Example"
},
"links": [],
"text": "The following pipeline definition shows an EmrConfiguration object and corresponding Property objects to launch an EmrCluster:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-example",
"main_header": "Example",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "This object includes the following fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-object-property",
"text": "Property"
},
"h2": {
"urllink": "#property-syntax",
"text": "Syntax"
},
"links": [],
"text": "",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-property.html#property-syntax",
"main_header": "Syntax",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html",
"title": "Pipeline Object Reference"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-utilities.html",
"title": "Utilities"
}
]
},
{
"h1": {
"urllink": "#dp-using-task-runner",
"text": "Working with Task Runner"
},
"links": [],
"text": "Task Runner is a task agent application that polls AWS Data Pipeline for scheduled tasks and executes them on Amazon EC2 instances, Amazon EMR clusters, or other computational resources, reporting status as it does so. Depending on your application, you may choose to:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html#dp-using-task-runner",
"main_header": "Working with Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-dp-managed",
"text": "Task Runner on AWS Data Pipeline-Managed Resources"
},
"links": [],
"text": "When a resource is launched and managed by AWS Data Pipeline, the web service automatically installs Task Runner on that resource to process tasks in the pipeline. You specify a computational resource (either an Amazon EC2 instance or an Amazon EMR cluster) for the runsOn field of an activity object. When AWS Data Pipeline launches this resource, it installs Task Runner on that resource and configures it to process all activity objects that have their runsOn field set to that resource. When AWS Data Pipeline terminates the resource, the Task Runner logs are published to an Amazon S3 location before it shuts down.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html#dp-how-task-runner-dp-managed",
"main_header": "Task Runner on AWS Data Pipeline-Managed Resources",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-dp-managed",
"text": "Task Runner on AWS Data Pipeline-Managed Resources"
},
"links": [],
"text": "For example, if you use the EmrActivity in a pipeline, and specify an EmrCluster resource in the runsOn field. When AWS Data Pipeline processes that activity, it launches an Amazon EMR cluster and installs Task Runner onto the master node. This Task Runner then processes the tasks for activities that have their runsOn field set to that EmrCluster object. The following excerpt from a pipeline definition shows this relationship between the two objects.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html#dp-how-task-runner-dp-managed",
"main_header": "Task Runner on AWS Data Pipeline-Managed Resources",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-dp-managed",
"text": "Task Runner on AWS Data Pipeline-Managed Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
}
],
"text": "For information and examples of running this activity, see EmrActivity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html#dp-how-task-runner-dp-managed",
"main_header": "Task Runner on AWS Data Pipeline-Managed Resources",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-dp-managed",
"text": "Task Runner on AWS Data Pipeline-Managed Resources"
},
"links": [],
"text": "If you have multiple AWS Data Pipeline-managed resources in a pipeline, Task Runner is installed on each of them, and they  all poll AWS Data Pipeline for tasks to process.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-dp-managed.html#dp-how-task-runner-dp-managed",
"main_header": "Task Runner on AWS Data Pipeline-Managed Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "You can install Task Runner on computational resources that you manage, such as an Amazon EC2 instance, or a physical server or workstation. Task Runner can be installed anywhere, on any compatible hardware or operating system, provided that it can communicate with the AWS Data Pipeline web service.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "This approach can be useful when, for example, you want to use AWS Data Pipeline to process data that is stored inside your organization\u00e2\u0080\u0099s firewall. By installing Task Runner on a server in the local network, you can access the local database securely and then poll AWS Data Pipeline for the next task to run. When AWS Data Pipeline ends processing or deletes the pipeline, the Task Runner instance remains running on your computational resource until you manually shut it down. The Task Runner logs persist after pipeline execution is complete.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "To use Task Runner on a resource that you manage, you must first download Task Runner, and then install it on your computational resource, using the procedures in this section.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"links": [],
"text": "To connect a Task Runner that you've installed to the pipeline activities it should process, add a workerGroup field to the object, and configure Task Runner to poll for that worker group value. You do this by passing the worker group string as a parameter (for example, --workerGroup=wg-12345) when you run the Task Runner JAR file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-how-task-runner-user-managed",
"main_header": "Executing Work on Existing Resources Using Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-installing-taskrunner",
"text": "Installing Task Runner"
},
"links": [],
"text": "This section explains how to install and configure Task Runner and its prerequisites. Installation is a straightforward manual process.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-installing-taskrunner",
"main_header": "Installing Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [],
"text": "In a new command prompt window that is set to the directory where you installed Task Runner, start Task Runner with the following command.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [],
"text": "The --config option points to your credentials file.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [],
"text": "The --workerGroup option specifies the name of your worker group, which must be the same value as specified in your pipeline for tasks to be processed.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [],
"text": "The --region option specifies the service region from which to pull tasks to execute.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [],
"text": "The --logUri option is used for pushing your compressed logs to a location in Amazon S3.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [],
"text": "When Task Runner is active, it prints the path to where log files are written in the terminal window. The following is an example.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-activate-task-runner",
"text": "Starting Task Runner"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html",
"title": "Task Runner Configuration Options"
}
],
"text": "Task Runner should be run detached from your login shell. If you are using a terminal application to connect to your computer, you may need to use a utility like nohup or screen to prevent the Task Runner application from exiting when you log out. For more information about command line options, see Task Runner Configuration Options.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-activate-task-runner",
"main_header": "Starting Task Runner",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-how-task-runner-user-managed",
"text": "Executing Work on Existing Resources Using Task Runner"
},
"h2": {
"urllink": "#dp-verify-task-runner",
"text": "Verifying Task Runner Logging"
},
"links": [],
"text": "The easiest way to verify that Task Runner is working is to check whether it is writing log files. Task Runner writes hourly log files to the directory, output/logs, under the directory where Task Runner is installed. The file name is Task Runner.log.YYYY-MM-DD-HH, where HH runs from 00 to 23, in UDT. To save storage space, any log files older than eight hours are compressed with GZip.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-verify-task-runner",
"main_header": "Verifying Task Runner Logging",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-rdssecurity",
"text": "(Optional) Granting Task Runner Access to Amazon RDS"
},
"links": [],
"text": "Amazon RDS allows you to control access to your DB instances using database security groups (DB security groups). A DB security group acts like a firewall controlling network access to your DB instance. By default, network access is turned off for your DB instances. You must modify your DB security groups to let Task Runner access your Amazon RDS instances. Task Runner gains Amazon RDS access from the instance on which it runs, so the accounts and security groups that you add to your Amazon RDS instance depend on where you install Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-rdssecurity.html#dp-taskrunner-rdssecurity",
"main_header": "(Optional) Granting Task Runner Access to Amazon RDS",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-threading",
"text": "Task Runner Threads and Preconditions"
},
"links": [],
"text": "Task Runner uses a thread pool for each of tasks, activities, and preconditions. The default setting for --tasks is 2, meaning that there are two threads allocated from the tasks pool and each thread polls the AWS Data Pipeline service for new tasks. Thus, --tasks is a performance tuning attribute that can be used to help optimize pipeline throughput.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html#dp-taskrunner-threading",
"main_header": "Task Runner Threads and Preconditions",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-threading",
"text": "Task Runner Threads and Preconditions"
},
"links": [],
"text": "Pipeline retry logic for preconditions happens in Task Runner. Two precondition threads are allocated to poll AWS Data Pipeline for precondition objects. Task Runner honors the precondition object retryDelay and preconditionTimeout fields that you define on preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html#dp-taskrunner-threading",
"main_header": "Task Runner Threads and Preconditions",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-threading",
"text": "Task Runner Threads and Preconditions"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-preconditions.html",
"title": "Preconditions"
}
],
"text": "In many cases, decreasing the precondition polling timeout and number of retries helps to improve the performance of your application. Similarly, applications with long-running preconditions may need to have the timeout and retry values increased. For more information about precondition objects, see Preconditions.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-threading.html#dp-taskrunner-threading",
"main_header": "Task Runner Threads and Preconditions",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-config-options",
"text": "Task Runner Configuration Options"
},
"links": [],
"text": "These are the configuration options available from the command line when you launch Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html#dp-taskrunner-config-options",
"main_header": "Task Runner Configuration Options",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-taskrunner-proxy",
"text": "Using Task Runner with a Proxy"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-config-options.html",
"title": "configuration"
},
{
"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-http-proxy.html",
"title": "AWS Command Line Interface"
}
],
"text": "If you are using a proxy host, you can either specify its configuration when invoking Task Runner or set the environment variable, HTTPS_PROXY. The environment variable used with Task Runner accepts the same configuration used for the AWS Command Line Interface.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-proxy.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-taskrunner-proxy.html#dp-taskrunner-proxy",
"main_header": "Using Task Runner with a Proxy",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-ami",
"text": "Task Runner and Custom AMIs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html",
"title": "Ec2Resource"
}
],
"text": "When you specify an Ec2Resource object for your pipeline, AWS Data Pipeline creates an EC2 instance for you, using an AMI that installs and configures Task Runner for you. A PV-compatible instance type is required in this case. Alternatively, you can create a custom AMI with Task Runner, and then specify the ID of this AMI using the imageId field of the Ec2Resource object. For more information, see Ec2Resource.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html#dp-custom-ami",
"main_header": "Task Runner and Custom AMIs",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-custom-ami",
"text": "Task Runner and Custom AMIs"
},
"links": [],
"text": "A custom AMI must meet the following requirements for AWS Data Pipeline to use it successfully for Task Runner:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-ami.html#dp-custom-ami",
"main_header": "Task Runner and Custom AMIs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-troubleshooting",
"text": "Troubleshooting"
},
"links": [],
"text": "When you have a problem with AWS Data Pipeline, the most common symptom is that a pipeline doesn't run. You can use the data that the console and CLI provide to identity the problem and find a solution.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html#dp-troubleshooting",
"main_header": "Troubleshooting",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-troubleshoot-locate-errors",
"text": "Locating Errors in Pipelines"
},
"links": [],
"text": "The AWS Data Pipeline console is a convenient tool to visually monitor the status of your pipelines and easily locate any errors related to failed or incomplete pipeline runs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-locate-errors.html#dp-troubleshoot-locate-errors",
"main_header": "Locating Errors in Pipelines",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-troubleshoot-emr",
"text": "Identifying the Amazon EMR Cluster that Serves Your Pipeline"
},
"links": [],
"text": "If an EMRCluster or EMRActivity fails and the error information provided by the AWS Data Pipeline console is unclear, you can identify the Amazon EMR cluster that serves your pipeline using the Amazon EMR console. This helps you locate the logs that Amazon EMR provides to get more details about errors that occur.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshoot-emr.html#dp-troubleshoot-emr",
"main_header": "Identifying the Amazon EMR Cluster that Serves Your Pipeline",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-pipeline-status",
"text": "Interpreting Pipeline Status Details"
},
"links": [],
"text": "The various status levels displayed in the AWS Data Pipeline console and CLI indicate the condition of a pipeline and its components. The pipeline status is simply an overview of a pipeline; to see more information, view the status of individual pipeline components. You can do this by clicking through a pipeline in the console or retrieving pipeline component details using the CLI.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html#dp-pipeline-status",
"main_header": "Interpreting Pipeline Status Details",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"links": [],
"text": "This section explains how to find the various logs that AWS Data Pipeline writes, which you can use to determine the source of certain failures and errors.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-error-logs",
"main_header": "Locating Error Logs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [],
"text": "We recommend that you configure pipelines to create log files in a persistent location, such as in the following example where you use the pipelineLogUri field on a pipeline's Default object to cause all pipeline components to use an Amazon S3 log location by default (you can override this by configuring a log location in a specific pipeline component).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [],
"text": "To configure the log location using the AWS Data Pipeline CLI in a pipeline JSON file, begin your pipeline file with the following text:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-pipeline-logs",
"text": "Pipeline Logs"
},
"links": [],
"text": "After you configure a pipeline log directory, Task Runner creates a copy of the logs in your directory, with the same formatting and file names described in the previous section about Task Runner logs.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-pipeline-logs",
"main_header": "Pipeline Logs",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-error-logs",
"text": "Locating Error Logs"
},
"h2": {
"urllink": "#dp-hadoop-logs",
"text": "Hadoop Job and Amazon EMR Step Logs"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hadoopactivity.html",
"title": "HadoopActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html",
"title": "HiveActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-pigactivity.html",
"title": "PigActivity"
},
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html",
"title": "EmrActivity"
},
{
"url": "https://docs.aws.amazon.com/emr/latest/DeveloperGuide/emr-manage-view-web-log-files.html",
"title": "View Log Files"
}
],
"text": "With any Hadoop-based activity such as HadoopActivity, HiveActivity, or PigActivity you can view Hadoop job logs at the location returned in the runtime slot, hadoopJobLog. EmrActivity has its own logging features and those logs are stored using the location chosen by Amazon EMR and returned by the runtime slot, emrStepLog. For more information, see View Log Files in the Amazon EMR Developer Guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-error-logs.html#dp-hadoop-logs",
"main_header": "Hadoop Job and Amazon EMR Step Logs",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"links": [],
"text": "This topic provides various symptoms of AWS Data Pipeline problems and the recommended steps to solve them.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-check-when-run-fails",
"main_header": "Resolving Common Problems",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-pipeline-doesnt-start",
"text": "Pipeline Stuck in Pending Status"
},
"links": [],
"text": "A pipeline that appears stuck in the PENDING status indicates that a pipeline has not yet been activated, or activation failed due to an error in the pipeline definition. Ensure that you did not receive any errors when you submitted your pipeline using the AWS Data Pipeline CLI or when you attempted to save or activate your pipeline using the AWS Data Pipeline console. Additionally, check that your pipeline has a valid definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-pipeline-doesnt-start",
"main_header": "Pipeline Stuck in Pending Status",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-pipeline-doesnt-start",
"text": "Pipeline Stuck in Pending Status"
},
"links": [],
"text": "To view your pipeline definition on the screen using the CLI:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-pipeline-doesnt-start",
"main_header": "Pipeline Stuck in Pending Status",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-pipeline-doesnt-start",
"text": "Pipeline Stuck in Pending Status"
},
"links": [],
"text": "Ensure that the pipeline definition is complete, check your closing braces, verify required commas, check for missing references, and other syntax errors. It is best to use a text editor that can visually validate the syntax of JSON files.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-pipeline-doesnt-start",
"main_header": "Pipeline Stuck in Pending Status",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-waiting-for-runner",
"text": "Pipeline Component Stuck in Waiting for Runner Status"
},
"links": [],
"text": "If your pipeline is in the SCHEDULED state and one or more tasks appear stuck in the WAITING_FOR_RUNNER state, ensure that you set a valid value for either the runsOn or workerGroup fields for those tasks. If both values are empty or missing, the task cannot start because there is no association between the task and a worker to perform the tasks. In this situation, you've defined work but haven't defined what computer does the work. If applicable, verify that the workerGroup value assigned to the pipeline component is exactly the same name and case as the workerGroup value that you configured for Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"main_header": "Pipeline Component Stuck in Waiting for Runner Status",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-waiting-for-runner",
"text": "Pipeline Component Stuck in Waiting for Runner Status"
},
"links": [],
"text": "Another potential cause of this problem is that the endpoint and access key provided to Task Runner is not the same as the AWS Data Pipeline console or the computer where the AWS Data Pipeline CLI tools are installed. You might have created new pipelines with no visible errors, but Task Runner polls the wrong location due to the difference in credentials, or polls the correct location with insufficient permissions to identify and run the work specified by the pipeline definition.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-waiting-for-runner",
"main_header": "Pipeline Component Stuck in Waiting for Runner Status",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-runs-stay-pending",
"text": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status"
},
"links": [],
"text": "If your pipeline is in the SCHEDULED state and one or more tasks appear stuck in the WAITING_ON_DEPENDENCIES state, make sure your pipeline's initial preconditions have been met. If the preconditions of the first object in the logic chain are not met, none of the objects that depend on that first object can move out of the WAITING_ON_DEPENDENCIES state.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-runs-stay-pending",
"main_header": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-runs-stay-pending",
"text": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status"
},
"links": [],
"text": "For example, consider the following excerpt from a pipeline definition. In this case, the InputData object has a precondition 'Ready' specifying that the data must exist before the InputData object is complete. If the data does not exist, the InputData object remains in the WAITING_ON_DEPENDENCIES state, waiting for the data specified by the path field to become available. Any objects that depend on InputData likewise remain in a WAITING_ON_DEPENDENCIES state waiting for the InputData object to reach the FINISHED state.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-runs-stay-pending",
"main_header": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-runs-stay-pending",
"text": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status"
},
"links": [],
"text": "Also, check that your objects have the proper permissions to access the data. In the preceding example, if the information in the credentials field did not have permissions to access the data specified in the path field, the InputData object would get stuck in a WAITING_ON_DEPENDENCIES state because it cannot access the data specified by the path field, even if that data exists.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-runs-stay-pending",
"main_header": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-runs-stay-pending",
"text": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status"
},
"links": [],
"text": "It is also possible that a resource communicating with Amazon S3 does not have a public IP address associated with it. For example, an Ec2Resource in a public subnet must have a public IP address associated with it.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-runs-stay-pending",
"main_header": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-runs-stay-pending",
"text": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-schedules.html",
"title": "Scheduling Pipelines"
}
],
"text": "Lastly, under certain conditions, resource instances can reach the WAITING_ON_DEPENDENCIES state much earlier than their associated activities are scheduled to start, which may give the impression that the resource or the activity is failing. For more information about the behavior of resources and the schedule type setting, see the Resources Ignore Schedule Type section in the Scheduling Pipelines topic.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-runs-stay-pending",
"main_header": "Pipeline Component Stuck in WAITING_ON_DEPENDENCIES Status",
"images": [],
"container_type": "p",
"children_tags": [
"code",
"em",
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-run-doesnt-start-scheduled",
"text": "Run Doesn't Start When Scheduled"
},
"links": [],
"text": "Check that you chose the correct schedule type that determines whether your task starts at the beginning of the schedule interval (Cron Style Schedule Type) or at the end of the schedule interval (Time Series Schedule Type).",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-run-doesnt-start-scheduled",
"main_header": "Run Doesn't Start When Scheduled",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-run-doesnt-start-scheduled",
"text": "Run Doesn't Start When Scheduled"
},
"links": [],
"text": "Additionally, check that you have properly specified the dates in your schedule objects and that the startDateTime and endDateTime values are in UTC format, such as in the following example:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-run-doesnt-start-scheduled",
"main_header": "Run Doesn't Start When Scheduled",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-out-of-order",
"text": "Pipeline Components Run in Wrong Order"
},
"links": [],
"text": "You might notice that the start and end times for your pipeline components are running in the wrong order, or in a different sequence than you expect. It is important to understand that pipeline components can start running simultaneously if their preconditions are met at start-up time. In other words, pipeline components do not execute sequentially by default; if you need a specific execution order, you must control the execution order with preconditions and dependsOn fields.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-out-of-order",
"main_header": "Pipeline Components Run in Wrong Order",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-out-of-order",
"text": "Pipeline Components Run in Wrong Order"
},
"links": [],
"text": "Verify that you are using the dependsOn field populated with a reference to the correct prerequisite pipeline components, and that all the necessary pointers between components are present to achieve the order you require.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-out-of-order",
"main_header": "Pipeline Components Run in Wrong Order",
"images": [],
"container_type": "p",
"children_tags": [
"code"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-securitytoken",
"text": "EMR Cluster Fails With Error: The security token included in the request is invalid"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "Verify your IAM roles, policies, and trust relationships as described in IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-securitytoken",
"main_header": "EMR Cluster Fails With Error: The security token included in the request is invalid",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-insufficient-permissions",
"text": "Insufficient Permissions to Access Resources"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "Permissions that you set on IAM roles determine whether AWS Data Pipeline can access your EMR clusters and EC2 instances to run your pipelines. Additionally, IAM provides the concept of trust relationships that go further to allow the creation of resources on your behalf. For example, when you create a pipeline that uses an EC2 instance to run a command to move data, AWS Data Pipeline can provision this EC2 instance for you. If you encounter problems, especially those involving resources that you can access manually but AWS Data Pipeline cannot, verify your IAM roles, policies, and trust relationships as described in IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-insufficient-permissions",
"main_header": "Insufficient Permissions to Access Resources",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-error-code-400",
"text": "Status Code: 400 Error Code: PipelineNotFoundException"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html",
"title": "IAM Roles for AWS Data Pipeline"
}
],
"text": "This error means that your IAM default roles might not have the required permissions necessary for AWS Data Pipeline to function correctly. For more information, see IAM Roles for AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-error-code-400",
"main_header": "Status Code: 400 Error Code: PipelineNotFoundException",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-bad-token",
"text": "Creating a Pipeline Causes a Security Token Error"
},
"links": [],
"text": "You receive the following error when you try to create a pipeline:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-bad-token",
"main_header": "Creating a Pipeline Causes a Security Token Error",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-bad-token",
"text": "Creating a Pipeline Causes a Security Token Error"
},
"links": [],
"text": "Failed to create pipeline with 'pipeline_name'. Error: UnrecognizedClientException - The security token included in the request is invalid.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-bad-token",
"main_header": "Creating a Pipeline Causes a Security Token Error",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-no-details-shown",
"text": "Cannot See Pipeline Details in the Console"
},
"links": [],
"text": "The AWS Data Pipeline console pipeline filter applies to the scheduled start date for a pipeline, without regard to when the pipeline was submitted. It is possible to submit a new pipeline using a scheduled start date that occurs in the past, which the default date filter may not show. To see the pipeline details, change your date filter to ensure that the scheduled pipeline start date fits within the date range filter.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-no-details-shown",
"main_header": "Cannot See Pipeline Details in the Console",
"images": [],
"container_type": "p",
"children_tags": [
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-error-code-s3-404",
"text": "Error in remote runner Status Code: 404, AWS Service: Amazon S3"
},
"links": [],
"text": "This error means that Task Runner could not access your files in Amazon S3. Verify that:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-error-code-s3-404",
"main_header": "Error in remote runner Status Code: 404, AWS Service: Amazon S3",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [],
"text": "In the Task Runner logs, you may see an error that is similar to the following:",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-access-denied",
"text": "Access Denied - Not Authorized to Perform Function datapipeline:"
},
"links": [
{
"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/ManagingPolicies.html",
"title": "Managing IAM Policies"
}
],
"text": "This error message indicates that the IAM role you specified needs additional permissions necessary to interact with AWS Data Pipeline. Ensure that your IAM role policy contains the following lines, where PollForTask is replaced with the name of the permission you want to add (use * to grant all permissions). For more information about how to create a new IAM role and apply a policy to it, see Managing IAM Policies in the Using IAM guide.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-access-denied",
"main_header": "Access Denied - Not Authorized to Perform Function datapipeline:",
"images": [],
"container_type": "p",
"children_tags": [
"a",
"em"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#AMIs-false-data-large-CSV",
"text": "Older Amazon EMR AMIs May Create False Data for Large CSV Files"
},
"links": [],
"text": "On Amazon EMR AMIs previous to 3.9 (3.8 and below) AWS Data Pipeline uses a custom InputFormat to read and write CSV files for use with MapReduce jobs. This is used when the service stages tables to and from Amazon S3. An issue with this InputFormat was discovered where reading records from large CSV files may result in producing tables that are not correctly copied. This issue was fixed in later Amazon EMR releases. Please use Amazon EMR AMI 3.9 or an Amazon EMR release 4.0.0 or greater.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#AMIs-false-data-large-CSV",
"main_header": "Older Amazon EMR AMIs May Create False Data for Large CSV Files",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-check-when-run-fails",
"text": "Resolving Common Problems"
},
"h2": {
"urllink": "#dp-increase-limits",
"text": "Increasing AWS Data Pipeline Limits"
},
"links": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"title": "AWS Data Pipeline Limits"
},
{
"url": "https://console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase&limitType=service-code-datapipe",
"title": "Data Pipeline Limit Increase"
}
],
"text": "Occasionally, you may exceed specific AWS Data Pipeline system limits. For example, the default pipeline limit is 20 pipelines with 50 objects in each. If you discover that you need more pipelines than the limit, consider merging multiple pipelines to create fewer pipelines with more objects in each. For more information about the AWS Data Pipeline limits, see AWS Data Pipeline Limits. However, if you are unable to work around the limits using the pipeline merge technique, request an increase in your capacity using this form: Data Pipeline Limit Increase.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-check-when-run-fails.html#dp-increase-limits",
"main_header": "Increasing AWS Data Pipeline Limits",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": [
{
"url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-troubleshooting.html",
"title": "Troubleshooting"
}
]
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"links": [],
"text": "To ensure that there is capacity for all users, AWS Data Pipeline imposes limits on the resources that you can allocate and the rate at which you can allocate resources.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits",
"main_header": "AWS Data Pipeline Limits",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-account",
"text": "Account Limits"
},
"links": [
{
"url": "https://console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase&limitType=service-code-datapipe",
"title": "Amazon Web Services Support Center request form"
}
],
"text": "The following limits apply to a single AWS account. If you require additional capacity, you can use the Amazon Web Services Support Center request form to increase your capacity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-account",
"main_header": "Account Limits",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "AWS Data Pipeline limits the rate at which you can call the web service API. These limits also apply to AWS Data Pipeline agents that call the web service API on your behalf, such as the console, CLI, and Task Runner.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "The following limits apply to a single AWS account. This means the total usage on the account, including that by IAM users, cannot exceed these limits.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "The burst rate lets you save up web service calls during periods of inactivity and expend them all in a short amount of time. For example, CreatePipeline has a regular rate of one call each five seconds. If you don't call the service for 30 seconds, you have six calls saved up. You could then call the web service six times in a second. Because this is below the burst limit and keeps your average calls at the regular rate limit, your calls are not throttled.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "If you exceed the rate limit and the burst limit, your web service call fails and returns a throttling exception. The default implementation of a worker, Task Runner, automatically retries API calls that fail with a throttling exception. Task Runner has a back off so that subsequent attempts to call the API occur at increasingly longer intervals. If you write a worker, we recommend that you implement similar retry logic.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-limits-web-service",
"text": "Web Service Call Limits"
},
"links": [],
"text": "These limits are applied against an individual AWS account.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-limits-web-service",
"main_header": "Web Service Call Limits",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-scaling-considerations",
"text": "Scaling Considerations"
},
"links": [],
"text": "AWS Data Pipeline scales to accommodate a huge number of concurrent tasks and you can configure it to automatically create the resources necessary to handle large workloads. These automatically created resources are under your control and count against your AWS account resource limits. For example, if you configure AWS Data Pipeline to automatically create a 20-node Amazon EMR cluster to process data and your AWS account has an EC2 instance limit set to 20, you may inadvertently exhaust your available backfill resources. As a result, consider these resource restrictions in your design or increase your account limits accordingly.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-scaling-considerations",
"main_header": "Scaling Considerations",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#dp-limits",
"text": "AWS Data Pipeline Limits"
},
"h2": {
"urllink": "#dp-scaling-considerations",
"text": "Scaling Considerations"
},
"links": [
{
"url": "https://console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase&limitType=service-code-datapipe",
"title": "Amazon Web Services Support Center request form"
}
],
"text": "If you require additional capacity, you can use the Amazon Web Services Support Center request form to increase your capacity.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-limits.html#dp-scaling-considerations",
"main_header": "Scaling Considerations",
"images": [],
"container_type": "p",
"children_tags": [
"a"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#RelatedResources",
"text": "AWS Data Pipeline Resources"
},
"links": [],
"text": "The following are resources to help you use AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/RelatedResources.html#RelatedResources",
"main_header": "AWS Data Pipeline Resources",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#DocHistory",
"text": "Document History"
},
"links": [],
"text": "This documentation is associated with the 2012-10-29 version of AWS Data Pipeline.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html#DocHistory",
"main_header": "Document History",
"images": [],
"container_type": "p",
"children_tags": [],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
},
{
"h1": {
"urllink": "#DocHistory",
"text": "Document History"
},
"links": [],
"text": "Latest documentation update: 9 November 2018.",
"base_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html",
"main_url": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/DocHistory.html#DocHistory",
"main_header": "Document History",
"images": [],
"container_type": "p",
"children_tags": [
"b"
],
"aws_service": "datapipeline",
"aws_guide": "developer",
"headers": []
}
]